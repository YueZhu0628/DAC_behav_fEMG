---
title: "DARDC_behavioral_Data_Analysis"
output:
  html_notebook
---
# Prepare working environment
```{r}
rm(list = ls())
```

set working directory
```{r}
getwd()
setwd("/Users/ottolab/Desktop/Yue/DARDC_v2_Data_Analysis")
# getwd()
```

```{r Library Packages, include=FALSE}
library(dplyr)
library(tidyr)
library(rstatix)
library(psych)
library(lattice)
library(ggplot2)
library(ggrepel)
library(ggpubr)
library(ggsci)
library(sjPlot)
library(ggeffects)
library(webshot)
library(ez)
library(BayesFactor)
library(lme4)
library(blme)
library(lmerTest)
library(emmeans)
library(afex)
library(performance) # check for assumptions of statistical analyses
library(grid)
library(mediation)
```

I need to preprocess the data of each participant, and then combine the processed data of all participants into one .csv file. /n
So the steps are:/n
1) Define a function of Data Preprocess./n
2) Loop the function for all participants./n


 ### ------------------------------------------------------------------------------------- ###

# DATA ANALYSIS VERSION 1

## Define Data Preprocess function
```{r}
DataInput_v1_1 <- function(p_raw_data_file){
  p_df_raw <- read.csv(p_raw_data_file)
  # print(colnames(p_df_raw))
  select_cols <- c("PID", "SONA", "Gender", "Age", "Run",
                   "Phase", "Block", "TrialN", "NbackLevel", "ISI", 
                   # trial info
                   "TrialIndex", "TrialNumber_total", "TrialNumber_valid",
                   # trial performance
                   "Trial_A_measure", 
                   "Trial_HR", "Trial_FAR", "Trial_CDR", "Trial_MSR", 
                   "Trial_ACC","Trial_RT",
                   "Letter_Resp", "Letter_RT", "Letter_Accuracy",
                   "Trial_HitsCount","Trial_FAsCount", 
                   "Trial_NoRespCount", "Trial_SameNoRespCount", "Trial_DiffNoRespCount",
                   "Trial_CDsCount",
                   "Trial_CorrectRespCount",
                   "Trial_SameLetterCount", "Trial_DiffLetterCount", "Trial_InvalidLetterCount",
                   # main task rating
                   "CueFigure", "CueMapping", "TrialDemand",
                   "Rating", "RatingRT", "RatingTimeOut",
                   # choice 
                   "CueLeft", "TrialDemandLeft", 
                   "CueRight", "TrialDemandRight",
                   "Choice", "DemandChoice", "ChoiceRT", 
                   # offline rating T1, T2
                   "OfflineRatingTimePoint",
                   "OfflineRatingCue", "OfflineRatingTrialDemand", 
                   "OfflineDemandRating", "OfflineDemandRatingRT", 
                   # demand rating
                   "DemandRatingQuesIndex", "QuesTLX", "DemandLevel", 
                   "DemandRating", "DemandRatingRT", 
                   # learning check
                   "Cue", "CueDemand", "CorrectAnswer", 
                   "Answer", "AnswerDemand", "AnswerRT", "AnswerACC",
                   # End
                   "END")
  for (col in select_cols){
    check <- col %in% colnames(p_df_raw)
    if (! check){
     print(c(col, check))
    }
  }
  p_df_selected <- p_df_raw[, select_cols]
  return(p_df_selected)
}
```

```{r}
DataInput_v1_2 <- function(p_raw_data_file){
  p_df_raw <- read.csv(p_raw_data_file)
  # print(colnames(p_df_raw))
  select_cols <- c("PID", "SONA", "Gender", "Age", "Run",
                   "Phase", "Block", "TrialN", "NbackLevel", "ISI", 
                   # trial info
                   "TrialIndex", "TrialNumber_total", "TrialNumber_valid",
                   # trial performance
                   "Trial_A_measure", 
                   "Trial_HR", "Trial_FAR", "Trial_CDR", "Trial_MSR", 
                   "Trial_ACC","Trial_RT",
                   "Letter_Resp", "Letter_RT", "Letter_Accuracy",
                   "Trial_HitsCount","Trial_FAsCount", 
                   "Trial_NoRespCount", "Trial_SameNoRespCount", "Trial_DiffNoRespCount",
                   "Trial_CDsCount",
                   "Trial_CorrectRespCount",
                   "Trial_SameLetterCount", "Trial_DiffLetterCount", "Trial_InvalidLetterCount",
                   # main task rating
                   "CueLearning", "CueMapping", "TrialDemandLearning",
                   "OnlineRating", "OnlineRatingRT", "OnlineRatingTimeOut",
                   # choice 
                   "PhaseType",
                   "CueLeft", "CueDemandLeft", 
                   "CueRight", "CueDemandRight",
                   "Choice", "ChoiceRT", 
                   "ChoiceDemand", "TaskDemand",
                   # offline rating T1, T2
                   "OfflineRatingTimePoint",
                   "CueFigure", "TrialDemand", 
                   "OfflineRating", "OfflineRatingRT", 
                   # demand rating
                   "DemandRatingQuesIndex", "QuesTLX", "DemandLevel", 
                   "DemandRating", "DemandRatingRT", 
                   # learning check
                   "CueCheck", "CueDemandCheck", "CorrectAnswer", 
                   "Answer", "AnswerDemand", "AnswerRT", "AnswerACC",
                   # End
                   "END")
  for (col in select_cols){
    check <- col %in% colnames(p_df_raw)
    if (! check){
     print(c(col, check))
    }
  }
  p_df_selected <- p_df_raw[, select_cols]
  return(p_df_selected)
}
```


## Load data of all participants
data_v1 of the 1st recruitment
```{r Load Data_v1_1}
data_path_v1_1 <- "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy/data_analysis/data"
data_all_v1 <- list()
p_folders <- list.dirs(data_path_v1_1, recursive = FALSE)
# print(p_folders)
for (p_folder in p_folders){
  # print(p_folder)
  p_raw_data_file <- file.path(p_folder, paste0(basename(p_folder), "_task.csv"))
  # print(p_raw_data_file)
  p_data <- DataInput_v1_1(p_raw_data_file)
  data_all_v1[[basename(p_folder)]] <- p_data  # R中用元素名称作为索引
}

df_all_v1_1 <- do.call(rbind, data_all_v1) %>% 
  rename(OnlineRating = Rating,
         OnlineRatingRT = RatingRT,
         OnlineRatingTimeOut = RatingTimeOut,
         CueLearning = CueFigure,
         CueDemandCheck = CueDemand,
         TrialDemandLearning = TrialDemand,
         CueFigure = OfflineRatingCue,
         TrialDemand = OfflineRatingTrialDemand,
         OfflineRating = OfflineDemandRating,
         OfflineRatingRT = OfflineDemandRatingRT,
         CueDemandLeft = TrialDemandLeft,
         CueDemandRight = TrialDemandRight,
         ChoiceDemand = DemandChoice,
         CueCheck = Cue) %>% 
  mutate(TaskDemand = ChoiceDemand) %>% 
  select(1:which(colnames(.) == "ChoiceDemand"), TaskDemand, everything())
```

```{r}
write.csv(df_all_v1_1, na = "",
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/df_all_v1_1.csv")
```


data_v1 of the 2nd recruitment
```{r Load Data_v1_2}
data_path_v1_2 <- "/Users/ottolab/Desktop/Yue/DARDC_v2_Data_Analysis/data_move_here"
data_all_v1 <- list()
p_folders <- list.dirs(data_path_v1_2, recursive = FALSE)
# print(p_folders)
for (p_folder in p_folders){
  # print(p_folder)
  p_raw_data_file <- file.path(p_folder, paste0(basename(p_folder), "_task.csv"))
  # print(p_raw_data_file)
  p_data <- DataInput_v1_2(p_raw_data_file)
  data_all_v1[[basename(p_folder)]] <- p_data  # R中用元素名称作为索引
}

df_all_v1_2 <- do.call(rbind, data_all_v1) %>% 
  filter(PhaseType != "FALSE" | is.na(PhaseType)) %>% 
  select(-PhaseType) %>% 
  mutate(Phase = ifelse(Phase == "TrueCHOICE", "CHOICE", Phase))
```

```{r}
write.csv(df_all_v1_2, na = "",
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/df_all_v1_2.csv")
```


combing data_v1 of two periods
```{r}
df_all_v1 <- rbind(df_all_v1_1, df_all_v1_2)
```

```{r}
write.csv(df_all_v1, na = "",
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/df_all_v1.csv")
```



## Preprocess the whole dataframe
filter rows
include rows whose: Phase in c("CALIBRATION", "TEST", "Test")
```{r}
df_all_v1_select <- 
  df_all_v1 %>% 
  filter(Phase %in% c("TEST", "Test", 
                      "CHOICE", "Choice",
                      "OFFLINERATING", 
                      "DEMANDRATING", "LEARNINGCHECK"))
```

```{r}
df_v1_TEST_Test <- 
  df_all_v1_select %>% 
  filter(Phase %in% c("TEST", "Test"))

df_v1_CHOICE_Choice <- 
  df_all_v1_select %>% 
  filter(Phase %in% c("CHOICE", "Choice"))

df_v1_OFFLINERATING <- 
  df_all_v1_select %>% 
  filter(Phase == "OFFLINERATING")

df_v1_DEMANDRATING <- 
  df_all_v1_select %>% 
  filter(Phase == "DEMANDRATING")

df_v1_LEARNINGCHECK <- 
  df_all_v1_select %>% 
  filter(Phase == "LEARNINGCHECK")
```


merge the corresponding trial row of "Test" and "TEST"
```{r}
df_v1_TEST <- 
  df_v1_TEST_Test %>% 
  group_by(Block, TrialN) %>% 
  mutate(
    Trial_A_measure = ifelse(Phase %in% c("TEST"), lag(Trial_A_measure), Trial_A_measure),
    Trial_HR = ifelse(Phase %in% c("TEST"), lag(Trial_HR), Trial_HR),
    Trial_FAR = ifelse(Phase %in% c("TEST"), lag(Trial_FAR), Trial_FAR),
    Trial_CDR = ifelse(Phase %in% c("TEST"), lag(Trial_CDR), Trial_CDR),
    Trial_MSR = ifelse(Phase %in% c("TEST"), lag(Trial_MSR), Trial_MSR),
    Trial_ACC = ifelse(Phase %in% c("TEST"), lag(Trial_ACC), Trial_ACC),
    Trial_RT = ifelse(Phase %in% c("TEST"), lag(Trial_RT), Trial_RT),
    Trial_HitsCount = ifelse(Phase %in% c("TEST"), lag(Trial_HitsCount), Trial_HitsCount),
    Trial_FAsCount = ifelse(Phase %in% c("TEST"), lag(Trial_FAsCount), Trial_FAsCount),
    Trial_SameNoRespCount = ifelse(Phase %in% c("TEST"), 
                                   lag(Trial_SameNoRespCount), Trial_SameNoRespCount),
    Trial_DiffNoRespCount = ifelse(Phase %in% c("TEST"), 
                                   lag(Trial_DiffNoRespCount), Trial_DiffNoRespCount),
    Trial_NoRespCount = ifelse(Phase %in% c("TEST"), 
                               lag(Trial_NoRespCount), Trial_NoRespCount),
    Trial_CDsCount = ifelse(Phase %in% c("TEST"), lag(Trial_CDsCount), Trial_CDsCount),
    Trial_CorrectRespCount = ifelse(Phase %in% c("TEST"), 
                                    lag(Trial_CorrectRespCount), Trial_CorrectRespCount),
    Trial_SameLetterCount = ifelse(Phase %in% c("TEST"), 
                                   lag(Trial_SameLetterCount), Trial_SameLetterCount),
    Trial_DiffLetterCount = ifelse(Phase %in% c("TEST"), 
                                   lag(Trial_DiffLetterCount), Trial_DiffLetterCount),
    Trial_InvalidLetterCount = ifelse(Phase %in% c("TEST"), 
                                      lag(Trial_InvalidLetterCount), Trial_InvalidLetterCount)) %>% 
  ungroup() %>% 
  filter(Phase == "TEST") %>% 
  group_by(PID) %>% 
  mutate(TrialDemandLearning_pre = lag(TrialDemandLearning),
         TrialTransition = ifelse(TrialDemandLearning == lag(TrialDemandLearning), 
                                  "repetition", "switch"),
         TrialTransition_pre = lag(TrialTransition),
         Trial_RT_pre = lag(Trial_RT),
         Trial_ACC_pre = lag(Trial_ACC),
         Trial_A_measure_pre = lag(Trial_A_measure)) %>% 
  ungroup() 


df_v1_CHOICE <- 
  df_v1_CHOICE_Choice %>% 
  group_by(Block, TrialN) %>% 
  mutate(
    Trial_A_measure = ifelse(Phase %in% c("CHOICE"), 
                             lag(Trial_A_measure), Trial_A_measure),
    Trial_HR = ifelse(Phase %in% c("CHOICE"), lag(Trial_HR), Trial_HR),
    Trial_FAR = ifelse(Phase %in% c("CHOICE"), lag(Trial_FAR), Trial_FAR),
    Trial_CDR = ifelse(Phase %in% c("CHOICE"), lag(Trial_CDR), Trial_CDR),
    Trial_MSR = ifelse(Phase %in% c("CHOICE"), lag(Trial_MSR), Trial_MSR),
    Trial_ACC = ifelse(Phase %in% c("CHOICE"), lag(Trial_ACC), Trial_ACC),
    Trial_RT = ifelse(Phase %in% c("CHOICE"), lag(Trial_RT), Trial_RT),
    Trial_HitsCount = ifelse(Phase %in% c("CHOICE"), 
                             lag(Trial_HitsCount), Trial_HitsCount),
    Trial_FAsCount = ifelse(Phase %in% c("CHOICE"), lag(Trial_FAsCount), Trial_FAsCount),
    Trial_SameNoRespCount = ifelse(Phase %in% c("CHOICE"), 
                                   lag(Trial_SameNoRespCount), Trial_SameNoRespCount),
    Trial_DiffNoRespCount = ifelse(Phase %in% c("CHOICE"), 
                                   lag(Trial_DiffNoRespCount), Trial_DiffNoRespCount),
    Trial_NoRespCount = ifelse(Phase %in% c("CHOICE"), 
                               lag(Trial_NoRespCount), Trial_NoRespCount),
    Trial_CDsCount = ifelse(Phase %in% c("CHOICE"), lag(Trial_CDsCount), Trial_CDsCount),
    Trial_CorrectRespCount = ifelse(Phase %in% c("CHOICE"), 
                                    lag(Trial_CorrectRespCount), Trial_CorrectRespCount),
    Trial_SameLetterCount = ifelse(Phase %in% c("CHOICE"), 
                                   lag(Trial_SameLetterCount), Trial_SameLetterCount),
    Trial_DiffLetterCount = ifelse(Phase %in% c("CHOICE"), 
                                   lag(Trial_DiffLetterCount), Trial_DiffLetterCount),
    Trial_InvalidLetterCount = ifelse(Phase %in% c("CHOICE"), 
                                      lag(Trial_InvalidLetterCount), Trial_InvalidLetterCount)) %>%
  ungroup() %>% 
  filter(Phase == "CHOICE") %>% 
  group_by(PID) %>% 
  mutate(TrialDemandLearning_pre = lag(TrialDemandLearning),
         TrialTransition = ifelse(TrialDemandLearning == lag(TrialDemandLearning), 
                                  "repetition", "switch"),
         TrialTransition_pre = lag(TrialTransition),
         Trial_RT_pre = lag(Trial_RT),
         Trial_ACC_pre = lag(Trial_ACC),
         Trial_A_measure_pre = lag(Trial_A_measure)) %>% 
  ungroup() 

df_v1_LEARNINGCHECK <- 
  df_v1_LEARNINGCHECK %>% 
  mutate(LearningCheck = ifelse(AnswerACC == 0, "NotLearned", "Learned"))

df_v1_DEMANDRATING <-
  df_v1_DEMANDRATING %>% 
  group_by(PID) %>% 
  mutate(DemandRatingZ = c(scale(DemandRating, center = TRUE, scale = TRUE)),
         DemandCheck = ifelse(DemandLevel==1, "easy", ifelse(DemandLevel==2, NA, "hard"))) %>%  
  filter(!is.na(DemandCheck)) %>% 
  mutate(DemandCheck = factor(DemandCheck, levels = c("easy", "hard"))) %>% 
  ungroup() 
```


```{r}
# check for participants who didn't learned the association
failedLearningPIDs_v1 <- 
  df_v1_LEARNINGCHECK$PID[df_v1_LEARNINGCHECK$LearningCheck=="NotLearned"] %>% 
  unique()
failedLearningPIDs_v1
```

```{r}
excludedPIDs_v1 <- 
  c(failedLearningPIDs_v1, 53, 55)
# PID 53 calibration too long; rated 1-back as harder
# PID 55 mouse malfunction
```


```{r}
df_all_v1_merged <- 
  bind_rows(df_v1_TEST, df_v1_CHOICE,
            df_v1_OFFLINERATING, df_v1_DEMANDRATING, df_v1_LEARNINGCHECK) 
write.csv(df_all_v1_merged, na = "",
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/df_all_v1_merged.csv")

df_v1_merged <- bind_rows(df_v1_TEST, df_v1_CHOICE) 
write.csv(df_v1_merged, na = "",
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/df_v1_merged.csv")
```



## df_v1_good: the dataframe ready for statistical analysis
Z scores were scaled by the means of each participant
```{r}
cols_v1 <- c("PID", "Gender", "Phase", "Block", "TrialN", "TrialIndex", "NbackLevel","ISI",
          # trial performance
          "Trial_A_measure", "Trial_A_measure_pre",
          "Trial_RT", "Trial_ACC", "Trial_RT_pre", "Trial_ACC_pre",
          "TrialDemandLearning", "TrialTransition", "TrialDemandLearning_pre", "TrialTransition_pre",
          # main task rating
          "OnlineRating", "OnlineRatingRT",
          "CueLearning", "CueMapping", 
          # choice 
          "CueLeft", "CueDemandLeft", 
          "CueRight", "CueDemandRight",
          "Choice", "ChoiceDemand", "TaskDemand", "ChoiceRT", 
          # offline rating T1, T2
          "OfflineRatingTimePoint",
          "CueFigure", "TrialDemand", 
          "OfflineRating", "OfflineRatingRT", 
          # demand rating
          "DemandRatingQuesIndex", "QuesTLX", "TrialDemand", 
          "DemandRating", "DemandRatingZ" , "DemandRatingRT", 
          # learning check
          "CueCheck", "CueDemandCheck", "CorrectAnswer", 
          "Answer", "AnswerDemand", "AnswerRT", "AnswerACC",
          "LearningCheck")

df_all_v1_good <- 
  df_all_v1_merged %>% 
  filter(!PID %in% excludedPIDs_v1) %>% 
  select(all_of(cols_v1)) %>% 
  group_by(PID) %>% 
  mutate(TaskDemand = ifelse(Phase == "CHOICE", TaskDemand, TrialDemandLearning),
         OnlineRatingMean = mean(OnlineRating, na.rm=TRUE),
         OnlineRatingSD = sd(OnlineRating, na.rm=TRUE),
         TaskDemand = factor(TaskDemand),
         TrialTransition = factor(TrialTransition)) %>%
  mutate(RTZ = c(scale(Trial_RT, center = TRUE, scale = TRUE)),
         ACCZ = c(scale(Trial_ACC, center = TRUE, scale = TRUE)),
         RT_preZ = c(scale(Trial_RT_pre, center = TRUE, scale = TRUE)),
         ACC_preZ = c(scale(Trial_ACC_pre, center = TRUE, scale = TRUE)),
         OnlineRatingZ = (OnlineRating-OnlineRatingMean)/OnlineRatingSD,
         OfflineRatingZ = ((OfflineRating-OnlineRatingMean)/OnlineRatingSD)) %>% 
  ungroup()

write.csv(df_all_v1_good, na = "",
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/df_all_v1_good.csv")


cols_v1 <- c("PID", "Gender", "Phase", "Block", "TrialN", "TrialIndex", "NbackLevel","ISI",
          # trial performance
          "Trial_A_measure", "Trial_A_measure_pre",
          "Trial_RT", "Trial_ACC", "Trial_RT_pre", "Trial_ACC_pre",
          "TrialDemandLearning", "TrialTransition", "TrialDemandLearning_pre", "TrialTransition_pre",
          # main task rating
          "OnlineRating", "OnlineRatingRT",
          "CueLearning", "CueMapping", 
          # choice 
          "CueLeft", "CueDemandLeft", 
          "CueRight", "CueDemandRight",
          "Choice", "ChoiceDemand", "TaskDemand", "ChoiceRT")

df_v1_good <- 
  df_v1_merged %>% 
  filter(!PID %in% excludedPIDs_v1) %>% 
  select(all_of(cols_v1)) %>% 
  group_by(PID) %>% 
  mutate(TaskDemand = ifelse(Phase %in% c("CHOICE"), TaskDemand, TrialDemandLearning),
         OnlineRatingMean = mean(OnlineRating, na.rm=TRUE),
         OnlineRatingSD = sd(OnlineRating, na.rm=TRUE),
         TaskDemand = factor(TaskDemand),
         TrialTransition = factor(TrialTransition)) %>%
  mutate(RTZ = c(scale(Trial_RT, center = TRUE, scale = TRUE)),
         ACCZ = c(scale(Trial_ACC, center = TRUE, scale = TRUE)),
         RT_preZ = c(scale(Trial_RT_pre, center = TRUE, scale = TRUE)),
         ACC_preZ = c(scale(Trial_ACC_pre, center = TRUE, scale = TRUE)),
         OnlineRatingZ = (OnlineRating-OnlineRatingMean)/OnlineRatingSD) %>% 
  ungroup()

write.csv(df_v1_good, na = "",
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/df_v1_good.csv")
```


```{r}
df_v1_good_TEST <- 
  df_v1_good %>% 
  filter(Phase == "TEST")

df_v1_good_CHOICE <- 
  df_v1_good %>% 
  filter(Phase == "CHOICE")

df_v1_good_OFFLINERATING <- 
  df_all_v1_good %>% 
  filter(Phase == "OFFLINERATING") 
```



## Overview of participants demographics
```{r}
df_v1_good_demo <- 
  df_v1_good %>%
  group_by(PID) %>% 
  filter(Phase=="TEST", TrialN==1)
```


```{r}
print(c("Gender"))
print(table(df_v1_good_demo$Gender))

print(c("Calibrated ISI"))
print(table(df_v1_good_demo$ISI))
# print(df_v1_good_demo$PID[df_v1_good_demo$ISI == 2583])
```

```{r}
df_v1_good_demo %>% 
  ggplot(aes(x=factor(sort(ISI))))+
  geom_bar(position = position_dodge(width = 0.5) ,width = 0.7, alpha=0.8)+
  labs(title = "Calibrated ISI", x = "ISI") +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



## Overview of performance distribution
```{r}
df_v1_good_TEST %>% 
  ggboxplot(data=., x="PID", y = "Trial_RT", 
          color = "TaskDemand", 
          width = 0.7, dodge = 0) + 
  scale_color_lancet() +
  labs(title = "Trial RT", x = "PID", y = "Trial RT (s)", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
df_v1_good_TEST %>% 
  ggboxplot(data=., x="PID", y = "Trial_ACC", 
          color = "TaskDemand", 
          width = 0.7, dodge = 0) + 
  scale_color_lancet() + 
  labs(title = "Trial ACC", x = "PID", y = "Trial ACC", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




## Check for Performance Difference between Demands 
compare the RT and ACC in 1-back and 3-back trials
### within each participant
Rts of Demand levels within each participant
```{r}
pw_rt <- 
  df_v1_good_TEST %>% 
  group_by(PID) %>%
  t_test(RTZ ~ TaskDemand, paired = TRUE, detailed = TRUE) %>%
  add_significance() %>% 
  add_xy_position(x = "TaskDemand")
```

ACCs of Demand levels within each participant
```{r}
pw_acc <- 
  df_v1_good_TEST %>% 
  group_by(PID) %>% 
  t_test(Trial_ACC ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "TaskDemand")
```

### across participants (means of participants)
RT
```{r}
df_rt_means_de_v1 <- 
  df_v1_good_TEST %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(RTs = mean(Trial_RT, na.rm = TRUE), .groups = "drop")

df_rtz_means_de_v1 <- 
  df_v1_good_TEST %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(RTZs = mean(RTZ, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_rt_v1 <- 
  df_rt_means_de_v1 %>% 
  t_test(RTs ~ TaskDemand, comparisons = list("easy", "hard"), paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

t_rtz_v1 <- 
  df_rtz_means_de_v1 %>% 
  t_test(RTZs ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

ggplot(data = df_rt_means_de_v1, aes(x = TaskDemand, y = RTs, color = TaskDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_y_continuous(limits = c(0.2, 0.95))+
  scale_color_lancet()+
  stat_pvalue_manual(t_rtz_v1, y.position = 0.9,
                     label = "{p.signif} (z-scored)", hide.ns = FALSE) +
  labs(title = "RT t-test", x = "Task Demand", y = "Trial RT (s)", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


ACC
```{r}
df_acc_means_de_v1 <- 
  df_v1_good_TEST %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(ACCs = mean(Trial_ACC, na.rm = TRUE), .groups = "drop")

df_accz_means_de_v1 <- 
  df_v1_good_TEST %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(ACCZs = mean(ACCZ, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_acc_v1 <- 
  df_acc_means_de_v1 %>% 
  t_test(ACCs ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

t_accz_v1 <- 
  df_accz_means_de_v1 %>% 
  t_test(ACCZs ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

ggplot(data = df_acc_means_de_v1, aes(x = TaskDemand, y = ACCs, color = TaskDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_color_lancet()+
  stat_pvalue_manual(t_accz_v1, y.position = 1.02, 
                     label = "{p.signif}", hide.ns = TRUE) + 
  labs(title = "ACC t-test", x = "Task Demand", y = "Trial ACC", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


A measure
```{r}
df_a_means_de_v1 <- 
  df_v1_good_TEST %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(A_measures = mean(Trial_A_measure, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_a_v1 <- 
  df_a_means_de_v1 %>% 
  t_test(A_measures ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

ggplot(data = df_a_means_de_v1, aes(x = TaskDemand, y = A_measures, color = TaskDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_color_lancet()+
  stat_pvalue_manual(t_a_v1, y.position = 1.02, label = "{p.signif}", hide.ns = TRUE) + 
  labs(title = "A_measure t-test", x = "Task Demand", y = "Trial A_measure", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



## Check for Performance Difference between Transitions
compare the RT and ACC in repetition and switch trials
### pairwise comparison within each participant
Rts of Transitions within each participant
```{r}
pw_rt_v1 <- 
  df_v1_good_TEST %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID) %>% 
  t_test(Trial_RT ~ TrialTransition, paired = FALSE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "TrialTransition")
```

ACCs of Transitions within each participant
```{r}
pw_acc_v1 <- 
  df_v1_good_TEST %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID) %>% 
  t_test(Trial_ACC ~ TrialTransition, paired = FALSE, detailed = TRUE) %>%
  add_significance() %>% 
  add_xy_position(x = "TrialTransition")
```


### comparison the means of participants
RT
```{r}
df_rt_means_tr_v1 <- 
  df_v1_good_TEST %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID, TrialTransition) %>% 
  summarise(RTs = mean(Trial_RT, na.rm = TRUE), .groups = "drop")

df_rtz_means_tr_v1 <- 
  df_v1_good_TEST %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID, TrialTransition) %>% 
  summarise(RTZs = mean(RTZ, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_rt_v1 <- 
  df_rt_means_tr_v1 %>% 
  t_test(RTs ~ TrialTransition, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TrialTransition") %>% 
  add_significance()

t_rtz_v1 <- 
  df_rtz_means_tr_v1 %>% 
  t_test(RTZs ~ TrialTransition, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TrialTransition") %>% 
  add_significance()

ggplot(data = df_rt_means_tr_v1, aes(x = TrialTransition, y = RTs, color = TrialTransition)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_y_continuous(limits = c(0.3, 0.95)) + 
  scale_color_jco()+
  stat_pvalue_manual(t_rtz_v1, y.position = 0.9,
                     label = " {p.signif} (z-scored)", hide.ns = FALSE) +
  # stat_pvalue_manual(t_rt, label = " p.signif", hide.ns = TRUE) +
  labs(title = "RT t-test", x = "Trial Transition", y = "Trial RT", color = "Trial Transition") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


ACC
```{r}
df_acc_means_tr_v1 <- 
  df_v1_good_TEST %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID, TrialTransition) %>% 
  summarise(ACCs = mean(Trial_ACC, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_acc_v1 <- 
  df_acc_means_tr_v1 %>% 
  t_test(ACCs ~ TrialTransition, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TrialTransition") %>% 
  add_significance()

ggplot(data = df_acc_means_tr_v1, aes(x = TrialTransition, y = ACCs, color = TrialTransition)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  scale_color_jco()+
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  stat_pvalue_manual(t_acc_v1, y.position = 1.02,
                     label = "{p.signif}", hide.ns = FALSE) + 
  labs(title = "ACC t-test", x = "Trial Transition", y = "Trial ACC", color = "Trial Transition") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


A measure
```{r}
df_a_means_tr_v1 <- 
  df_v1_good_TEST %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID, TrialTransition) %>% 
  summarise(A_measures = mean(Trial_A_measure, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_a_v1 <- 
  df_a_means_tr_v1 %>% 
  t_test(A_measures ~ TrialTransition, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "Trial_Transition") %>% 
  add_significance()

ggplot(data = df_a_means_tr_v1, aes(x = TrialTransition, y = A_measures, color = TrialTransition)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  scale_color_jco()+
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  stat_pvalue_manual(t_a_v1, xmin = "group1", xmax = "group2", y.position = 1.02,
                     label = "{p.signif}", hide.ns = FALSE) + 
  labs(title = "A_measure t-test", x = "Trial Transition", y = "Trial A_measure", color = "Trial Transition") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




## Check for DemandRatings as Demand-Manipulation Check
```{r}
df_dr_means_de_v1 <- 
  df_v1_DEMANDRATING %>% 
  filter(!PID %in% excludedPIDs_v1) %>% 
  filter(!DemandCheck == "mid") %>% 
  group_by(PID, DemandCheck) %>% 
  summarise(DR_aves = mean(DemandRating, na.rm=TRUE),
            DR_aveZs = mean(DemandRatingZ, na.rm=TRUE),
            .groups = "drop")

t_dr_v1 <- 
  df_dr_means_de_v1 %>% 
  t_test(DR_aveZs ~ DemandCheck, comparisons = list("easy", "hard"), paired = TRUE) %>% 
  add_xy_position(x = "DemandCheck") %>% 
  add_significance()
```


boxplot
(higher rating = rated as more demanding)
```{r}
ggplot(data = df_dr_means_de_v1, aes(x = DemandCheck, y = DR_aves, color = DemandCheck)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  stat_pvalue_manual(t_dr_v1, y = 108, 
                     label = "{p.signif}", hide.ns = TRUE) + 
  scale_color_lancet() +
  labs(title = "DemandRating t-test", x = "TrialDemand", y = "DemandRating_ave", 
       color = "DemandCheck") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


The participant should be excluded due to rating 1-back as harder?
```{r}
df_dr_means_de_diff_v1 <- 
  df_dr_means_de_v1 %>% 
  select(-DR_aves) %>% 
  pivot_wider(names_from = DemandCheck, names_prefix = "DR_aveZs_" ,values_from = DR_aveZs) %>% 
  group_by(PID) %>% 
  mutate(DR_aveZs_diff = DR_aveZs_hard -DR_aveZs_easy) %>% 
  ungroup() %>% 
  mutate(DemandCheckType = ifelse(DR_aveZs_diff > 0, "3_as_harder", "1_as_harder")) 

ggplot(data = df_dr_means_de_diff_v1, aes(x="", y = DR_aveZs_diff, color=DemandCheckType, label=PID)) +
# ggplot(data = df_dr_means_de_diff_v1, aes(x="", y = DR_aveZs_diff, color=DemandCheckType)) +
  geom_boxplot(position="identity", width = 0.2, alpha = 0.9) +
  geom_jitter(width = 0.2) + 
  geom_label_repel(aes(color=DemandCheckType), max.overlaps = nrow(df_dr_means_de_diff_v1)) +
  scale_color_d3()+
  labs(title = "DemandRating_diff (hard-easy)", x = "", y = "DemandRating_aveZ_diff") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```






## Descriptive Analysis of Ratings
### histogram of OnlineRatingZ
```{r}
df_v1_good_TEST %>% 
  ggplot(aes(x = OnlineRatingZ)) +
  geom_histogram(position = position_dodge(width = 0.6), bins = 20, alpha = 0.8)+
  scale_x_continuous(breaks = seq(-5, 5)) +
  labs(title = "Ratings across participants", x = "Ratings(z-scored)", y = "Counts") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

separate distributions of different demands
```{r}
df_v1_good_TEST %>% 
  ggplot(aes(x = OnlineRatingZ, color  = TaskDemand, fill = TaskDemand)) +
  geom_histogram(position = position_dodge(width = 0.25), bins = 20, alpha = 0.7)+
  scale_x_continuous(breaks = seq(-5, 5)) +
  scale_color_lancet() + 
  scale_fill_lancet() + 
  labs(title = "Ratings across participants", x = "Ratings(z-scored)", y = "Counts", fill = "Task Demand") + 
  guides(color = "none") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



### Plots ~ Task Demand (easy / hard)
boxplot of raw OnlineRatings (means of participants)
```{r}
df_ra_means_de_v1 <- 
  df_v1_good_TEST %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(ISI = mean(ISI, na.rm=TRUE),
    OnlineRatings = mean(OnlineRating, na.rm = TRUE), 
            .groups = "drop") %>% 
  group_by(PID) %>% 
  mutate(OnlineRatings_diff = 
           OnlineRatings[TaskDemand == "easy"] - OnlineRatings[TaskDemand == "hard"],
         avoid = ifelse(OnlineRatings_diff > 0, "demand_aversive", "no_demand_aversive")) %>% 
  ungroup()
```

```{r}
t_ra_v1 <- 
  df_ra_means_de_v1 %>%
  t_test(OnlineRatings ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "TaskDemand")


df_ra_means_de_v1 %>%
  ggplot(aes(x = TaskDemand, y = OnlineRatings, color = TaskDemand)) + 
  geom_boxplot(position = "identity", alpha = 0.7, width = 0.5, outlier.color = "black") +
  stat_boxplot(geom = "errorbar", width = 0.15) +
  geom_jitter(width = 0.15) + 
  scale_color_lancet() + 
  stat_pvalue_manual(t_ra_v1, y.position = 95, label = "p = {p}{p.signif}") +
  labs(title = "OnlineRating t-test", x = "Task Demand", y = "Rating(raw)", 
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
  
```

boxplot of OnlineRatingZ (means of participants)
```{r}
df_ra_means_de_v1 <- 
  df_v1_good_TEST %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(
    ISI = mean(ISI, na.rm=TRUE),
    CueMapping = first(CueMapping, na.rm = TRUE),
    Gender = first(Gender, na.rm = TRUE),
    OnlineRatingZs = mean(OnlineRatingZ, na.rm = TRUE), 
            .groups = "drop") %>% 
  group_by(PID) %>% 
  mutate(OnlineRatingZs_diff = 
           OnlineRatingZs[TaskDemand == "easy"] - OnlineRatingZs[TaskDemand == "hard"],
         avoid = ifelse(OnlineRatingZs_diff > 0, "demand_aversive", "no_demand_aversive")) %>% 
  ungroup()
```


```{r}
t_ra_v1 <- 
  df_ra_means_de_v1 %>%
  t_test(OnlineRatingZs ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "TaskDemand")


df_ra_means_de_v1 %>%
  ggplot(aes(x = TaskDemand, y = OnlineRatingZs, color = TaskDemand)) + 
  geom_boxplot(position = "identity", alpha = 0.7, width = 0.5, outlier.color = "black") +
  stat_boxplot(geom = "errorbar", width = 0.15) +
  geom_jitter(width = 0.15) + 
  scale_color_lancet() + 
  stat_pvalue_manual(t_ra_v1, y.position = 1.1, label = "p = {p}{p.signif}") +
  labs(title = "OnlineRating t-test", x = "Task Demand", y = "Rating Means (z-scored)", 
       color = "TaskDemand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
  
```

RatingZ-pairs of each participant in different demands
```{r}
df_ra_means_de_v1 %>% 
  ggpaired(x = "TaskDemand", y = "OnlineRatingZs", id="PID",
           color = "TaskDemand", line.color = "grey", palette = "lancet") +
  # stat_pvalue_manual(t_ra_v1, label = "p = {p}{p.signif}")+
  stat_compare_means(method = "t.test", paired = TRUE, label =  "p.format", label.x = 1.5) +
  labs(title = "Rating pairs", x = "Task Demand", y = "Rating(z-scored)", 
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
df_ra_means_de_v1 %>% 
  ggplot(aes(x="", y=OnlineRatingZs_diff))+
  geom_boxplot(width = 0.4, outlier.color="red")+
  geom_jitter(aes(color=avoid), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")+
  scale_color_d3()+
  labs(title = "Rating Diff (easy - hard)", x = "", y = "Rating(z-scored)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
table(df_ra_means_de_v1$avoid[df_ra_means_de_v1$TaskDemand == "easy"])
```




Cohen's d effect size
```{r}
# across participants
cohen_d_ra_de_v1 <- cohen.d(OnlineRatingZ ~ TaskDemand | Subject(PID), paired = TRUE, 
                               data = df_v1_good_TEST)
cohen_d_ra_de_v1
```

```{r}
# means of participants
cohen_d_ra_means_de_v1 <- cohen.d(OnlineRatingZs ~ TaskDemand | Subject(PID), paired = TRUE, 
                               data = df_ra_means_de)
cohen_d_ra_means_de_v1
```



line plot of means by trial (points are means of each participant)
```{r}
df_means_by_trial_v1 <- 
  df_v1_good_TEST %>%
  # filter(TrialN != 1) %>%
  group_by(PID, TaskDemand, TrialIndex) %>% 
  summarise(OnlineRatingZ = mean(OnlineRatingZ, na.rm=TRUE),
            RTZ = mean(RTZ, na.rm=TRUE),
            ACCZ = mean(ACCZ, na.rm=TRUE,),
            .groups = "drop") 

ggplot(data = df_means_by_trial_v1, 
       aes(x = TrialIndex, y = OnlineRatingZ, group = TrialIndex, 
             color = TaskDemand, fill = TaskDemand)) +
  # geom_rect(xmin = 50.45, xmax = 50.55, ymin = -Inf, ymax = Inf,
  #           color = "gray", fill = "gray", alpha = 0.2) +
  geom_boxplot(width = 0.8, position = position_dodge(width = 0.8), 
               alpha = 0.5, outlier.color = "black") + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  geom_jitter(shape = 1) +
  stat_summary(fun = mean, geom = "line", aes(group = TaskDemand)) +
  stat_summary(fun = mean, geom = "point", aes(group = TaskDemand, color = TaskDemand)) +
  scale_x_continuous(limits = c(0, 51), breaks = seq(1, 50, 1))+
  scale_color_lancet() + 
  scale_fill_lancet() + 
  labs(title = "Ratings by Trial (means across participants)", x = "Trial Index", y = "Rating (z-scored)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5) )
```


lines of each participant (not necessary)
```{r}
ggplot(data = df_v1_good_TEST, aes(x = TrialIndex, y = OnlineRatingZ, 
                                group = interaction(PID, TaskDemand), 
             color = factor(PID))) +
  # scale_color_lancet() + 
  geom_rect(xmin = 50.45, xmax = 50.55, ymin = -Inf, ymax = Inf,
            color = "gray", fill = "gray", alpha = 0.2) +
  geom_point(aes(shape = TaskDemand)) +
  geom_line(aes(linetype = TaskDemand)) + 
  scale_x_continuous(limits = c(1, 51), breaks = seq(1, 50, 1))+
  labs(title = "Ratings by Trial (means within participant)", 
       x = "Trial Index", y = "Rating (z-scored)", color = "PID") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```




trial-by-trial ratings plot by TaskDemand (grandmeans of participants' means)
```{r}
df_grandmeans_by_trial_v1 <- 
  df_means_by_trial_v1 %>%
  group_by(TaskDemand, TrialIndex) %>% 
  summarise(OnlineRatingZ = mean(OnlineRatingZ, na.rm=TRUE),
            RTZ = mean(RTZ, na.rm=TRUE),
            ACCZ = mean(ACCZ, na.rm=TRUE,),
            .groups = "drop")

ggplot(data = df_grandmeans_by_trial_v1, aes(x = TrialIndex, y = OnlineRatingZ, group = TrialIndex, 
             color = TaskDemand, fill = TaskDemand)) +
  # geom_rect(xmin = 50.45, xmax = 50.55, ymin = -Inf, ymax = Inf,
  #           color = "gray", fill = "gray", alpha = 0.2) +
  # geom_boxplot(width = 0.8, position = position_dodge(width = 0.8), 
  #              alpha = 0.5, outlier.color = "black") + 
  # stat_boxplot(geom = "errorbar", width = 0.15) + 
  # geom_jitter(shape = 1) +
  geom_point()+
  stat_summary(fun = mean, geom = "line", aes(group = TaskDemand)) +
  stat_summary(fun = mean, geom = "point", aes(group = TaskDemand, color = TaskDemand)) +
  scale_x_continuous(limits = c(0, 51), breaks = seq(1, 50, 1))+
  scale_color_lancet() + 
  scale_fill_lancet() + 
  labs(title = "Ratings by Trial (grandmeans of participants)", x = "Trial N", y = "Rating (z-scored)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5) )
```




### difference between means of easy vs hard Trial_Demand of each 10 trials 
```{r}
# review the number of easy and hard trials in each trial section
# it turns out to be relatively balanced

result_table <- 
  df_v1_good_TEST %>%
  mutate(Trial_Index_Group = cut(TrialIndex, breaks = c(1, 10, 20, 30, 40, 50), labels = FALSE)) %>%
  group_by(PID, Trial_Index_Group, TaskDemand) %>%
  summarise(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = TaskDemand, values_from = count, values_fill = 0)

# Print the resulting table
# print(result_table)

```


### lines plot of ratingZ in easy and hard demands
```{r}
# Create a function to calculate mean for each section of trials
calculate_section_means <- function(data, col_reps, section) {
  data %>%
    group_by(PID, TaskDemand) %>%
    summarise(across(starts_with(col_reps),
                     ~ mean(.[TrialIndex %in% section], na.rm = TRUE),
                     .names = "OnlineRatingZ_mean"), .groups = "drop")
}

# Define the sections
trial_sections <- list("10" = 1:10, "20" = 11:20, "30" = 21:30, "40" = 31:40, "50" = 41:50)
# trial_sections <- list("05" = 1:5, "10" = 6:10, 
#                        "15" = 11:15, "20" = 16:20, 
#                        "25" = 21:25, "30" = 26:30, 
#                        "35" = 31:35, "40" = 36:40, 
#                        "45" = 41:45, "50" = 46:50)

# Initialize an empty list to store the results
result_list <- list()

# Use lapply to iterate over each section and calculate means
result_list <- lapply(names(trial_sections), function(section_name) {
  section <- trial_sections[[section_name]]
  
  # Calculate means for the current section
  result <- calculate_section_means(df_v1_good_TEST, "OnlineRatingZ", section)
  
  # Add a column indicating the section name
  result$Section <- section_name
  
  return(result)
})

# Combine the results into a single data frame
df_ra_means_demands_tsecs_v1 <- bind_rows(result_list)

df_ra_grandmeans_demands_tsecs_v1 <-
  df_ra_means_demands_tsecs_v1 %>% 
  group_by(Section, TaskDemand) %>% 
  summarise(OnlineRatingZ_gmean = mean(OnlineRatingZ_mean, na.rm = TRUE), .groups = "drop") 
```


```{r}
t_ra_set_v1 <- 
  df_ra_means_demands_tsecs_v1 %>% 
  group_by(Section) %>% 
  t_test(OnlineRatingZ_mean ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "Section")
```


```{r}
df_ra_means_demands_tsecs_v1 %>% 
  ggplot(aes(x = Section, y = OnlineRatingZ_mean,color = TaskDemand)) + 
  geom_boxplot(position = "dodge2", 
               alpha = 0.7, width = 0.5, outlier.color = "black") +
  geom_point(position = position_dodge(width = 0.5), alpha = 0.5) +
  scale_color_lancet() +
  # scale_fill_lancet() +
  # stat_summary(fun = mean, geom = "point", shape = 17, color = "black", size = 2) +
  # stat_compare_means(label = "p.signif", paired = TRUE, hide.ns = FALSE, label.y = 2.2) + 
  stat_pvalue_manual(t_ra_set_v1, label = "{p.signif}", hide.ns = FALSE,
                      tip.length = 0, linetype = "blank") +
  labs(title = "OnlineRating t-test by Section", x = "Task Demand", y = "Rating(z-scored)", 
       color = "TaskDemand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
# Plot
df_ra_grandmeans_demands_tsecs_v1 %>% 
  ggplot(aes(x = Section, y = OnlineRatingZ_gmean, 
             group = TaskDemand, color = TaskDemand)) +
  geom_point() +
  geom_line() +
  scale_color_lancet() +
  stat_pvalue_manual(t_ra_set_v1, label = "p = {p}{p.signif}", hide.ns = FALSE,
                     y.position = c(0.32, 0.28, 0.23, 0.19, 0.15),
                      tip.length = 0, linetype = "blank") +
  scale_y_continuous(limits = c(-0.6, 0.4)) + 
  labs(title = "Ratings by Trial Sections", x = "Trial Section", y = "Rating Mean (z-scored)",
       color = "Task Demand") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```




### line plot of rating difference between demands of each 10 trials
```{r}
df_ra_means_diffs_10ts_v1 <-
  df_ra_means_demands_tsecs_v1 %>% 
  group_by(PID) %>% 
  pivot_wider(names_from = TaskDemand, values_from = OnlineRatingZ_mean,
              names_glue = "OnlineRatingZ_mean_{TaskDemand}") %>% 
  group_by(PID, Section) %>%   
  summarise(OnlineRatingZ_diff = OnlineRatingZ_mean_easy - OnlineRatingZ_mean_hard,
              .groups = "drop")

df_ra_grandmeans_diffs_10ts_v1 <- 
  df_ra_means_diffs_10ts_v1 %>% 
  group_by(Section) %>% 
  summarise(OnlineRatingZ_easy = mean(OnlineRatingZ_easy, na.rm = TRUE),
            OnlineRatingZ_hard = mean(OnlineRatingZ_hard, na.rm = TRUE),
            OnlineRatingZ_diff = mean(OnlineRatingZ_diff, na.rm = TRUE),
            .groups = "drop")
```

```{r}
df_ra_means_diffs_10ts_v1_wide <- 
  df_ra_means_diffs_10ts_v1 %>% 
  pivot_wider(names_from = "Section", values_from = "OnlineRatingZ_diff", 
              names_prefix = "OnlineRatingZ_diff_")
```


line plot
```{r}
df_ra_grandmeans_diffs_10ts_v1 %>% 
  mutate(TrialSection = as.numeric(Section)) %>% 
  ggplot(aes(x = TrialSection, y = OnlineRatingZ_diff)) +
  geom_point()+
  geom_line()+
  labs(title = "Rating_Diffs by Trial sections", x = "Trial Section", y = "Rating_Diff (z-scored)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```



### Trial Transition of Previous Trial (repeat / switch)
boxplot (means of participants)
```{r}
df_ra_means_trpre_v1 <- 
  df_v1_good_TEST %>% 
  filter(!is.na(TrialTransition_pre)) %>% 
  group_by(PID, TrialTransition_pre) %>% 
  summarise(OnlineRatingZs = mean(OnlineRatingZ, na.rm = TRUE), .groups = "drop")

t_ra_v1 <-
  df_ra_means_trpre_v1 %>%
  t_test(OnlineRatingZs ~ TrialTransition_pre, paired = TRUE, detailed = TRUE) %>%
  # adjust_pvalue(method = "bonferroni") %>%
  add_significance() %>% 
  add_x_position(x = "TrialTransition_pre")

df_ra_means_trpre_v1 %>% 
  ggplot(aes(x = TrialTransition_pre, y = OnlineRatingZs, color = TrialTransition_pre)) + 
  geom_boxplot(alpha = 0.7, width = 0.45, outlier.color = "black", 
               position = "identity") +
  stat_boxplot(geom = "errorbar", width = 0.15, 
               position = position_dodge(width = 0.5)) + 
  geom_jitter(shape = 1) + 
  stat_pvalue_manual(t_ra_v1, xmin = "group1", xmax = "group2",
                     y.position = 0.43, label = "p.signif", hide.ns = FALSE) +
  scale_color_jco() + 
  labs(title = "Rating t-test", x = "TrialTransition_pre", y = "Rating (z-scored)", 
       color = "Trial Transition_pre") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
df_ra_means_trpre_v1 %>% 
  ggpaired(x = "TrialTransition_pre", y = "OnlineRatingZs", id = "PID",
           color = "TrialTransition_pre", line.color = "grey", 
           palette = "jco") + 
  stat_pvalue_manual(t_ra_v1, xmin = "group1", xmax = "group2",
                     y.position = 0.43, label = "p.signif", hide.ns = FALSE) +
  labs(title = "Rating pairs", x = "Trial Transition_pre", y = "Rating(z-scored)", 
       color = "Trial Transition_pre") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




### Trial_Demand*Trial_Transition_pre
dataframe: mean of ratings
```{r}
df_means_on_de_trpre_v1 <- 
  df_v1_good_TEST %>% 
  filter(!is.na(TrialTransition_pre)) %>%
  group_by(PID, TaskDemand, TrialTransition_pre) %>% 
  summarise(OnlineRatingZ = mean(OnlineRatingZ, na.rm=TRUE),
            RTZ = mean(RTZ, na.rm=TRUE),
            ACCZ = mean(ACCZ, na.rm=TRUE,),
            .groups = "drop")
```


boxplot (grandmean of participants)
```{r}
df_ra_means_de_trpre_v1 <- 
  df_v1_good_TEST %>% 
  filter(!is.na(TrialTransition_pre)) %>%
  group_by(PID, TaskDemand, TrialTransition_pre) %>% 
  summarise(OnlineRatingZs = mean(OnlineRatingZ, na.rm = TRUE), .groups = "drop")

t_ra_v1 <-
  df_ra_means_de_trpre_v1 %>%
  group_by(TrialTransition_pre) %>% 
  t_test(OnlineRatingZs ~ TaskDemand, paired = TRUE, detailed = TRUE) %>%
  add_significance() %>% 
  add_xy_position(x = "TrialTransition_pre")

df_ra_means_de_trpre_v1 %>% 
  ggplot(aes(x = TrialTransition_pre, y = OnlineRatingZs, color = TaskDemand)) + 
  geom_boxplot(alpha = 0.7, width = 0.45, outlier.color = "black", 
               position = position_dodge(width = 0.7)) +
  stat_boxplot(geom = "errorbar", width = 0.15, 
               position = position_dodge(width = 0.7)) + 
  geom_jitter(shape = 1, position = position_dodge(width = 0.7)) + 
  scale_color_lancet() + 
  stat_pvalue_manual(t_ra_v1, label = "p.signif",
                     y.position = c(1.2, 1.3), hide.ns = TRUE) +
  labs(title = "Rating t-test", x = "Trial Transition_pre", y = "Rating(z-scored)", 
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




## Offline Ratings
```{r}
df_off_ra_means_de_v1 <- 
  df_v1_good_OFFLINERATING %>% 
  filter(!PID %in% excludedPIDs_v1) %>% 
  group_by(PID, OfflineRatingTimePoint, TrialDemand) %>% 
  summarise(OfflineRatingZs = mean(OfflineRatingZ, na.rm=TRUE), .groups = "drop")
```

```{r}
df_off_ra_means_de_times_v1 <- 
  df_off_ra_means_de_v1 %>% 
  pivot_wider(names_from = OfflineRatingTimePoint, values_from = OfflineRatingZs,
              names_glue = "OfflineRating_{OfflineRatingTimePoint}Zs")
```

boxplot
```{r}
t_off_ra_de_v1 <- 
  df_off_ra_means_de_v1 %>% 
  filter(!OfflineRatingTimePoint == "T3") %>% 
  filter(!PID == 22) %>% 
  group_by(OfflineRatingTimePoint) %>% 
  t_test(OfflineRatingZs ~ TrialDemand, paired = TRUE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "OfflineRatingTimePoint")
```


```{r}
df_off_ra_means_de_v1 %>% 
  filter(!OfflineRatingTimePoint == "T3") %>% 
  filter(!PID == 22) %>% 
  ggplot(aes(x = OfflineRatingTimePoint, y = OfflineRatingZs, color = TrialDemand)) +
  geom_boxplot(width = 0.5, position = position_dodge(width = 0.8)) +
  stat_boxplot(geom = "errorbar", width = 0.15, position = position_dodge(width = 0.8)) +
  geom_point( position = position_dodge(width = 0.8), alpha = 0.7) + 
  stat_pvalue_manual(t_off_ra_de_v1, hide.ns = FALSE) + 
  scale_color_lancet() + 
  labs(title = "Offline Rating T-test on Task Demand", x = "Time Point", color = "Task Demand") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```



## Choice
```{r}
df_v1_good_CHOICE <- 
  df_v1_good_CHOICE %>% 
  mutate(DemandChoice_code = ifelse(ChoiceDemand == "easy", 1, 0))
```

```{r}
df_dc_props_easy_v1 <- 
  df_v1_good_CHOICE %>% 
  group_by(PID) %>% 
  summarise(DEC_prop = mean(DemandChoice_code, na.rm=TRUE), .groups = "drop") %>% 
  mutate(DemandAvoidance = ifelse(DEC_prop > 0.5, "demand_avoid", "no_demand_avoid"))
```

```{r}
table(df_dc_props_easy_v1$DemandAvoidance)
```

10 participants' easy choice rates < 0.5 (no demand avoidance)
```{r}
# ggplot(df_dc_props_easy, aes(x="", y = DCERs, label=PID)) +
ggplot(df_dc_props_easy_v1, aes(x="", y = DEC_prop)) +
  geom_boxplot(position="identity",outlier.color = "black",width = 0.4) +
  geom_point(aes(color=DemandAvoidance), position = position_dodge2(width = 0.15), alpha = 0.7) +
  geom_hline(yintercept = 0.5,linetype = "dashed", color = "red") + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  # geom_text_repel(aes(color=DemandAvoidance), max.overlaps = nrow(df_dc_props_easy)) +
  scale_color_d3()+
  labs(title = "Easy Choice Rates", x="", y = "Rate") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```


### Choice curve
```{r}
df_v1_good_CHOICE_mean <- 
  df_v1_good_CHOICE %>% 
  group_by(TrialN) %>% 
  summarise(DEC_parti_prop = mean(DemandChoice_code))
```

```{r}
df_v1_good_CHOICE_mean %>% 
  ggplot(aes(x = TrialN, y = DEC_parti_prop)) +
  geom_point()+
  geom_line()+
  scale_x_continuous(breaks = seq(1, 10)) +
  labs(title = "Easy Choice Participant Proportion (mean) ", x="TrialN", y = "Proportion") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```


```{r}
df_v1_good_CHOICE_count <- 
  df_v1_good_CHOICE %>% 
  group_by(TrialN, ChoiceDemand) %>% 
  summarise(DEC_parti_count = n(), .groups = "drop")
```

```{r}
df_v1_good_CHOICE_count %>% 
  ggplot(aes(x = TrialN, y = DEC_parti_count, color=ChoiceDemand)) +
  geom_point()+
  geom_line()+
  scale_x_continuous(breaks = seq(1, 10)) +
  scale_color_lancet()+
  labs(title = "Amount of Participants Choosing Easy/Hard", x="TrialN", y = "Count") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```




## Correlations
create dataframes
```{r}
df_ra_means_de_diffs_v1 <- 
  df_ra_means_de_v1 %>% 
  group_by(PID) %>% 
  summarise(
    ISI = mean(ISI, na.rm=TRUE),
    CueMapping = first(CueMapping, na.rm = TRUE),
    Gender = first(Gender, na.rm = TRUE),
    OnlineRatingZs_diff = OnlineRatingZs[TaskDemand == "easy"] - OnlineRatingZs[TaskDemand == "hard"], .groups = "drop") 
```

```{r}
df_ra_dc_v1 <- 
  merge(df_ra_means_de_diffs_v1, df_ra_means_diffs_10ts_v1_wide, by="PID", all = TRUE) %>% 
  merge(df_dc_props_easy_v1, by="PID", all = TRUE)
```

```{r}
df_off_ra_means_de_diffs_v1 <- 
  df_off_ra_means_de_times_v1 %>% 
  group_by(PID) %>% 
  summarise(
    OfflineRating_T1Zs_diff = OfflineRating_T1Zs[TrialDemand == "easy"] - OfflineRating_T1Zs[TrialDemand == "hard"], 
    OfflineRating_T2Zs_diff = OfflineRating_T2Zs[TrialDemand == "easy"] - OfflineRating_T2Zs[TrialDemand == "hard"], 
    .groups = "drop") 
```

```{r}
df_ra_on_off_v1 <- 
  merge(df_ra_means_de_diffs_v1, df_ra_means_diffs_10ts_v1_wide, by="PID", all = TRUE) %>% 
  merge(df_off_ra_means_de_diffs_v1, by="PID", all = TRUE)
```


```{r}
df_cors_v1 <- 
  merge(df_ra_means_de_diffs_v1, df_ra_means_diffs_10ts_v1_wide, by="PID", all = TRUE) %>% 
  merge(df_off_ra_means_de_diffs_v1, by="PID", all = TRUE) %>% 
  merge(df_dc_props_easy_v1, by="PID", all=TRUE)

df_cors_pure_v1 <- 
  merge(df_ra_means_de_diffs_v1, df_ra_means_diffs_10ts_v1_wide, by="PID", all = TRUE) %>% 
  merge(df_off_ra_means_de_diffs_v1, by="PID", all = TRUE) %>% 
  merge(df_dc_props_easy_v1, by="PID", all=TRUE) %>% 
  select(-c(PID, ISI, DemandAvoidance))
```



correlation matrix output
```{r}
ra_dc_cors_matrix_v1 <- rcorr(as.matrix(df_cors_pure_v1))
write.csv(ra_dc_cors_matrix_v1$r, 
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/cors_matrix_r.csv")
write.csv(ra_dc_cors_matrix_v1[["P"]], 
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/cors_matrix_p.csv")
```

```{r}
ra_dc_cors_v1 <- cor(df_cors_pure_v1)
print(ra_dc_cors_v1)
write.csv(ra_dc_cors_v1, 
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/cors.csv")
```


```{r}
print("Online ~ Choice")
bf <- correlationBF(df_cors_v1$OnlineRatingZs_diff, df_cors_v1$DEC_prop)
bf

print("Offline_T1 ~ Choice")
bf <- correlationBF(df_cors_v1$OfflineRating_T1Zs_diff, df_cors_v1$DEC_prop)
bf

print("Offline_T2 ~ Choice")
bf <- correlationBF(df_cors_v1$OfflineRating_T2Zs_diff, df_cors_v1$DEC_prop)
bf

# samples <- posterior(bf, iterations = 1000)
# summary(samples)
```



Scatter plots
```{r}
# ggplot(df_cors_v1, aes(x = OnlineRatingZs_diff, y = OfflineRating_T1Zs_diff, label = PID)) +
ggplot(df_cors_v1, aes(x = OnlineRatingZs_diff, y = OfflineRating_T1Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v1)) + 
  labs(title = "Correlation Online & Offline Rating T1", 
       x = "Online Rating Diff (easy - hard)", y = "Offline Rating T1 Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_v1, aes(x = OnlineRatingZs_diff, y = OfflineRating_T2Zs_diff, label = PID)) +
ggplot(df_cors_v1, aes(x = OnlineRatingZs_diff, y = OfflineRating_T2Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v1)) + 
  labs(title = "Correlation Online & Offline Rating T2", 
       x = "Online Rating Diff (easy - hard)", y = "Offline Rating T2 Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_v1, aes(x = OfflineRating_T1Zs_diff, y = OfflineRating_T2Zs_diff, label = PID)) +
ggplot(df_cors_v1, aes(x = OfflineRating_T1Zs_diff, y = OfflineRating_T2Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v1)) + 
  labs(title = "Correlation Offline Rating T1 & T2", 
       x = "Offline Rating T1 Diff (easy - hard)", y = "Offline Rating T2 Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_v1, aes(x = OnlineRatingZs_diff, y = DCERs, label = PID)) +
ggplot(df_cors_v1, aes(x = OnlineRatingZs_diff, y = DEC_prop)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v1)) + 
  labs(title = "Correlation Online Rating & Choice", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v1, aes(x = OnlineRatingZ_diff_50, y = DCERs, label = PID)) +
ggplot(df_cors_v1, aes(x = OnlineRatingZ_diff_50, y = DEC_prop)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v1)) + 
  labs(title = "Correlation Online Rating in 40-50 Trial Section & Choice", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v1, aes(x = OfflineRating_T1Zs_diff, y = DCERs, label = PID)) +
ggplot(df_cors_v1, aes(x = OfflineRating_T1Zs_diff, y = DEC_prop)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v1)) + 
  labs(title = "Correlation Offline Rating T1 & Choice", 
       x = "Offline Rating T1 Diff (easy - hard)", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v1, aes(x = OfflineRating_T2Zs_diff, y = DCERs, label = PID)) +
ggplot(df_cors_v1, aes(x = OfflineRating_T2Zs_diff, y = DEC_prop)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v1)) + 
  labs(title = "Correlation Choice & Offline Rating T2", 
       x = "Offline Rating T2 Diff (easy - hard)", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




## Regressions
### Y: OnlineRatingZ
#### Fixed Models (Mean across participants)
X: Trial Demand, Trial Transition, Trial_A_measure, Trial_RT, RatingRT, TrialN
```{r}
reg.m0 <- 
  df_ra_means_de %>%  
  lm(OnlineRatingZs ~ TaskDemand, data = .)
summary(reg.m0)
```


Bootstrap Resampling
```{r}
boot_lm_function <- function(data, indices) {
  sampled_data <- data[indices, ]
  model <- lm(OnlineRatingZs ~ TaskDemand, data = sampled_data)
  return(coef(model))
}

# Perform bootstrap resampling
bootstrap_results <- boot::boot(data = df_ra_means_de, statistic = boot_lm_function, R = 5000)

# Display the bootstrap results
print(bootstrap_results)
boot::boot.ci(boot.out = bootstrap_results, type = "bca")
```


```{r}
df_ra_means_mult <- 
  df_v1_good_TEST %>% 
  group_by(PID, TaskDemand, TrialDemandLearning_pre, TrialTransition, TrialTransition_pre) %>% 
  summarise(OnlineRatingZs = mean(OnlineRatingZ, na.rm = TRUE), 
            RTZs = mean(RTZ, na.rm = TRUE),
            ACCZs = mean(ACCZ, na.rm = TRUE),
            Trial_A_measures = mean(Trial_A_measure, na.rm = TRUE),
            RT_preZs = mean(RT_preZ, na.rm = TRUE),
            ACC_preZs = mean(RT_preZ, na.rm = TRUE),
            Trial_A_measure_pres = mean(Trial_A_measure_pre, na.rm = TRUE),
            .groups = "drop")


print("Model1: main effects")
reg.m0 <-
  df_ra_means_mult %>%
  lm(OnlineRatingZs ~ TaskDemand + TrialDemandLearning_pre +
       TrialTransition + TrialTransition_pre +
       RT_preZs + ACC_preZs + Trial_A_measure_pres, data = .)
summary(reg.m0)


print("Model2: main + interaction effects")
reg.m0 <-
  df_ra_means_mult %>%
  lm(OnlineRatingZs ~ TaskDemand * TrialDemandLearning_pre * TrialTransition_pre +
       RT_preZs + ACC_preZs + Trial_A_measure_pres, data = .)
summary(reg.m0)
```



```{r}
# how to explain the Rating difference on current transition?
reg.m0 <- 
  df_ra_means_mult %>%
  lm(OnlineRatingZs ~ TaskDemand*TrialTransition, data = .)
summary(reg.m0)
```


Rating had no effect on trial performance.
```{r}
reg.m0 <- 
  df_ra_means_mult %>%
  lm(RTZs ~ OnlineRatingZs, data = .)
summary(reg.m0)
```

Rating had seeming effect on ACC and A_measure, but the effect actually came from task demand.
```{r}
reg.m0 <- 
  df_ra_means_mult %>%
  lm(ACCZs ~ OnlineRatingZs*TaskDemand, data = .)
summary(reg.m0)
```

```{r}
reg.m0 <- 
  df_ra_means_mult %>%
  lm(Trial_A_measures ~ OnlineRatingZs*TaskDemand, data = .)
summary(reg.m0)
```





#### Mixed-effect Models 
Overall
```{r}
reg.m1.1_lmerTest <- 
  df_v1_good_TEST %>% 
  lmerTest::lmer(OnlineRatingZ ~ TaskDemand*TrialN + RT_preZ + ACC_preZ + 
                   (0 + TaskDemand|TrialN)+
                   (1 + TaskDemand|TrialDemandLearning_pre) + (1 + TaskDemand|TrialTransition_pre)+ 
                   (1 | PID) + (1 | ISI) + (1 | CueMapping) + (1 | Gender), 
          data = .)
# summary(reg.m1.1_lmerTest)
```

```{r}
reg.m1.2_lmerTest <- 
  df_v1_good_TEST %>% 
  filter(TrialN %in% c(seq(30, 50))) %>% 
  lmerTest::lmer(OnlineRatingZ ~ TaskDemand*TrialN + RT_preZ + ACC_preZ + 
                   (0 + TaskDemand|TrialN) +
                   (1 + TaskDemand|TrialDemandLearning_pre) + (1 + TaskDemand|TrialTransition_pre)+ 
                   (1| PID) + (1 | ISI) + (1 | CueMapping) + (1 | Gender), 
          data = .)
# summary(reg.m1.2_lmerTest)
```

```{r}
reg.m1.3_lmerTest <- 
  df_v1_good_TEST %>% 
  filter(TrialN %in% c(seq(20, 50))) %>% 
  lmerTest::lmer(OnlineRatingZ ~ TaskDemand*TrialN + RT_preZ + ACC_preZ + 
                   (0 + TaskDemand|TrialN) + 
                   (1 + TaskDemand|TrialDemandLearning_pre) + (1 + TaskDemand|TrialTransition_pre)+ 
                   (1| PID) + (1 | ISI) + (1 | CueMapping) + (1 | Gender), 
          data = .)
# summary(reg.m1.3_lmerTest)
```

```{r}
tab_model(reg.m1.1_lmerTest, reg.m1.2_lmerTest, reg.m1.3_lmerTest,
          show.re.var = FALSE, show.ngroups = FALSE,
          dv.labels = c("OnlineRatingZ", 
                        "OnlineRatingZ in 40-50 Trial Section",
                        "OnlineRatingZ in 20-50 Trial Section"),
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/reg_m1.html")
```


#### Compare Models
(AIC counts more on model fitness; BIC counts more on model simplicity. Smaller value, better model)
```{r}
AIC(logLik(reg.m1.1_lmerTest))
AIC(logLik(reg.m1.2_lmerTest))
AIC(logLik(reg.m1.3_lmerTest))
```

```{r}
# higher penalty on complex models than AIC
BIC(logLik(reg.m1.1_lmerTest))
BIC(logLik(reg.m1.2_lmerTest))
BIC(logLik(reg.m1.3_lmerTest))
```

#### Model optimization
```{r}
# lmer4 package, try all optimizers
reg.m1.2_lmerTest_all <- allFit(reg.m1.2_lmerTest)
summary(reg.m1.2_lmerTest_all)
```

```{r}
ss <- getME(reg.m1.2_lmerTest, c("theta"))
reg.m1.2_lmerTest_nloptwrap <- 
  df_v1_good_TEST %>%  
  lmerTest::lmer(OnlineRatingZ ~ TaskDemand + TrialTransition_pre + RT_preZ + ACC_preZ + 
          (1 + TaskDemand | PID) + (1 | ISI), data = .,
       start = ss, control = lmerControl(optimizer = "nloptwrap"))
summary(reg.m1.2_lmerTest_nloptwrap)
```


interaction plot
```{r}
plot_model(reg.m1.1_lmerTest, type = "pred", terms = c("TrialN", "TaskDemand"),
           colors = c("#00468BFF", "#ED0000FF"),
           # show.data = TRUE, jitter = 0.5, 
           show.values = TRUE, show.p = TRUE)+
  labs(title = "Regression: Online Rating ~ TaskDemand*TrialN", 
       x = "TrialN",
       y = "Online Rating (z-scored)",
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



### Y: Choice Rate
Online Rating
```{r}
reg.m2.1_lmerTest <- 
  df_cors_v1 %>% 
  lmerTest::lmer(DCERs ~ OnlineRatingZs_diff + 
                   (1 | ISI) + (1 | CueMapping) + (1 | Gender), data=.)
# summary(reg.m2.1_lmerTest)
```

```{r}
reg.m2.2_lmerTest <- 
  df_cors_v1 %>% 
  lmerTest::lmer(DCERs ~ OnlineRatingZ_diff_50 + 
                   (1 | ISI) + (1 | CueMapping) + (1 | Gender), data=.)
# summary(reg.m2.2_lmerTest)
```

compare models
(AIC counts more on model fitness; BIC counts more on model simplicity. Smaller value, better model)
```{r}
AIC(logLik(reg.m2.1_lmerTest))
AIC(logLik(reg.m2.2_lmerTest))
```

```{r}
# higher penalty on complex models than AIC
BIC(logLik(reg.m2.1_lmerTest))
BIC(logLik(reg.m2.2_lmerTest))
```



Offline Ratings
```{r}
reg.m2.3_lmerTest <- 
  df_cors_v1 %>% 
  lmerTest::lmer(DCERs ~ OfflineRating_T1Zs_diff +
                   (1 | ISI) + (1 | CueMapping) + (1 | Gender), data=.)
# summary(reg.m2.3_lmerTest)
```

```{r}
tab_model(reg.m2.1_lmerTest, reg.m2.2_lmerTest, reg.m2.3_lmerTest, 
          show.re.var = FALSE, show.ngroups = FALSE,
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/reg_m2.html")
```

compare models
(AIC counts more on model fitness; BIC counts more on model simplicity. Smaller value, better model)
```{r}
AIC(logLik(reg.m2.1_lmerTest))
AIC(logLik(reg.m2.2_lmerTest))
```

```{r}
# higher penalty on complex models than AIC
BIC(logLik(reg.m2.1_lmerTest))
BIC(logLik(reg.m2.2_lmerTest))
```






### Y: Performance
```{r}
reg.m3.1_lmerTest <- 
  df_v1_good_TEST %>% 
  lmerTest::lmer(RTZ ~ OnlineRatingZ + TaskDemand*TrialN 
                 + RT_preZ + ACC_preZ + Trial_A_measure_pre +
                   (0 + TaskDemand|TrialN) + 
                   (1 + TaskDemand|TrialTransition_pre) + (1 + TaskDemand|TrialDemandLearning_pre) +
                   (1| PID) + (1 | ISI) + (1 | CueMapping) + (1 | Gender), 
                 data = .)
# summary(reg.m3.1_lmerTest)
```

```{r}
reg.m3.2_lmerTest <- 
  df_v1_good_TEST %>% 
  lmerTest::lmer(ACCZ ~ OnlineRatingZ + TaskDemand*TrialN 
                 + RT_preZ + ACC_preZ + Trial_A_measure_pre +
                   (0 + TaskDemand|TrialN) + 
                   (1 + TaskDemand|TrialTransition_pre) + (1 + TaskDemand|TrialDemandLearning_pre) +
                   (1| PID) + (1 | ISI) + (1 | CueMapping) + (1 | Gender), 
                 data = .)
```

```{r}
reg.m3.3_lmerTest <- 
  df_v1_good_TEST %>% 
  lmerTest::lmer(Trial_A_measure ~ OnlineRatingZ + TaskDemand*TrialN 
                 + RT_preZ + ACC_preZ + Trial_A_measure_pre +
                   (0 + TaskDemand|TrialN) + 
                   (1 + TaskDemand|TrialTransition_pre) + (1 + TaskDemand|TrialDemandLearning_pre) +
                   (1| PID) + (1 | ISI) + (1 | CueMapping) + (1 | Gender), 
                 data = .)
```

```{r}
tab_model(reg.m3.1_lmerTest, reg.m3.2_lmerTest, reg.m3.3_lmerTest,
          show.re.var = FALSE, show.ngroups = FALSE,
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/reg_m3.html")
```





#### Regression Model Diagnosis
```{r}
plot(reg.m1.2_lmerTest, type = c("p", "smooth"))
# scale-location plots
plot(reg.m1.2_lmerTest, sqrt(abs(resid(.))) ~ fitted(.), type = c("p", "smooth"))
# qq plot
qqmath(reg.m1.2_lmerTest, id = 0.05)
```

```{r}
# posterior predictive simulation
iqrvec <- sapply(simulate(reg.m1.2_lmerTest, 1000), IQR)
obsval <- IQR(df_ra_dc$DCERs)
post.pred.p <- mean(obsval >= c(obsval, iqrvec))
post.pred.p
```


```{r}
plot(reg.m2.2_lmerTest, type = c("p", "smooth"))
# scale-location plots
plot(reg.m2.2_lmerTest, sqrt(abs(resid(.))) ~ fitted(.), type = c("p", "smooth"))
# qq plot
qqmath(reg.m2.2_lmerTest, id = 0.05)
```

```{r}
# posterior predictive simulation
iqrvec <- sapply(simulate(reg.m2.2_lmerTest, 1000), IQR)
obsval <- IQR(df_ra_dc$DCERs)
post.pred.p <- mean(obsval >= c(obsval, iqrvec))
post.pred.p
```




### Performance Pre & Ratings
```{r}
df_ra_rt_means_de_v1 <- 
  df_v1_good_TEST %>%
  group_by(PID, TaskDemand) %>% 
  summarise(RT_preZs = mean(RT_preZ, na.rm = TRUE),
            OnlineRatingZs = mean(OnlineRatingZ, na.rm = TRUE),
            .groups = "drop")

df_ra_rt_means_de_v1 %>%
  ggplot(aes(x = RT_preZs, y = OnlineRatingZs, color = TaskDemand)) +
  geom_point() + 
  geom_smooth(method = "lm")+
  scale_color_lancet()
```

```{r}
df_ra_acc_means_de_v1 <- 
  df_v1_good_TEST %>%
  group_by(PID, TaskDemand) %>% 
  summarise(ACC_pres = mean(Trial_ACC_pre, na.rm = TRUE),
            OnlineRatingZs = mean(OnlineRatingZ, na.rm = TRUE),
            .groups = "drop")

df_ra_acc_means_de_v1 %>%
  ggplot(aes(x = ACC_pres, y = OnlineRatingZs, color = TaskDemand)) +
  geom_point() + 
  geom_smooth(method = "lm")+
  scale_color_lancet()
```

```{r}
df_ra_a_means_de_v1 <- 
  df_v1_good %>%
  group_by(PID, TaskDemand) %>% 
  summarise(A_preZs = mean(Trial_A_measure_pre, na.rm = TRUE),
            OnlineRatingZs = mean(OnlineRatingZ, na.rm = TRUE),
            .groups = "drop")

df_ra_a_means_de_v1 %>%
  ggplot(aes(x = A_preZs, y = OnlineRatingZs, color = TaskDemand)) +
  geom_point() + 
  geom_smooth(method = "lm")+
  scale_color_lancet()
```








### ------------------------------------------------------------------------------------- ###

# DATA ANALYSIS VERSION 2
## Load data
```{r}
DataInput_v2 <- function(p_raw_data_file){
  p_df_raw <- read.csv(p_raw_data_file)
  # print(colnames(p_df_raw))
  select_cols <- c("PID", "SONA", "Gender", "Age", "Run",
                   # Procedure Info
                   "Phase", "Block", "TrialN", "NbackLevel", "ISI", 
                   # Calibration Info
                   "TrialCountCali", "CaliEnd_A_measure",
                   # Trial Info
                   "TrialIndex", "TrialNumber_total", "TrialNumber_valid",
                   # Trial Performance
                   "Trial_A_measure", 
                   "Trial_HR", "Trial_FAR", "Trial_CDR", "Trial_MSR", 
                   "Trial_ACC","Trial_RT",
                   "Letter_Resp", "Letter_RT", "Letter_Accuracy",
                   "Trial_HitsCount","Trial_FAsCount", 
                   "Trial_NoRespCount", "Trial_SameNoRespCount", "Trial_DiffNoRespCount",
                   "Trial_CDsCount",
                   "Trial_CorrectRespCount",
                   "Trial_SameLetterCount", "Trial_DiffLetterCount", "Trial_InvalidLetterCount",
                   # Learning Phase (Test) rating
                   "CueLearning", "CueMapping", "TrialDemandLearning",
                   "OnlineRating", "OnlineRatingRT", "OnlineRatingTimeOut",
                   # True & False choice 
                   "CueLeft", "CueDemandLeft", 
                   "CueRight", "CueDemandRight",
                   "PhaseType",
                   "Choice", "ChoiceRT", 
                   "ChoiceDemand", "TaskDemand",
                   # Offline Rating T1, T2, T3
                   "OfflineRatingTimePoint",
                   "CueFigure", "TrialDemand",
                   "OfflineRating", "OfflineRatingRT",
                   # Demand Rating
                   "DemandRatingQuesIndex", "QuesTLX", "DemandLevel", 
                   "DemandRating", "DemandRatingRT", 
                   # Learning check
                   "CueCheck", "CueDemandCheck", "CorrectAnswer", 
                   "Answer", "AnswerDemand", "AnswerRT", "AnswerACC",
                   # End
                   "END")
  # for (col in select_cols){
  #   check <- col %in% colnames(p_df_raw)
  #   if (! check){
  #    print(c(col, check))
  #   }
  # }
  p_df_selected <- p_df_raw[, select_cols]
  return(p_df_selected)
}
```


data_v2 of the 2nd recruitment
```{r Load Data_v2}
data_path_v2 <- "/Users/ottolab/Desktop/Yue/DARDC_v2_Data_Analysis/data_move_here"
data_all_v2 <- list()
p_folders <- list.dirs(data_path_v2, recursive = FALSE)
# print(p_folders)
for (p_folder in p_folders){
  # print(p_folder)
  p_raw_data_file <- file.path(p_folder, paste0(basename(p_folder), "_task.csv"))
  # print(p_raw_data_file)
  p_data <- DataInput_v2(p_raw_data_file)
  data_all_v2[[basename(p_folder)]] <- p_data  # R中用元素名称作为索引
}

df_all_v2 <- do.call(rbind, data_all_v2)
```


```{r}
write.csv(df_all_v2, na = "",
          file = "/Users/ottolab/Desktop/Yue/DARDC_v2_Data_Analysis/df_all_v2.csv")
```



## Preprocess the whole dataframe
filter rows
include rows whose: Phase in c("CALIBRATION", "TEST", "Test")
```{r}
df_all_v2_select <- 
  df_all_v2 %>% 
  filter(Phase %in% c("TEST", "Test", 
                      "TrueCHOICE", "FalseCHOICE", "Choice", 
                      "OFFLINERATING", 
                      "DEMANDRATING", "LEARNINGCHECK"))
```

```{r}
df_v2_TEST_Test <- 
  df_all_v2_select %>% 
  filter(Phase %in% c("TEST", "Test"))

df_v2_CHOICE_Choice <- 
  df_all_v2_select %>% 
  filter(Phase %in% c("TrueCHOICE", "FalseCHOICE", "Choice"))

df_v2_OFFLINERATING <- 
  df_all_v2_select %>% 
  filter(Phase == "OFFLINERATING")

df_v2_DEMANDRATING <- 
  df_all_v2_select %>% 
  filter(Phase == "DEMANDRATING")

df_v2_LEARNINGCHECK <- 
  df_all_v2_select %>% 
  filter(Phase == "LEARNINGCHECK")
```


merge the corresponding trial row of "Test" and "TEST"
```{r}
df_v2_LEARNING <- 
  df_v2_TEST_Test %>% 
  group_by(PID, Block, TrialN) %>% 
  mutate(
    Trial_A_measure = ifelse(Phase %in% c("TEST"), lag(Trial_A_measure), Trial_A_measure),
    Trial_HR = ifelse(Phase %in% c("TEST"), lag(Trial_HR), Trial_HR),
    Trial_FAR = ifelse(Phase %in% c("TEST"), lag(Trial_FAR), Trial_FAR),
    Trial_CDR = ifelse(Phase %in% c("TEST"), lag(Trial_CDR), Trial_CDR),
    Trial_MSR = ifelse(Phase %in% c("TEST"), lag(Trial_MSR), Trial_MSR),
    Trial_ACC = ifelse(Phase %in% c("TEST"), lag(Trial_ACC), Trial_ACC),
    Trial_RT = ifelse(Phase %in% c("TEST"), lag(Trial_RT), Trial_RT),
    Trial_HitsCount = ifelse(Phase %in% c("TEST"), lag(Trial_HitsCount), Trial_HitsCount),
    Trial_FAsCount = ifelse(Phase %in% c("TEST"), lag(Trial_FAsCount), Trial_FAsCount),
    Trial_SameNoRespCount = ifelse(Phase %in% c("TEST"), 
                                   lag(Trial_SameNoRespCount), Trial_SameNoRespCount),
    Trial_DiffNoRespCount = ifelse(Phase %in% c("TEST"), 
                                   lag(Trial_DiffNoRespCount), Trial_DiffNoRespCount),
    Trial_NoRespCount = ifelse(Phase %in% c("TEST"), 
                               lag(Trial_NoRespCount), Trial_NoRespCount),
    Trial_CDsCount = ifelse(Phase %in% c("TEST"), lag(Trial_CDsCount), Trial_CDsCount),
    Trial_CorrectRespCount = ifelse(Phase %in% c("TEST"), 
                                    lag(Trial_CorrectRespCount), Trial_CorrectRespCount),
    Trial_SameLetterCount = ifelse(Phase %in% c("TEST"), 
                                   lag(Trial_SameLetterCount), Trial_SameLetterCount),
    Trial_DiffLetterCount = ifelse(Phase %in% c("TEST"), 
                                   lag(Trial_DiffLetterCount), Trial_DiffLetterCount),
    Trial_InvalidLetterCount = ifelse(Phase %in% c("TEST"), 
                                      lag(Trial_InvalidLetterCount), Trial_InvalidLetterCount)) %>% 
  ungroup() %>% 
  filter(Phase == "TEST") %>% 
  group_by(PID) %>% 
  mutate(TrialDemandLearning_pre = lag(TrialDemandLearning),
         TrialTransition = ifelse(TrialDemandLearning == lag(TrialDemandLearning), 
                                  "repetition", "switch"),
         TrialTransition_pre = lag(TrialTransition),
         Trial_RT_pre = lag(Trial_RT),
         Trial_ACC_pre = lag(Trial_ACC),
         Trial_A_measure_pre = lag(Trial_A_measure)) %>% 
  ungroup() 


df_v2_CHOICE <- 
  df_v2_CHOICE_Choice %>% 
  group_by(PID, Block, TrialN) %>% 
  mutate(
    Trial_A_measure = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                             lag(Trial_A_measure), Trial_A_measure),
    Trial_HR = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), lag(Trial_HR), Trial_HR),
    Trial_FAR = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), lag(Trial_FAR), Trial_FAR),
    Trial_CDR = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), lag(Trial_CDR), Trial_CDR),
    Trial_MSR = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), lag(Trial_MSR), Trial_MSR),
    Trial_ACC = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), lag(Trial_ACC), Trial_ACC),
    Trial_RT = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), lag(Trial_RT), Trial_RT),
    Trial_HitsCount = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                             lag(Trial_HitsCount), Trial_HitsCount),
    Trial_FAsCount = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                            lag(Trial_FAsCount), Trial_FAsCount),
    Trial_SameNoRespCount = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                                   lag(Trial_SameNoRespCount), Trial_SameNoRespCount),
    Trial_DiffNoRespCount = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                                   lag(Trial_DiffNoRespCount), Trial_DiffNoRespCount),
    Trial_NoRespCount = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                               lag(Trial_NoRespCount), Trial_NoRespCount),
    Trial_CDsCount = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                            lag(Trial_CDsCount), Trial_CDsCount),
    Trial_CorrectRespCount = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                                    lag(Trial_CorrectRespCount), Trial_CorrectRespCount),
    Trial_SameLetterCount = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                                   lag(Trial_SameLetterCount), Trial_SameLetterCount),
    Trial_DiffLetterCount = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                                   lag(Trial_DiffLetterCount), Trial_DiffLetterCount),
    Trial_InvalidLetterCount = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                                      lag(Trial_InvalidLetterCount), Trial_InvalidLetterCount)) %>%
  ungroup() %>% 
  filter(Phase %in% c("TrueCHOICE", "FalseCHOICE")) %>% 
  group_by(PID, PhaseType) %>% 
  mutate(TaskDemand_pre = lag(TaskDemand),
         TrialTransition = ifelse(TaskDemand == lag(TaskDemand), 
                                  "repetition", "switch"),
         TrialTransition_pre = lag(TrialTransition),
         Trial_RT_pre = lag(Trial_RT),
         Trial_ACC_pre = lag(Trial_ACC),
         Trial_A_measure_pre = lag(Trial_A_measure)) %>% 
  ungroup() 

df_v2_LEARNINGCHECK <- 
  df_v2_LEARNINGCHECK %>% 
  mutate(LearningCheck = ifelse(AnswerACC == 0, "NotLearned", "Learned"))

df_v2_DEMANDRATING <-
  df_v2_DEMANDRATING %>% 
  group_by(PID) %>% 
  mutate(DemandRatingZ = c(scale(DemandRating, center = TRUE, scale = TRUE)),
         DemandCheck = ifelse(DemandLevel==1, "easy", ifelse(DemandLevel==2, "mid", "hard"))) %>% 
  mutate(DemandCheck = factor(DemandCheck, levels = c("easy", "mid", "hard"))) %>% 
  ungroup() 
```


```{r}
# check for participants who didn't learned the association
failedLearningPIDs_v2 <- 
  df_v2_LEARNINGCHECK$PID[df_v2_LEARNINGCHECK$LearningCheck=="NotLearned"] %>% 
  unique()
failedLearningPIDs_v2
# PID 81 might learned the association, but didn't explore in False Choice Phase.
# PID 83 might learned the association, but preferred hard task.

```

```{r}
# exclude PID 53, due to rating 1-back as harder
# exclude PID 55, due to mouse malfunction
# exclude PID 93, due to computer crush.
# exclude PID 101, due to self-reported learning failure.
# exclude PID 102, due to self-reported learning failure.
# include PID 113, due to self-reported learning success (wrong answer by mistake).
# exclude PID 114, due to extreme offline ratings.
# exclude PID 115, due to self-reported learning failure.
# exclude PID 109, due to extreme low accuracy (< 0.5).
# exclude PID 129, due to extreme offline ratings.

excludedPIDs_v2 <-
  c(failedLearningPIDs_v2, 53, 55, 93, 101, 102, 109, 114, 115, 129)
excludedPIDs_v2 <- excludedPIDs_v2 [!excludedPIDs_v2 %in% c(113)]
```


```{r}
df_all_v2_merged <- 
  bind_rows(df_v2_LEARNING, df_v2_CHOICE,
            df_v2_OFFLINERATING, df_v2_DEMANDRATING, df_v2_LEARNINGCHECK) 
write.csv(df_all_v2_merged, na = "",
          file = "/Users/ottolab/Desktop/Yue/DARDC_v2_Data_Analysis/df_all_v2_merged.csv")

df_v2_merged <- bind_rows(df_v2_LEARNING, df_v2_CHOICE) 
write.csv(df_v2_merged, na = "",
          file = "/Users/ottolab/Desktop/Yue/DARDC_v2_Data_Analysis/df_v2_merged.csv")
```



## Clean dataframe: df_v2_good
(the dataframe ready for statistical analysis)
Z scores were scaled by the means of each participant
```{r}
cols_v2 <- c("PID", "Gender", "Phase", "Block", "TrialN", "TrialIndex", "NbackLevel","ISI",
          # trial performance
          "Trial_A_measure", "Trial_A_measure_pre",
          "Trial_RT", "Trial_ACC", "Trial_RT_pre", "Trial_ACC_pre",
          "TrialDemandLearning", "TrialTransition", "TrialDemandLearning_pre", "TrialTransition_pre",
          # online rating
          "OnlineRating", "OnlineRatingRT",
          "CueLearning", "CueMapping", 
          # choice 
          "PhaseType",
          "CueLeft", "CueDemandLeft", 
          "CueRight", "CueDemandRight",
          "Choice", "ChoiceDemand", "TaskDemand", "ChoiceRT", 
          # offline rating T1, T2
          "OfflineRatingTimePoint",
          "CueFigure", "TrialDemand", 
          "OfflineRating", "OfflineRatingRT", 
          # demand rating
          "DemandRatingQuesIndex", "QuesTLX", "TaskDemand", "TaskDemand_pre", 
          "DemandRating", "DemandRatingZ" , "DemandRatingRT", 
          # learning check
          "CueCheck", "CueDemandCheck", "CorrectAnswer", 
          "Answer", "AnswerDemand", "AnswerRT", "AnswerACC",
          "LearningCheck")

df_all_v2_good <- 
  df_all_v2_merged %>% 
  filter(!PID %in% excludedPIDs_v2) %>% 
  dplyr::select(all_of(cols_v2)) %>% 
  group_by(PID) %>% 
  mutate(TaskDemand = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"), 
                             TaskDemand, TrialDemandLearning),
         OnlineRatingMean = mean(OnlineRating, na.rm=TRUE),
         OnlineRatingSD = sd(OnlineRating, na.rm=TRUE),
         TaskDemand = factor(TaskDemand),
         TrialTransition = factor(TrialTransition)) %>%
  mutate(RTZ = c(scale(Trial_RT, center = TRUE, scale = TRUE)),
         ACCZ = c(scale(Trial_ACC, center = TRUE, scale = TRUE)),
         RT_preZ = c(scale(Trial_RT_pre, center = TRUE, scale = TRUE)),
         ACC_preZ = c(scale(Trial_ACC_pre, center = TRUE, scale = TRUE)),
         OnlineRatingZ = (OnlineRating-OnlineRatingMean)/OnlineRatingSD,
         OfflineRatingZ = ((OfflineRating-OnlineRatingMean)/OnlineRatingSD)) %>% 
  ungroup()

write.csv(df_all_v2_good, na = "",
          file = "/Users/ottolab/Desktop/Yue/DARDC_v2_Data_Analysis/df_all_v2_good.csv")


cols_v2 <- c("PID", "Gender", "Phase", "Block", "TrialN", "TrialIndex", "NbackLevel","ISI",
          # trial performance
          "Trial_A_measure", "Trial_A_measure_pre",
          "Trial_RT", "Trial_ACC", "Trial_RT_pre", "Trial_ACC_pre",
          "TrialDemandLearning", "TrialTransition", "TrialDemandLearning_pre", "TrialTransition_pre",
          # online rating
          "OnlineRating", "OnlineRatingRT",
          "CueLearning", "CueMapping", 
          # choice 
          "PhaseType",
          "CueLeft", "CueDemandLeft", 
          "CueRight", "CueDemandRight",
          "Choice", "ChoiceDemand", "TaskDemand", "ChoiceRT")

df_v2_good <- 
  df_v2_merged %>% 
  filter(!PID %in% excludedPIDs_v2) %>% 
  dplyr::select(all_of(cols_v2)) %>% 
  group_by(PID) %>% 
  mutate(TaskDemand = ifelse(Phase %in% c("TrueCHOICE", "FalseCHOICE"),
                             TaskDemand, TrialDemandLearning),
         OnlineRatingMean = mean(OnlineRating, na.rm=TRUE),
         OnlineRatingSD = sd(OnlineRating, na.rm=TRUE),
         TaskDemand = factor(TaskDemand),
         TrialTransition = factor(TrialTransition)) %>%
  mutate(RTZ = c(scale(Trial_RT, center = TRUE, scale = TRUE)),
         ACCZ = c(scale(Trial_ACC, center = TRUE, scale = TRUE)),
         RT_preZ = c(scale(Trial_RT_pre, center = TRUE, scale = TRUE)),
         ACC_preZ = c(scale(Trial_ACC_pre, center = TRUE, scale = TRUE)),
         OnlineRatingZ = (OnlineRating-OnlineRatingMean)/OnlineRatingSD) %>% 
  ungroup()

write.csv(df_v2_good, na = "",
          file = "/Users/ottolab/Desktop/Yue/DARDC_v2_Data_Analysis/df_v2_good.csv")
```


```{r}
df_v2_good_LEARNING <- 
  df_v2_good %>% 
  filter(Phase == "TEST")

df_v2_good_TRUECHOICE <- 
  df_v2_good %>% 
  filter(Phase == "TrueCHOICE")

df_v2_good_FALSECHOICE <- 
  df_v2_good %>% 
  filter(Phase == "FalseCHOICE")

df_v2_good_OFFLINERATING <- 
  df_all_v2_good %>% 
  filter(Phase == "OFFLINERATING") 
```



## Overview of participants demographics
```{r}
df_v2_good_demo <- 
  df_v2_good %>%
  group_by(PID) %>% 
  filter(Phase=="TEST", TrialN==1)
```


```{r}
print(c("Gender"))
print(table(df_v2_good_demo$Gender))

print(c("Calibrated ISI"))
print(table(df_v2_good_demo$ISI))
```

```{r}
df_v2_good_demo %>% 
  ggplot(aes(x=factor(sort(ISI))))+
  geom_bar(position = position_dodge(width = 0.5) ,width = 0.7, alpha=0.8)+
  labs(title = "Calibrated ISI", x = "ISI") +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
print(df_v2_good_demo$PID[df_v2_good_demo$ISI == 2583])
# output: [1] 68 69 77 74 63 127
```



## Overview of performance distribution
### Learning Phase
#### RT
barplot
```{r}
df_v2_good_LEARNING %>% 
  # group_by(PID) %>% 
  # summarise(Trial_RT = mean(Trial_RT, na.rm=TRUE), .groups = "drop") %>%
  group_by(PID, TaskDemand) %>% 
  summarise(Trial_RT = mean(Trial_RT, na.rm=TRUE), .groups = "drop") %>%
  mutate(PID = factor(PID)) %>% 
  ggplot(data=., aes(x=PID, y = Trial_RT,
            color = TaskDemand, fill = TaskDemand)) + 
  geom_bar(stat = "identity", position = position_dodge(0.5)) + 
  scale_color_lancet()+
  scale_fill_lancet(alpha = 0.5)+
  labs(title = "Trial RT", x = "PID", y = "Trial RT (s)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


boxplot
```{r}
df_v2_good_LEARNING %>%  
  ggboxplot(data=., x="PID", y = "Trial_RT", 
          color = "TaskDemand", 
          width = 0.7, dodge = 0) + 
  scale_color_lancet()+
  labs(title = "Trial RT", x = "PID", y = "Trial RT (s)", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### ACC
barplot
```{r}
df_v2_good_LEARNING %>% 
  # group_by(PID) %>% 
  # summarise(Trial_ACC = mean(Trial_ACC, na.rm=TRUE), .groups = "drop") %>%
  # mutate(PID = factor(PID)) %>% 
  # ggplot(data=., aes(x=PID, y = Trial_ACC)) + 
  # geom_bar(stat = "identity", position = position_dodge(0.15)) +
  group_by(PID, TaskDemand) %>%
  summarise(Trial_ACC = mean(Trial_ACC, na.rm=TRUE), .groups = "drop") %>%
  mutate(PID = factor(PID)) %>%
  ggplot(data=., aes(x=PID, y = Trial_ACC,
            color = TaskDemand, fill = TaskDemand)) +
  geom_bar(stat = "identity", position = position_dodge(0.5)) +
  scale_color_lancet()+
  scale_fill_lancet(alpha = 0.5)+
  labs(title = "Trial ACC", x = "PID", y = "Trial ACC") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

boxplot
```{r}
df_v2_good_LEARNING %>% 
  ggboxplot(data=., x="PID", y = "Trial_ACC", 
          color = "TaskDemand", 
          width = 0.7, dodge = 0) + 
  scale_color_lancet()+
  labs(title = "Trial ACC", x = "PID", y = "Trial ACC", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### A measure
barplot
```{r}
df_v2_good_LEARNING %>% 
  # group_by(PID) %>% 
  # summarise(Trial_A_measure = mean(Trial_A_measure, na.rm=TRUE), .groups = "drop") %>%
  # mutate(PID = factor(PID)) %>% 
  # ggplot(data=., aes(x=PID, y = Trial_A_measure)) + 
  # geom_bar(stat = "identity", position = position_dodge(0.15)) +
  group_by(PID, TaskDemand) %>%
  summarise(Trial_A_measure = mean(Trial_A_measure, na.rm=TRUE), .groups = "drop") %>%
  mutate(PID = factor(PID)) %>%
  ggplot(data=., aes(x=PID, y = Trial_A_measure,
            color = TaskDemand, fill = TaskDemand)) +
  geom_bar(stat = "identity", position = position_dodge(0.5)) +
  scale_color_lancet()+
  scale_fill_lancet(alpha = 0.5)+
  labs(title = "Trial_A_measure", x = "PID", y = "Trial_A_measure") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


### True Choice Phase
#### RT
barplot
```{r}
df_v2_good_TRUECHOICE %>% 
  # group_by(PID) %>% 
  # summarise(Trial_RT = mean(Trial_RT, na.rm=TRUE), .groups = "drop") %>%
  group_by(PID, TaskDemand) %>% 
  summarise(Trial_RT = mean(Trial_RT, na.rm=TRUE), .groups = "drop") %>%
  mutate(PID = factor(PID)) %>% 
  ggplot(data=., aes(x=PID, y = Trial_RT,
            color = TaskDemand, fill = TaskDemand)) + 
  geom_bar(stat = "identity", position = position_dodge(0.5)) + 
  scale_color_lancet()+
  scale_fill_lancet(alpha = 0.5)+
  labs(title = "Trial RT", x = "PID", y = "Trial RT (s)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

boxplot
```{r}
df_v2_good_TRUECHOICE %>% 
  ggboxplot(data=., x="PID", y = "Trial_RT", 
          color = "TaskDemand", 
          width = 0.7, dodge = 0) + 
  scale_color_lancet()+
  labs(title = "Trial RT", x = "PID", y = "Trial RT (s)", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

#### ACC
barplot
```{r}
df_v2_good_TRUECHOICE %>% 
  # group_by(PID) %>% 
  # summarise(Trial_ACC = mean(Trial_ACC, na.rm=TRUE), .groups = "drop") %>%
  # mutate(PID = factor(PID)) %>% 
  # ggplot(data=., aes(x=PID, y = Trial_ACC)) + 
  # geom_bar(stat = "identity", position = position_dodge(0.15)) +
  group_by(PID, TaskDemand) %>%
  summarise(Trial_ACC = mean(Trial_ACC, na.rm=TRUE), .groups = "drop") %>%
  mutate(PID = factor(PID)) %>%
  ggplot(data=., aes(x=PID, y = Trial_ACC,
            color = TaskDemand, fill = TaskDemand)) +
  geom_bar(stat = "identity", position = position_dodge(0.5)) +
  scale_color_lancet()+
  scale_fill_lancet(alpha = 0.5)+
  labs(title = "Trial ACC", x = "PID", y = "Trial ACC") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

boxplot
```{r}
df_v2_good_TRUECHOICE %>% 
  ggboxplot(data=., x="PID", y = "Trial_ACC", 
          color = "TaskDemand", 
          width = 0.7, dodge = 0) + 
  scale_color_lancet()+
  labs(title = "Trial ACC", x = "PID", y = "Trial ACC", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### A measure
barplot
```{r}
df_v2_good_TRUECHOICE %>% 
  # group_by(PID) %>% 
  # summarise(Trial_A_measure = mean(Trial_A_measure, na.rm=TRUE), .groups = "drop") %>%
  # mutate(PID = factor(PID)) %>% 
  # ggplot(data=., aes(x=PID, y = Trial_A_measure)) + 
  # geom_bar(stat = "identity", position = position_dodge(0.15)) +
  group_by(PID, TaskDemand) %>%
  summarise(Trial_A_measure = mean(Trial_A_measure, na.rm=TRUE), .groups = "drop") %>%
  mutate(PID = factor(PID)) %>%
  ggplot(data=., aes(x=PID, y = Trial_A_measure,
            color = TaskDemand, fill = TaskDemand)) +
  geom_bar(stat = "identity", position = position_dodge(0.5)) +
  scale_color_lancet()+
  scale_fill_lancet(alpha = 0.5)+
  labs(title = "Trial_A_measure", x = "PID", y = "Trial_A_measure") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



### False Choice Phase
#### RT
barplot
```{r}
df_v2_good_FALSECHOICE %>% 
  # group_by(PID) %>% 
  # summarise(Trial_RT = mean(Trial_RT, na.rm=TRUE), .groups = "drop") %>%
  group_by(PID, ChoiceDemand) %>% 
  summarise(Trial_RT = mean(Trial_RT, na.rm=TRUE), .groups = "drop") %>%
  mutate(PID = factor(PID)) %>% 
  ggplot(data=., aes(x=PID, y = Trial_RT,
            color = ChoiceDemand, fill = ChoiceDemand)) + 
  geom_bar(stat = "identity", position = position_dodge(0.5)) + 
  scale_color_lancet()+
  scale_fill_lancet(alpha = 0.5)+
  labs(title = "Trial RT", x = "PID", y = "Trial RT (s)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

boxplot
```{r}
df_v2_good_FALSECHOICE %>% 
  ggboxplot(data=., x="PID", y = "Trial_RT", 
          color = "ChoiceDemand", 
          width = 0.7, dodge = 0) + 
  scale_color_lancet()+
  labs(title = "Trial RT", x = "PID", y = "Trial RT (s)", color = "Choice Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### ACC
barplot
```{r}
df_v2_good_FALSECHOICE %>% 
  # group_by(PID) %>% 
  # summarise(Trial_ACC = mean(Trial_ACC, na.rm=TRUE), .groups = "drop") %>%
  # mutate(PID = factor(PID)) %>% 
  # ggplot(data=., aes(x=PID, y = Trial_ACC)) + 
  # geom_bar(stat = "identity", position = position_dodge(0.15)) +
  group_by(PID, ChoiceDemand) %>%
  summarise(Trial_ACC = mean(Trial_ACC, na.rm=TRUE), .groups = "drop") %>%
  mutate(PID = factor(PID)) %>%
  ggplot(data=., aes(x=PID, y = Trial_ACC,
            color = ChoiceDemand, fill = ChoiceDemand)) +
  geom_bar(stat = "identity", position = position_dodge(0.5)) +
  scale_color_lancet()+
  scale_fill_lancet(alpha = 0.5)+
  labs(title = "Trial ACC", x = "PID", y = "Trial ACC") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

boxplot
```{r}
df_v2_good_FALSECHOICE %>% 
  ggboxplot(data=., x="PID", y = "Trial_ACC", 
          color = "ChoiceDemand", 
          width = 0.7, dodge = 0) + 
  scale_color_lancet()+
  labs(title = "Trial ACC", x = "PID", y = "Trial ACC", color = "Choice Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### A measure
barplot
```{r}
df_v2_good_FALSECHOICE %>% 
  # group_by(PID) %>% 
  # summarise(Trial_A_measure = mean(Trial_A_measure, na.rm=TRUE), .groups = "drop") %>%
  # mutate(PID = factor(PID)) %>% 
  # ggplot(data=., aes(x=PID, y = Trial_A_measure)) + 
  # geom_bar(stat = "identity", position = position_dodge(0.15)) +
  group_by(PID, ChoiceDemand) %>%
  summarise(Trial_A_measure = mean(Trial_A_measure, na.rm=TRUE), .groups = "drop") %>%
  mutate(PID = factor(PID)) %>%
  ggplot(data=., aes(x=PID, y = Trial_A_measure,
            color = ChoiceDemand, fill = ChoiceDemand)) +
  geom_bar(stat = "identity", position = position_dodge(0.5)) +
  scale_color_lancet()+
  scale_fill_lancet(alpha = 0.5)+
  labs(title = "Trial_A_measure", x = "PID", y = "Trial_A_measure") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




## Check for Performance Difference between Demands 
### Learning Phase
compare the RT, ACC, A measure in 1-back and 3-back trials
#### RT
paired by demand within participant
```{r}
pw_rt_v2 <- 
  df_v2_good_LEARNING %>% 
  group_by(PID) %>%
  t_test(RTZ ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "TaskDemand")
table(pw_rt_v2$p.signif)
```

paired by demand across participants (means of participants)
```{r}
df_rt_means_de_v2 <- 
  df_v2_good_LEARNING %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(RTs = mean(Trial_RT, na.rm = TRUE), .groups = "drop")

df_rtz_means_de_v2 <- 
  df_v2_good_LEARNING %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(RTZs = mean(RTZ, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_rt_v2 <- 
  df_rt_means_de_v2 %>% 
  t_test(RTs ~ TaskDemand, 
         comparisons = list("easy", "hard"),paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

t_rtz_v2 <- 
  df_rtz_means_de_v2 %>% 
  t_test(RTZs ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

ggplot(data = df_rt_means_de_v2, aes(x = TaskDemand, y = RTs, color = TaskDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  stat_boxplot(geom = "errorbar", width = 0.15) +
  scale_y_continuous(limits = c(0.15, 1.05))+
  scale_color_lancet()+
  stat_pvalue_manual(t_rtz_v2, y.position = 0.9,
                     label = " p = {p}{p.signif} (z-scored)", hide.ns = TRUE) +
  labs(title = "RT t-test", x = "Task Demand", y = "Trial RT (s)", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



#### ACC
ACCs of Demand levels within each participant
```{r}
pw_acc_v2 <- 
  df_v2_good_LEARNING %>% 
  group_by(PID) %>% 
  pairwise_t_test(Trial_ACC ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "TaskDemand")
table(pw_acc_v2$p.adj.signif)
```

paired by demand across participants (means of participants)
```{r}
df_acc_means_de_v2 <- 
  df_v2_good_LEARNING %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(ACCs = mean(Trial_ACC, na.rm = TRUE), .groups = "drop")

df_accz_means_de_v2 <- 
  df_v2_good_LEARNING %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(ACCZs = mean(ACCZ, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_acc_v2 <- 
  df_acc_means_de_v2 %>% 
  t_test(ACCs ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

t_accz_v2 <- 
  df_accz_means_de_v2 %>% 
  t_test(ACCZs ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

ggplot(data = df_acc_means_de_v2, aes(x = TaskDemand, y = ACCs, color = TaskDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_y_continuous(limits = c(0.5, 1.05)) + 
  scale_color_lancet()+
  stat_pvalue_manual(t_accz_v2, y.position = 1.02, 
                     label = "p = {p}{p.signif}", hide.ns = TRUE) + 
  labs(title = "ACC t-test", x = "Task Demand", y = "Trial ACC", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### A measure
paired by demand across participants (means of participants)
```{r}
df_a_means_de_v2 <- 
  df_v2_good_LEARNING %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(A_measures = mean(Trial_A_measure, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_a_v2 <- 
  df_a_means_de_v2 %>% 
  t_test(A_measures ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

ggplot(data = df_a_means_de_v2, 
       aes(x = TaskDemand, y = A_measures, color = TaskDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter( width = 0.15) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_y_continuous(limits = c(0.5, 1.05)) + 
  scale_color_lancet()+
  stat_pvalue_manual(t_a_v2, y.position = 1.02, label = "p = {p}{p.signif}", hide.ns = TRUE) + 
  labs(title = "A_measure t-test", x = "Task Demand", y = "Trial A_measure", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


### True Choice Phase
#### RT
```{r}
df_rt_means_de_true_choice_v2 <- 
  df_v2_good_TRUECHOICE %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(RTs = mean(Trial_RT, na.rm = TRUE), .groups = "drop")

df_rtz_means_de_true_choice_v2 <- 
  df_v2_good_TRUECHOICE %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(RTZs = mean(RTZ, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_rt_true_choice_v2 <- 
  df_rt_means_de_true_choice_v2 %>% 
  t_test(RTs ~ TaskDemand, comparisons = list("easy", "hard"), paired = FALSE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

t_rtz_true_choice_v2 <- 
  df_rtz_means_de_true_choice_v2 %>% 
  t_test(RTZs ~ TaskDemand,paired = FALSE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

ggplot(data = df_rt_means_de_true_choice_v2, aes(x = TaskDemand, y = RTs, color = TaskDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15, alpha = 0.7) + 
  stat_boxplot(geom = "errorbar", width = 0.15) +
  scale_y_continuous(limits = c(0.15, 1.05))+
  scale_color_lancet()+
  stat_pvalue_manual(t_rtz_true_choice_v2, y.position = 0.9,
                     label = " p = {p}{p.signif} (z-scored)", hide.ns = FALSE) +
  labs(title = "RT t-test", x = "Task Demand", y = "Trial RT (s)", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### ACC
```{r}
df_acc_means_de_true_choice_v2 <- 
  df_v2_good_TRUECHOICE %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(ACCs = mean(Trial_ACC, na.rm = TRUE), .groups = "drop")

df_accz_means_de_true_choice_v2 <- 
  df_v2_good_TRUECHOICE %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(ACCZs = mean(ACCZ, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_acc_true_choice_v2 <- 
  df_acc_means_de_true_choice_v2 %>% 
  t_test(ACCs ~ TaskDemand, paired = FALSE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

t_accz_true_choice_v2 <- 
  df_accz_means_de_true_choice_v2 %>% 
  t_test(ACCZs ~ TaskDemand, paired = FALSE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

ggplot(data = df_acc_means_de_true_choice_v2, aes(x = TaskDemand, y = ACCs, color = TaskDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15, alpha = 0.7) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_y_continuous(limits = c(0.5, 1.05)) + 
  scale_color_lancet()+
  stat_pvalue_manual(t_accz_true_choice_v2, y.position = 1.02, 
                     label = "p = {p}{p.signif}", hide.ns = FALSE) + 
  labs(title = "ACC t-test", x = "Task Demand", y = "Trial ACC", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### A measure
```{r}
df_a_means_de_true_choice_v2 <- 
  df_v2_good_TRUECHOICE %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(A_measures = mean(Trial_A_measure, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_a_true_choice_v2 <- 
  df_a_means_de_true_choice_v2 %>% 
  t_test(A_measures ~ TaskDemand, paired = FALSE, detailed = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()

ggplot(data = df_a_means_de_true_choice_v2, aes(x = TaskDemand, y = A_measures, color = TaskDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter( width = 0.15, alpha = 0.7) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_y_continuous(limits = c(0.5, 1.05)) + 
  scale_color_lancet()+
  stat_pvalue_manual(t_a_true_choice_v2, y.position = 1.02, label = "p = {p}{p.signif}", hide.ns = FALSE) + 
  labs(title = "A_measure t-test", x = "Task Demand", y = "Trial A_measure", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


### False Choice Phase
#### RT
```{r}
df_rt_means_de_false_choice_v2 <- 
  df_v2_good_FALSECHOICE %>% 
  group_by(PID, ChoiceDemand) %>% 
  summarise(RTs = mean(Trial_RT, na.rm = TRUE), .groups = "drop")

df_rtz_means_de_false_choice_v2 <- 
  df_v2_good_FALSECHOICE %>% 
  group_by(PID, ChoiceDemand) %>% 
  summarise(RTZs = mean(RTZ, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_rt_false_choice_v2 <- 
  df_rt_means_de_false_choice_v2 %>% 
  t_test(RTs ~ ChoiceDemand, comparisons = list("easy", "hard"), paired = FALSE, detailed = TRUE) %>% 
  add_xy_position(x = "ChoiceDemand") %>% 
  add_significance()

t_rtz_false_choice_v2 <- 
  df_rtz_means_de_false_choice_v2 %>% 
  t_test(RTZs ~ ChoiceDemand,paired = FALSE, detailed = TRUE) %>% 
  add_xy_position(x = "ChoiceDemand") %>% 
  add_significance()

ggplot(data = df_rt_means_de_false_choice_v2, aes(x = ChoiceDemand, y = RTs, color = ChoiceDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15, alpha = 0.7) + 
  stat_boxplot(geom = "errorbar", width = 0.15) +
  scale_y_continuous(limits = c(0.15, 1.05))+
  scale_color_lancet()+
  stat_pvalue_manual(t_rtz_false_choice_v2, y.position = 0.9,
                     label = " p = {p}{p.signif} (z-scored)", hide.ns = FALSE) +
  labs(title = "RT t-test", x = "Choice Demand", y = "Trial RT (s)", color = "Choice Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### ACC
```{r}
df_acc_means_de_false_choice_v2 <- 
  df_v2_good_FALSECHOICE %>% 
  group_by(PID, ChoiceDemand) %>% 
  summarise(ACCs = mean(Trial_ACC, na.rm = TRUE), .groups = "drop")

df_accz_means_de_false_choice_v2 <- 
  df_v2_good_FALSECHOICE %>% 
  group_by(PID, ChoiceDemand) %>% 
  summarise(ACCZs = mean(ACCZ, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_acc_false_choice_v2 <- 
  df_acc_means_de_false_choice_v2 %>% 
  t_test(ACCs ~ ChoiceDemand, paired = FALSE, detailed = TRUE) %>% 
  add_xy_position(x = "ChoiceDemand") %>% 
  add_significance()

t_accz_false_choice_v2 <- 
  df_accz_means_de_false_choice_v2 %>% 
  t_test(ACCZs ~ ChoiceDemand, paired = FALSE, detailed = TRUE) %>% 
  add_xy_position(x = "ChoiceDemand") %>% 
  add_significance()

ggplot(data = df_acc_means_de_false_choice_v2, aes(x = ChoiceDemand, y = ACCs, color = ChoiceDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15, alpha = 0.7) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_y_continuous(limits = c(0.5, 1.05)) + 
  scale_color_lancet()+
  stat_pvalue_manual(t_accz_false_choice_v2, y.position = 1.02, 
                     label = "p = {p}{p.signif}", hide.ns = FALSE) + 
  labs(title = "ACC t-test", x = "Choice Demand", y = "Trial ACC", color = "Choice Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### A measure
```{r}
df_a_means_de_false_choice_v2 <- 
  df_v2_good_FALSECHOICE %>% 
  group_by(PID, ChoiceDemand) %>% 
  summarise(A_measures = mean(Trial_A_measure, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_a_false_choice_v2 <- 
  df_a_means_de_false_choice_v2 %>% 
  t_test(A_measures ~ ChoiceDemand, paired = FALSE, detailed = TRUE) %>% 
  add_xy_position(x = "ChoiceDemand") %>% 
  add_significance()

ggplot(data = df_a_means_de_false_choice_v2, aes(x = ChoiceDemand, y = A_measures, color = ChoiceDemand)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter( width = 0.15, alpha = 0.7) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_y_continuous(limits = c(0.5, 1.05)) + 
  scale_color_lancet()+
  stat_pvalue_manual(t_a_false_choice_v2, y.position = 1.02, label = "p = {p}{p.signif}", hide.ns = FALSE) + 
  labs(title = "A_measure t-test", x = "Choice Demand", y = "Trial A_measure", color = "Choice Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




### Choice RT
```{r}
df_v2_good_FALSECHOICE_remove_outliers <- 
  df_v2_good_FALSECHOICE %>%
  group_by(PID) %>% 
  dplyr::filter(abs(scale(ChoiceRT)) <= 2.5) %>% 
  ungroup()
```


```{r}
df_v2_good_FALSECHOICE_remove_outliers %>%  
  ggboxplot(data=., x="PID", y = "ChoiceRT", 
          color = "ChoiceDemand",
          width = 0.7, dodge = 0) + 
  scale_color_lancet()+
  labs(title = "Trial RT", x = "PID", y = "Trial RT (s)", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


boxplot
```{r}
df_v2_good_FALSECHOICE %>%  
  ggboxplot(data=., x="PID", y = "ChoiceRT", 
          color = "ChoiceDemand",
          width = 0.7, dodge = 0) + 
  scale_color_lancet()+
  labs(title = "Trial RT", x = "PID", y = "Trial RT (s)", color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



## Check for Performance Difference between Transitions
compare the RT, ACC and A measure in repetition and switch trials
### Learning Phase
#### Rt
paired by transition within each participant
```{r}
pw_rt_v2 <- 
  df_v2_good_LEARNING %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID) %>% 
  t_test(Trial_RT ~ TrialTransition, paired = FALSE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "TrialTransition")
table(pw_rt_v2$p.signif)
```

paired by transition across participants (means of participants)
```{r}
df_rt_means_tr_v2 <- 
  df_v2_good_LEARNING %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID, TrialTransition) %>% 
  summarise(RTs = mean(Trial_RT, na.rm = TRUE), .groups = "drop")

df_rtz_means_tr_v2 <- 
  df_v2_good_LEARNING %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID, TrialTransition) %>% 
  summarise(RTZs = mean(RTZ, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_rt_v2 <- 
  df_rt_means_tr_v2 %>% 
  t_test(RTs ~ TrialTransition, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TrialTransition") %>% 
  add_significance()

t_rtz_v2 <- 
  df_rtz_means_tr_v2 %>% 
  t_test(RTZs ~ TrialTransition, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TrialTransition") %>% 
  add_significance()

ggplot(data = df_rt_means_tr_v2, aes(x = TrialTransition, y = RTs, color = TrialTransition)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  scale_color_jco()+
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  stat_pvalue_manual(t_rtz_v2, y.position = 0.75,
                     label = "{p.signif} (z-scored)", hide.ns = FALSE) +
  # stat_pvalue_manual(t_rt, label = " p.signif", hide.ns = TRUE) +
  scale_y_continuous(limits = c(0.15, 1.05)) + 
  labs(title = "RT t-test", x = "Trial Transition", y = "Trial RT", color = "Trial Transition") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### ACC 
paired by transition within each participant
```{r}
pw_acc_v2 <- 
  df_v2_good_LEARNING %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID) %>% 
  t_test(Trial_ACC ~ TrialTransition, paired = FALSE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "TrialTransition")
table(pw_acc_v2$p.signif)
```

paired by transition across participants (means of participants)
```{r}
df_acc_means_tr_v2 <- 
  df_v2_good_LEARNING %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID, TrialTransition) %>% 
  summarise(ACCs = mean(Trial_ACC, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_acc_v2 <- 
  df_acc_means_tr_v2 %>% 
  t_test(ACCs ~ TrialTransition, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "TrialTransition") %>% 
  add_significance()

ggplot(data = df_acc_means_tr_v2, 
       aes(x = TrialTransition, y = ACCs, color = TrialTransition)) +  #, label=PID
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  scale_color_jco()+
  # geom_label_repel(aes(color=TrialTransition), 
  #                  max.overlaps = nrow(df_acc_means_tr_v2)) +
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  stat_pvalue_manual(t_acc_v2, y.position = 0.99,
                     label = "{p.signif}", hide.ns = FALSE) + 
  scale_y_continuous(limits = c(0.5, 1.05)) + 
  labs(title = "ACC t-test", x = "Trial Transition", y = "Trial ACC", color = "Trial Transition") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
# excluded PID 109 ACC < 0.5
```


#### A measure
paired by transition across participants (means of participants)
```{r}
df_a_means_tr_v2 <- 
  df_v2_good_LEARNING %>% 
  filter(! is.na(TrialTransition)) %>% 
  group_by(PID, TrialTransition) %>% 
  summarise(A_measures = mean(Trial_A_measure, na.rm = TRUE), .groups = "drop")
```

boxplot
```{r}
t_a_v2 <- 
  df_a_means_tr_v2 %>% 
  t_test(A_measures ~ TrialTransition, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "Trial_Transition") %>% 
  add_significance()

ggplot(data = df_a_means_tr_v2, aes(x = TrialTransition, y = A_measures, color = TrialTransition)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter(width = 0.15) + 
  scale_color_jco()+
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  stat_pvalue_manual(t_a_v2, xmin = "group1", xmax = "group2",
                     y.position = 0.98,
                     label = "{p.signif}", hide.ns = FALSE) + 
  scale_y_continuous(limits = c(0.5, 1.05)) + 
  labs(title = "A_measure t-test", x = "Trial Transition", y = "Trial A_measure", color = "Trial Transition") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```





## Check for DemandRatings (Demand-Manipulation Check)
```{r}
df_dr_means_de_v2 <- 
  df_v2_DEMANDRATING %>% 
  filter(!PID %in% excludedPIDs_v2) %>% 
  group_by(PID, DemandCheck) %>% 
  summarise(DR_aves = mean(DemandRating, na.rm=TRUE),
            DR_aveZs = mean(DemandRatingZ, na.rm=TRUE),
            .groups = "drop")

t_dr_v2 <- 
  df_dr_means_de_v2 %>% 
  t_test(DR_aveZs ~ DemandCheck, paired = TRUE, detailed = TRUE) %>% 
  add_xy_position(x = "DemandCheck") %>% 
  add_significance()
```

boxplot
(higher rating = rated as more demanding)
```{r}
ggplot(data = df_dr_means_de_v2, aes(x = DemandCheck, y = DR_aves, color = DemandCheck)) + 
  geom_boxplot(width = 0.5, alpha = 0.7) +
  geom_jitter() + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  stat_pvalue_manual(t_dr_v2, y = 105, step.increase = 0.2,
                     label = "p = {p}{p.adj.signif}", hide.ns = TRUE) + 
  scale_color_manual(labels=c("1-Back", "2-Back", "3-Back"),
                     values = c("#00468BFF", "#42B540FF", "#ED0000FF")) + 
  scale_x_discrete(labels = c("1-Back", "2-Back", "3-Back")) + 
  labs(title = "DemandRating t-test", x = "TrialDemand", y = "DemandRating_ave", 
       color = "DemandCheck") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


The participant should be excluded due to rating 1-back as harder?
```{r}
df_dr_means_de_diff_v2 <- 
  df_dr_means_de_v2 %>% 
  select(-DR_aves) %>% 
  pivot_wider(names_from = DemandCheck, names_prefix = "DR_aveZs_" ,values_from = DR_aveZs) %>% 
  group_by(PID) %>% 
  mutate(DR_aveZs_diff = DR_aveZs_hard -DR_aveZs_easy) %>% 
  ungroup() %>% 
  mutate(DemandCheckType = ifelse(DR_aveZs_diff > 0, "3_as_harder", "1_as_harder")) 

# ggplot(data = df_dr_means_de_diff, aes(x="", y = DR_aveZs_diff, color=DemandCheckType, label=PID)) +
ggplot(data = df_dr_means_de_diff_v2, aes(x="", y = DR_aveZs_diff, color=DemandCheckType)) +
  geom_boxplot(position="identity", width = 0.2, alpha = 0.9) +
  geom_jitter(width = 0.2) + 
  # geom_label_repel(aes(color=DemandCheckType), max.overlaps = nrow(df_dr_means_de_diff)) + 
  scale_color_d3()+
  labs(title = "DemandRating_diff (hard-easy)", x = "", y = "DemandRating_aveZ_diff") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```





## Online Ratings
### Descriptive Analysis 
histogram of OnlineRatingZ
```{r}
df_v2_good_LEARNING %>% 
  group_by(PID) %>% 
  summarise(OnlineRatingZ = mean(OnlineRatingZ, ra.rm = TRUE)) %>% 
  ggplot(aes(x = OnlineRatingZ)) +
  geom_histogram(position = position_dodge(width = 0.6), bins = 5, alpha = 0.8)+
  # scale_x_continuous(limits = c(-1, 1)) +
  labs(title = "Rating Means of participants", x = "Ratings(z-scored)", y = "Counts") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

separate distributions of different demands
```{r}
df_v2_good_LEARNING %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(OnlineRatingZ = mean(OnlineRatingZ, ra.rm = TRUE), .groups = "drop") %>% 
  ggplot(aes(x = OnlineRatingZ, color  = TaskDemand, fill = TaskDemand)) +
  geom_histogram(position = position_dodge(width = 0.25), bins = 10, alpha = 0.7)+
  scale_x_continuous(breaks = seq(-5, 5)) +
  scale_color_lancet() + 
  scale_fill_lancet() + 
  labs(title = "Rating Means of participants", x = "Ratings(z-scored)", y = "Counts", fill = "Task Demand") + 
  guides(color = "none") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



### Rating ~ Task Demand (easy / hard)
#### boxplot of raw OnlineRatings (means of participants)
```{r}
df_ra_means_de_v2 <- 
  df_v2_good_LEARNING %>% 
  group_by(PID, TaskDemand) %>% 
  summarise(ISI = mean(ISI, na.rm=TRUE),
    OnlineRatings = mean(OnlineRating, na.rm = TRUE), 
            .groups = "drop") %>% 
  group_by(PID) %>% 
  mutate(OnlineRatings_diff = 
           OnlineRatings[TaskDemand == "easy"] - OnlineRatings[TaskDemand == "hard"],
         aversive = ifelse(OnlineRatings_diff > 0, "demand_aversive", "no_demand_aversive")) %>% 
  ungroup()
```

```{r}
t_ra_v2 <- 
  df_ra_means_de_v2 %>%
  t_test(OnlineRatings ~ TaskDemand,  detailed = TRUE, paired = TRUE) %>% 
  add_xy_position(x = "TaskDemand") %>% 
  add_significance()


df_ra_means_de_v2 %>%
  ggplot(aes(x = TaskDemand, y = OnlineRatings, color = TaskDemand)) + 
  geom_boxplot(position = "identity", alpha = 0.7, width = 0.5, outlier.color = "black") +
  stat_boxplot(geom = "errorbar", width = 0.15) +
  geom_jitter(width = 0.15) + 
  scale_color_lancet() + 
  stat_summary(fun = mean, geom = "point", shape = 17, color = "black", size = 2) +
  stat_pvalue_manual(t_ra_v2, y.position = 102, label = "p = {p}{p.signif}") +
  labs(title = "OnlineRating t-test", x = "Task Demand", y = "Rating(raw)", 
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
  
```

#### boxplot of OnlineRatingZ (means of participants)
```{r}
df_ra_means_de_v2 <- 
  df_v2_good_LEARNING %>% 
  group_by(PID, TaskDemand) %>% 
  dplyr::summarise(ISI = mean(ISI, na.rm=TRUE),
            CueMapping = first(CueMapping),
            Gender = first(Gender),
            OnlineRatingZs = mean(OnlineRatingZ), 
            .groups = "drop") 
```

```{r}
t_raz_v2 <- 
  df_ra_means_de_v2 %>%
  t_test(OnlineRatingZs ~ TaskDemand, detailed = TRUE, paired = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "TaskDemand")
# There's only one comparison between the two levels of TaskDemand so no "adjustment" was applied.
# And thus, no p.adj in the output.

df_ra_means_de_v2 %>%
  ggplot(aes(x = TaskDemand, y = OnlineRatingZs, color = TaskDemand)) + 
  geom_boxplot(position = "identity", alpha = 0.7, width = 0.5, outlier.color = "black") +
  stat_boxplot(geom = "errorbar", width = 0.15) +
  geom_jitter(width = 0.15) + 
  scale_color_lancet() + 
  stat_summary(fun = mean, geom = "point", shape = 17, color = "black", size = 2) +
  stat_pvalue_manual(t_raz_v2, y.position = 1.1, label = "p = {p}{p.signif}", hide.ns = FALSE) + 
  labs(title = "OnlineRating t-test", x = "Task Demand", y = "Rating(z-scored)", 
       color = "TaskDemand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
  
```

#### RatingZ-pairs of each participant in different demands
```{r}
df_ra_means_de_v2 %>% 
  ggpaired(x = "TaskDemand", y = "OnlineRatingZs", id="PID", 
           color = "TaskDemand", line.color = "grey", palette = "lancet") +
  stat_summary(fun = "mean", shape = 17) +
  stat_compare_means(method = "t.test", paired = TRUE, label = "p", label.x = 1.5) +
  labs(title = "Rating pairs", x = "Task Demand", y = "Rating(z-scored)", 
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

#### Demand Aversiveness 
(easy - hard) > 0 or < 0 
```{r}
df_ra_means_de_diff_v2 <- 
  df_ra_means_de_v2 %>% 
  group_by(PID) %>% 
  summarise(
    OnlineRatingZs_diff = OnlineRatingZs[TaskDemand == "easy"] - OnlineRatingZs[TaskDemand == "hard"],
    DemandAversive_on = ifelse(OnlineRatingZs_diff > 0, "demand_aversive", "no_demand_aversive"),
    .groups = "drop")
```


```{r}
df_ra_means_de_diff_v2 %>% 
  ggplot(aes(x="", y=OnlineRatingZs_diff))+
  geom_boxplot(width = 0.5, outlier.color="red")+
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  geom_jitter(aes(color=DemandAversive_on), width = 0.15) +
  scale_color_d3()+
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") + 
  labs(title = "Rating Diff (easy - hard)", x = "", y = "Rating(z-scored)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
table(df_ra_means_de_diff_v2$DemandAversive_on)
```

```{r}
df_ra_means_de_v2_wide <- 
  df_ra_means_de_v2 %>% 
  pivot_wider(values_from = OnlineRatingZs, names_from = TaskDemand,
              names_glue = "OnlineRatingZs_{TaskDemand}")
```

```{r}
df_v2_good_LEARNING <- 
  df_v2_good_LEARNING %>% 
  merge(df_ra_means_de_diff_v2[, c("PID", "DemandAversive_on")], by="PID", all.x = T)
```



#### boxplot of OnlineRatingZ grouped by aversiveness (means of participants)
```{r}
df_ra_means_de_v2 <- 
  df_v2_good_LEARNING %>% 
  group_by(PID, TaskDemand, DemandAversive_on) %>% 
  summarise(ISI = mean(ISI, na.rm=TRUE),
            CueMapping = first(CueMapping),
            Gender = first(Gender),
            OnlineRatingZs = mean(OnlineRatingZ, na.rm = TRUE), 
            .groups = "drop") 
```

```{r}
t_raz_v2 <- 
  df_ra_means_de_v2 %>%
  group_by(DemandAversive_on) %>% 
  t_test(OnlineRatingZs ~ TaskDemand, detailed = TRUE, paired = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "DemandAversive")
# There's only one comparison between the two levels of TaskDemand so no "adjustment" was applied.
# And thus, no p.adj in the output.

df_ra_means_de_v2 %>%
  ggplot(data=., aes(x = DemandAversive_on, y = OnlineRatingZs)) + 
  geom_boxplot(aes(color = TaskDemand),
               position = position_dodge(width = 0.5), 
            alpha = 0.7, outlier.color = "black") +
  stat_boxplot(aes(color = TaskDemand),
               geom = "errorbar", width = 0.15,
               position = position_dodge(width = 0.5)) +
  geom_jitter(aes(color = TaskDemand),
              position = position_dodge2(width = 0.5)) +
  stat_summary(aes(x = DemandAversive_on, y = OnlineRatingZs, 
                   color = TaskDemand),
               fun = mean, 
               geom = "point", shape = 17, size = 2,
               position = position_dodge(0.5)) +
  stat_pvalue_manual(t_raz_v2, y.position = 1.1, 
                     label = "p = {p}{p.signif}", hide.ns = FALSE) +  
  scale_color_lancet() + 
  labs(title = "OnlineRating t-test", 
       x = "Demand Aversive", y = "Rating(z-scored)", 
       color = "TaskDemand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
  
```



Cohen's d effect size
```{r}
# collapse across all participants
cohen_d_ra_de_v2 <- 
  effsize::cohen.d(OnlineRatingZ ~ TaskDemand | Subject(PID), paired = TRUE, 
                               data = df_v2_good_LEARNING)
cohen_d_ra_de_v2
```

```{r}
# means of participants
cohen_d_ra_means_de_v2 <- 
  cohen.d(OnlineRatingZs ~ TaskDemand | Subject(PID), paired = TRUE, 
                               data = df_ra_means_de_v2)
cohen_d_ra_means_de_v2
```



#### line plot of means by trial (points are means of each participant)
```{r}
df_means_by_trial_v2 <- 
  df_v2_good_LEARNING %>%
  # filter(TrialN != 1) %>%
  group_by(PID, TaskDemand, TrialN) %>% 
  summarise(OnlineRatingZ = mean(OnlineRatingZ, na.rm=TRUE),
            RTZ = mean(RTZ, na.rm=TRUE),
            ACCZ = mean(ACCZ, na.rm=TRUE,),
            .groups = "drop")
```


```{r}
ggplot(data = df_means_by_trial_v2, 
       aes(x = TrialN, y = OnlineRatingZ, group = TrialN, 
             color = TaskDemand, fill = TaskDemand)) +
  # geom_rect(xmin = 50.45, xmax = 50.55, ymin = -Inf, ymax = Inf,
  #           color = "gray", fill = "gray", alpha = 0.2) +
  geom_boxplot(width = 0.9, position = position_dodge(width = 0.9), alpha = 0.8,
              outlier.shape = 8, outlier.size = 2) + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  geom_point(shape = 1) +
  stat_summary(fun = mean, geom = "line", aes(group = TaskDemand)) +
  stat_summary(fun = mean, geom = "point", aes(group = TaskDemand, color = TaskDemand)) +
  scale_x_continuous(limits = c(0, 51), breaks = seq(1, 50, 1))+
  scale_color_lancet() + 
  scale_fill_lancet() + 
  labs(title = "Ratings by Trial (means across participants)", x = "TrialN", y = "Rating (z-scored)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5) )
```


#### lines of each participant 
All
```{r}
df_v2_good_LEARNING %>% 
  ggplot(data = ., aes(x = TrialIndex, y = OnlineRatingZ, 
                                group = interaction(PID, TaskDemand), 
                                color = TaskDemand)) +
  geom_rect(xmin = 50.45, xmax = 50.55, ymin = -Inf, ymax = Inf,
            color = "gray", fill = "gray", alpha = 0.2) +
  geom_point() +
  geom_line(aes(linetype = TaskDemand), show.legend = FALSE) + 
  scale_color_lancet() +
  scale_x_continuous(limits = c(1, 51), breaks = seq(1, 50, 1))+
  labs(title = "Ratings by Trial (means within participant)", 
       x = "Trial Index", y = "Rating (z-scored)", 
       color = "Task Demand") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

participant with demand aversiveness
```{r}
df_v2_good_LEARNING %>% 
  filter(DemandAversive == "demand_aversive") %>% 
  ggplot(data = ., aes(x = TrialIndex, y = OnlineRatingZ, 
                                group = interaction(PID, TaskDemand), 
                                color = TaskDemand)) +
  geom_rect(xmin = 50.45, xmax = 50.55, ymin = -Inf, ymax = Inf,
            color = "gray", fill = "gray", alpha = 0.2) +
  geom_point() +
  geom_line(aes(linetype = TaskDemand), show.legend = FALSE) + 
  scale_color_lancet() +
  scale_x_continuous(limits = c(1, 51), breaks = seq(1, 50, 1))+
  labs(title = "Ratings by Trial (means within participant)", 
       x = "Trial Index", y = "Rating (z-scored)", 
       color = "Task Demand") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

participant without demand aversiveness
```{r}
df_v2_good_LEARNING %>% 
  filter(DemandAversive == "no_demand_aversive") %>% 
  ggplot(data = ., aes(x = TrialIndex, y = OnlineRatingZ, 
                                group = interaction(PID, TaskDemand), 
                                color = TaskDemand)) +
  geom_rect(xmin = 50.45, xmax = 50.55, ymin = -Inf, ymax = Inf,
            color = "gray", fill = "gray", alpha = 0.2) +
  geom_point() +
  geom_line(aes(linetype = TaskDemand), show.legend = FALSE) + 
  scale_color_lancet() +
  scale_x_continuous(limits = c(1, 51), breaks = seq(1, 50, 1))+
  labs(title = "Ratings by Trial (means within participant)", 
       x = "Trial Index", y = "Rating (z-scored)", 
       color = "Task Demand") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```



#### check for extreme PID 
PID 74 has extremely low rating on Trial_42 (easy)
```{r}
df_means_by_trial_one_v2 <- 
  df_v2_good_LEARNING %>%
  # filter(TrialN != 1) %>%
  filter(PID == 74 & TrialN == 42) %>%
  group_by(PID, TaskDemand, TrialN) %>% 
  summarise(OnlineRatingZ = mean(OnlineRatingZ, na.rm=TRUE),
            RTZ = mean(RTZ, na.rm=TRUE),
            ACCZ = mean(ACCZ, na.rm=TRUE,),
            .groups = "drop")
```


```{r}
ggplot(data = df_means_by_trial_one, 
       aes(x = TrialN, y = OnlineRatingZ, group = TrialN, 
             color = TaskDemand, fill = TaskDemand)) +
  # geom_rect(xmin = 50.45, xmax = 50.55, ymin = -Inf, ymax = Inf,
  #           color = "gray", fill = "gray", alpha = 0.2) +
  geom_boxplot(width = 0.8, position = position_dodge(width = 0.8), 
               alpha = 0.8, outlier.color = "black") + 
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  geom_jitter(shape = 1) +
  stat_summary(fun = mean, geom = "line", aes(group = TaskDemand)) +
  stat_summary(fun = mean, geom = "point", aes(group = TaskDemand, color = TaskDemand)) +
  scale_x_continuous(limits = c(1, 51), breaks = seq(1, 50, 1))+
  scale_color_lancet() + 
  scale_fill_lancet() + 
  labs(title = "Ratings by Trial (selected participants)", x = "TrialN", y = "Rating (z-scored)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5) )
```



trial-by-trial ratings plot by TaskDemand (grandmeans of participants' means)
```{r}
df_grandmeans_by_trial_v2 <- 
  df_means_by_trial_v2 %>%
  group_by(TaskDemand, TrialN) %>% 
  summarise(OnlineRatingZ = mean(OnlineRatingZ, na.rm=TRUE),
            RTZ = mean(RTZ, na.rm=TRUE),
            ACCZ = mean(ACCZ, na.rm=TRUE,),
            .groups = "drop")
```


```{r}
ggplot(data = df_grandmeans_by_trial_v2, aes(x = TrialN, y = OnlineRatingZ, group = TrialN, 
             color = TaskDemand, fill = TaskDemand)) +
  # geom_rect(xmin = 50.45, xmax = 50.55, ymin = -Inf, ymax = Inf,
  #           color = "gray", fill = "gray", alpha = 0.2) +
  # geom_boxplot(width = 0.8, position = position_dodge(width = 0.8), 
  #              alpha = 0.5, outlier.color = "black") + 
  # stat_boxplot(geom = "errorbar", width = 0.15) + 
  # geom_jitter(shape = 1) +
  geom_point()+
  stat_summary(fun = mean, geom = "line", aes(group = TaskDemand)) +
  stat_summary(fun = mean, geom = "point", aes(group = TaskDemand, color = TaskDemand)) +
  scale_x_continuous(limits = c(1, 51), breaks = seq(1, 50, 1))+
  scale_color_lancet() + 
  scale_fill_lancet() + 
  labs(title = "Ratings by Trial (grandmeans of participants)", x = "Trial N", y = "Rating (z-scored)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5) )
```




#### Difference in each 10 trials 
```{r}
# review the number of easy and hard trials in each trial section
# it turns out to be relatively balanced

result_table <- 
  df_v2_good_LEARNING %>%
  mutate(Trial_Index_Group = cut(TrialIndex, breaks = c(1, 10, 20, 30, 40, 50), labels = FALSE)) %>%
  group_by(PID, Trial_Index_Group, TaskDemand) %>%
  summarise(count = n(), .groups = "drop") %>%
  pivot_wider(names_from = TaskDemand, values_from = count, values_fill = 0)

# Print the resulting table
# print(result_table)

```

```{r}
# Create a function to calculate mean for each section of trials
calculate_section_means <- function(data, col_reps, section) {
  data %>%
    group_by(PID, TaskDemand) %>%
    summarise(across(starts_with(col_reps),
                     ~ mean(.[TrialN %in% section], na.rm = TRUE),
                     .names = "OnlineRatingZ_mean"), .groups = "drop")
}

# Define the sections
trial_sections <- list("10" = 1:10, "20" = 11:20, "30" = 21:30, "40" = 31:40, "50" = 41:50)
# trial_sections <- list("05" = 1:5, "10" = 6:10, 
#                        "15" = 11:15, "20" = 16:20, 
#                        "25" = 21:25, "30" = 26:30, 
#                        "35" = 31:35, "40" = 36:40, 
#                        "45" = 41:45, "50" = 46:50)

# Initialize an empty list to store the results
result_list <- list()

# Use lapply to iterate over each section and calculate means
result_list <- lapply(names(trial_sections), function(section_name) {
  section <- trial_sections[[section_name]]
  
  # Calculate means for the current section
  result <- calculate_section_means(df_v2_good_LEARNING, "OnlineRatingZ", section)
  
  # Add a column indicating the section name
  result$Section <- section_name
  
  return(result)
})

# Combine the results into a single data frame
df_ra_means_demands_tsecs_v2 <- bind_rows(result_list)

df_ra_grandmeans_demands_tsecs_v2 <-
  df_ra_means_demands_tsecs_v2 %>% 
  group_by(Section, TaskDemand) %>% 
  summarise(OnlineRatingZ_gmean = mean(OnlineRatingZ_mean, na.rm = TRUE), .groups = "drop") 
```

```{r}
t_ra_set_v2 <- 
  df_ra_means_demands_tsecs_v2 %>% 
  group_by(Section) %>% 
  t_test(OnlineRatingZ_mean ~ TaskDemand, paired = TRUE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "Section")
```

boxplots
```{r}
df_ra_means_demands_tsecs_v2 %>% 
  ggplot(aes(x = Section, y = OnlineRatingZ_mean,color = TaskDemand)) + 
  geom_boxplot(position = "dodge2", 
               alpha = 0.7, width = 0.5, outlier.color = "black") +
  geom_point(position = position_dodge(width = 0.5), alpha = 0.5) +
  scale_color_lancet() +
  # scale_fill_lancet() +
  # stat_summary(fun = mean, geom = "point", shape = 17, color = "black", size = 2) +
  # stat_compare_means(label = "p.signif", paired = TRUE, hide.ns = FALSE, label.y = 2.2) + 
  stat_pvalue_manual(t_ra_set_v2, label = "p = {p}{p.signif}", hide.ns = FALSE,
                      tip.length = 0, linetype = "blank") +
  labs(title = "OnlineRating t-test by Section", x = "Task Demand", y = "Rating(z-scored)", 
       color = "TaskDemand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


line plot
```{r}
# Plot
df_ra_grandmeans_demands_tsecs_v2 %>% 
  ggplot(aes(x = Section, y = OnlineRatingZ_gmean, 
             group = TaskDemand, color = TaskDemand)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(-0.7, 0.7)) + 
  scale_color_lancet() +
  stat_pvalue_manual(t_ra_set_v2, label = "p = {p}{p.signif}", hide.ns = FALSE,
                     y.position = c(0.36, 0.28, 0.26, 0.34, 0.29),
                      tip.length = 0, linetype = "blank") +
  labs(title = "Ratings by Trial Sections", x = "Trial Section", y = "Rating GrandMean (z-scored)",
       color = "Task Demand") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```



line plot of each participant
```{r}
df_ra_means_diffs_10ts_v2 <-
  df_ra_means_demands_tsecs_v2 %>% 
  group_by(PID) %>% 
  pivot_wider(names_from = TaskDemand, values_from = OnlineRatingZ_mean,
              names_glue = "OnlineRatingZ_mean_{TaskDemand}") %>% 
  group_by(PID, Section) %>%   
  summarise(OnlineRatingZ_diff = OnlineRatingZ_mean_easy - OnlineRatingZ_mean_hard,
              .groups = "drop")

# grand mean of participants
df_ra_grandmeans_diffs_10ts_v2 <- 
  df_ra_means_diffs_10ts_v2 %>% 
  group_by(Section) %>% 
  summarise(OnlineRatingZ_diff = mean(OnlineRatingZ_diff, na.rm = TRUE),
            .groups = "drop")

# wide dataframe of each 
df_ra_means_diffs_10ts_v2_wide <- 
  df_ra_means_diffs_10ts_v2 %>% 
  pivot_wider(names_from = Section, values_from = OnlineRatingZ_diff, names_prefix = "OnlineRatingZ_diff_")
```


```{r}
df_ra_grandmeans_diffs_10ts_v2 %>% 
  mutate(TrialSection = as.numeric(Section)) %>% 
  ggplot(aes(x = TrialSection, y = OnlineRatingZ_diff)) +
  geom_point()+
  geom_line()+
  labs(title = "Rating_Diffs by Trial sections", x = "Trial Section", y = "Rating_Diff (z-scored)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```




### Trial Transition of Previous Trial (repeat / switch)
boxplot (means of participants)
```{r}
df_ra_means_trpre_v2 <- 
  df_v2_good_LEARNING %>% 
  filter(!is.na(TrialTransition_pre)) %>% 
  group_by(PID, TrialTransition_pre) %>% 
  summarise(OnlineRatingZs = mean(OnlineRatingZ, na.rm = TRUE), .groups = "drop")

t_ra_v2 <-
  df_ra_means_trpre_v2 %>%
  t_test(OnlineRatingZs ~ TrialTransition_pre, paired = TRUE) %>%
  adjust_pvalue(method = "bonferroni") %>%
  add_significance() %>% 
  add_xy_position(x = "TrialTransition_pre")

df_ra_means_trpre_v2 %>% 
  ggplot(aes(x = TrialTransition_pre, y = OnlineRatingZs, color = TrialTransition_pre)) + 
  geom_boxplot(alpha = 0.7, width = 0.45, outlier.color = "black", 
               position = "identity") +
  stat_boxplot(geom = "errorbar", width = 0.15, 
               position = position_dodge(width = 0.5)) + 
  geom_jitter(shape = 1) + 
  stat_pvalue_manual(t_ra_v2, xmin = "group1", xmax = "group2",
                     y.position = 0.27, label = "{p.adj.signif}", hide.ns = FALSE) +
  scale_color_jco() + 
  labs(title = "Rating t-test", x = "TrialTransition_pre", y = "Rating (z-scored)", 
       color = "Trial Transition_pre") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
df_ra_means_trpre_v2 %>% 
  ggpaired(x = "TrialTransition_pre", y = "OnlineRatingZs", id = "PID",
           color = "TrialTransition_pre", line.color = "grey", 
           palette = "jco") + 
  stat_pvalue_manual(t_ra_v2, xmin = "group1", xmax = "group2",
                     y.position = 0.3, label = "p.adj.signif", hide.ns = FALSE) +
  labs(title = "Rating pairs", x = "Trial Transition_pre", y = "Rating(z-scored)", 
       color = "Trial Transition_pre") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




### Trial_Demand*Trial_Transition_pre
dataframe: mean of ratings
```{r}
df_means_on_de_trpre_V2 <- 
  df_v2_good_LEARNING %>% 
  filter(!is.na(TrialTransition_pre)) %>%
  group_by(PID, TaskDemand, TrialTransition_pre) %>% 
  summarise(OnlineRatingZ = mean(OnlineRatingZ, na.rm=TRUE),
            RTZ = mean(RTZ, na.rm=TRUE),
            ACCZ = mean(ACCZ, na.rm=TRUE,),
            .groups = "drop")
```


boxplot (grandmean of participants)
```{r}
df_ra_means_de_trpre_v2 <- 
  df_v2_good_LEARNING %>% 
  filter(!is.na(TrialTransition_pre)) %>%
  group_by(PID, TaskDemand, TrialTransition_pre) %>% 
  summarise(OnlineRatingZs = mean(OnlineRatingZ, na.rm = TRUE), .groups = "drop")

t_ra_v2 <-
  df_ra_means_de_trpre_v2 %>%
  group_by(TaskDemand) %>% 
  t_test(OnlineRatingZs ~ TrialTransition_pre, paired = TRUE, detailed = TRUE) %>%
  add_significance() %>% 
  add_xy_position(x = "TaskDemand")

df_ra_means_de_trpre_v2 %>% 
  ggplot(aes(x = TrialTransition_pre, y = OnlineRatingZs, color = TaskDemand)) + 
  geom_boxplot(alpha = 0.7, width = 0.45, outlier.color = "black", 
               position = position_dodge(width = 0.5)) +
  stat_boxplot(geom = "errorbar", width = 0.15, 
               position = position_dodge(width = 0.5)) + 
  geom_jitter(shape = 1, position = position_dodge(width = 0.5)) + 
  scale_color_lancet() + 
  stat_pvalue_manual(t_ra_v2, step.group.by = "TaskDemand", label = "p = {p}{p.signif}",
                     y.position = c(1.2, 1.3),
                     hide.ns = FALSE) +
  labs(title = "Rating t-test", x = "Trial Transition Pre", y = "Rating(z-scored)", 
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




## Offline Ratings
### Rating ~ Task Demand
```{r}
df_off_ra_means_de_v2 <- 
  df_v2_good_OFFLINERATING %>% 
  filter(!PID %in% excludedPIDs_v2) %>% 
  group_by(PID, OfflineRatingTimePoint, TrialDemand) %>% 
  summarise(OfflineRatingZs = mean(OfflineRatingZ, na.rm=TRUE), .groups = "drop")
```

```{r}
df_off_ra_means_de_v2_by_demands <- 
  df_off_ra_means_de_v2 %>% 
  pivot_wider(names_from = TrialDemand, values_from = OfflineRatingZs,
              names_glue = "OfflineRatingZs_{TrialDemand}") %>% 
  mutate(OfflineRatingZs_diff = OfflineRatingZs_easy - OfflineRatingZs_hard) %>% 
  mutate(DemandAversive_off = ifelse(OfflineRatingZs_diff > 0, "demand_aversive", "no_demand_aversive"))
```

```{r}
df_off_ra_means_de_v2_by_times <- 
  df_off_ra_means_de_v2 %>% 
  pivot_wider(names_from = OfflineRatingTimePoint, 
              values_from = OfflineRatingZs,
              names_glue = "OfflineRatingZs_{OfflineRatingTimePoint}")
```

```{r}
df_off_ra_means_de_v2_by_demands_by_times <- 
  df_off_ra_means_de_v2_by_demands %>% 
  pivot_wider(names_from = OfflineRatingTimePoint, 
              values_from = c(OfflineRatingZs_easy,
                              OfflineRatingZs_hard,
                              OfflineRatingZs_diff,
                              DemandAversive_off),
              names_glue = "{.value}_{OfflineRatingTimePoint}")
```


### boxplot
```{r}
t_off_ra_de_v2 <- 
  df_off_ra_means_de_v2 %>% 
  group_by(OfflineRatingTimePoint) %>% 
  t_test(OfflineRatingZs ~ TrialDemand, paired = TRUE, detailed = TRUE) %>% 
  add_significance() %>% 
  add_xy_position(x = "OfflineRatingTimePoint")
```


```{r}
df_off_ra_means_de_v2 %>% 
  ggplot(aes(x = OfflineRatingTimePoint, y = OfflineRatingZs, color = TrialDemand)) +
  geom_boxplot(width = 0.5, position = position_dodge(width = 0.8)) +
  stat_boxplot(geom = "errorbar", width = 0.15, position = position_dodge(width = 0.8)) +
  geom_jitter(position = position_dodge2(width = 0.8), alpha = 0.7) +
  stat_pvalue_manual(t_off_ra_de_v2, label = "p.signif", hide.ns = FALSE,
                     y.position = 3.5) + 
  scale_color_lancet() + 
  scale_y_continuous(limits = c(-5, 5)) +
  labs(title = "Offline Rating T-test on Task Demand", x = "Time Point", color = "Task Demand") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
extremes_pids <- 
  df_off_ra_means_de_v2 %>% 
  group_by(OfflineRatingTimePoint, TrialDemand) %>% 
  filter(OfflineRatingZs == min(OfflineRatingZs) | OfflineRatingZs == max(OfflineRatingZs))
# PID 114 extreme offline ratings.
```


### compare offline diff in Timepoints
```{r}
t_off_raz_diff_time_v2 <- 
  df_off_ra_means_de_v2_by_demands %>% 
  pairwise_t_test(OfflineRatingZs_diff ~ OfflineRatingTimePoint,
                  paired = TRUE, p.adjust.method = "bonferroni") %>% 
  add_significance() %>% 
  add_xy_position(x="OfflineRatingTimePoint")
```

```{r}
df_off_ra_means_de_v2_by_demands %>% 
  ggplot(aes(x = OfflineRatingTimePoint, y = OfflineRatingZs_diff)) +
  geom_boxplot(width = 0.5, position = position_dodge(width = 0.5)) +
  stat_boxplot(geom = "errorbar", width = 0.15, position = position_dodge(width = 0.5)) +
  geom_jitter(position = position_dodge2(width = 0.5), alpha = 0.7) +
  stat_pvalue_manual(t_off_raz_diff_time_v2, 
                     label = "p = {p.adj}{p.adj.signif}", hide.ns = FALSE,
                     y.position = c(8.2, 9.2, 10.5)) + 
  scale_color_lancet() + 
  scale_y_continuous(limits = c(-5, 11)) +
  labs(title = "Offline Rating Diff (easy - hard) T-test on Time Points", 
       x = "Time Point") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```



## True Choice
### Descriptive
```{r}
df_v2_good_TRUECHOICE <- 
  df_v2_good_TRUECHOICE %>% 
  mutate(DemandChoice_code = ifelse(ChoiceDemand == "easy", 1, 0)) %>% 
  merge(df_ra_means_de_diff_v2[, c("PID", "DemandAversive_on")], by="PID", all.x = T)
```

```{r}
df_true_dc_props_easy_v2 <- 
  df_v2_good_TRUECHOICE %>% 
  group_by(PID,DemandAversive_on) %>% 
  summarise(DEC_prop_true = mean(DemandChoice_code, na.rm=TRUE), .groups = "drop") %>% 
  mutate(DemandAvoidance_true = ifelse(DEC_prop_true > 0.5, "demand_avoid", "no_demand_avoid"))
```

```{r}
table(df_true_dc_props_easy_v2$DemandAvoidance_true)
```

9 participants' easy choice rates < 0.5 (no demand avoidance)
```{r}
# ggplot(df_true_dc_props_easy, aes(x="", y = DCERs_true, label=PID)) +
df_true_dc_props_easy_v2 %>% 
  group_by(DemandAversive) %>% 
  ggplot(data = ., aes(x=DemandAversive_on, y = DEC_prop_true)) +
  # ggplot(data = ., aes(x="", y = DEC_prop_true)) +
  geom_boxplot(position="identity", width = 0.4) +
  geom_point(aes(group = PID, color=DemandAvoidance_true), 
             alpha = 0.8, position = position_dodge2(width = 0.15)) +
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level") ,color = "red") + 
  geom_hline(aes(yintercept = mean(DEC_prop_true, na.rm = TRUE), linetype = "Mean") ,color = "blue") + 
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  stat_boxplot(geom = "errorbar", width = 0.15) + 
  scale_y_continuous(limits = c(0, 1), n.breaks = 10) +
  # geom_text_repel(aes(color=DemandAvoidance_true), max.overlaps = nrow(df_true_dc_props_easy)) +
  scale_color_d3()+
  labs(title = "Easy Choice Probability in True Choice Phase", 
       x="DemandAversive", 
       # x="", 
       y = "P(Easy Choice)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

```{r}
w_prop_true_v2 <- 
  df_true_dc_props_easy_v2 %>% 
  wilcox_test(DEC_prop_true ~ 1, mu=.5, alternative = "greater", detailed = TRUE) %>% 
  add_significance()
```

group by DemandAversive
```{r}
w_prop_true_v2_by_aversive_on <- 
  df_true_dc_props_easy_v2 %>% 
  group_by(DemandAversive_on) %>% 
  wilcox_test(DEC_prop_true ~ 1, mu=.5, alternative = "greater", detailed = TRUE) %>% 
  add_significance()
# significantly > 0.5 among aversive group
```



### Choice Curves
#### Participant Proportions
All
```{r}
df_v2_good_TRUECHOICE_mean <- 
  df_v2_good_TRUECHOICE %>% 
  group_by(TrialN) %>% 
  summarise(DEC_parti_prop_true = mean(DemandChoice_code))
```

```{r}
df_v2_good_TRUECHOICE_mean %>% 
  ggplot(aes(x = TrialN, y = DEC_parti_prop_true)) +
  geom_point()+
  geom_line()+
  scale_x_continuous(breaks = seq(1, 50)) +
  scale_y_continuous(limits = c(0,1), n.breaks = 10) +
  labs(title = "Easy Choice Participant Proportion (mean) ", x="TrialN", y = "Proportion") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

group by Demand aversive
```{r}
df_v2_good_TRUECHOICE_mean_by_aversive_on <- 
  df_v2_good_TRUECHOICE %>% 
  group_by(TrialN, DemandAversive_on) %>% 
  summarise(DEC_parti_prop_true = mean(DemandChoice_code), .groups = "drop")
```

```{r}
df_v2_good_TRUECHOICE_mean_by_aversive %>% 
  ggplot(aes(x = TrialN, y = DEC_parti_prop_true, color = DemandAversive)) +
  geom_point()+
  geom_line()+
  scale_color_d3()+
  scale_x_continuous(breaks = seq(1, 50)) +
  scale_y_continuous(limits = c(0,1), n.breaks = 10) +
  labs(title = "Easy Choice Participant Proportion (mean) ", x="TrialN", y = "Proportion") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```



#### Accumulated proportion by trialN
All
```{r}
df_true_dc_props_easy_by_trial_v2 <- 
  df_v2_good_TRUECHOICE %>%
  group_by(PID) %>% 
  mutate(DEC_prop_true = cummean(DemandChoice_code)) %>% 
  mutate(DemandAvoidance_false = ifelse(DEC_prop_true > 0.5, "demand_avoid", "no_demand_avoid"))
```

```{r}
df_true_dc_props_easy_by_trial_v2_mean <- 
  df_true_dc_props_easy_by_trial_v2 %>% 
  group_by(TrialN) %>% 
  summarise(DEC_prop_true_mean = mean(DEC_prop_true, na.rm = TRUE), .groups = "drop")
```

```{r}
df_true_dc_props_easy_by_trial_v2_mean %>% 
  ggplot(aes(x=TrialN, y=DEC_prop_true_mean))+
  geom_line(linewidth = 0.7)+
  geom_point(shape=17)+
  # scale_color_d3()+
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level") ,color = "red")+
  geom_hline(aes(yintercept = mean(DEC_prop_true_mean, na.rm = TRUE), 
                 linetype = "Mean across TrialN"),color = "blue")+
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  geom_boxplot(data=df_true_dc_props_easy_by_trial_v2, 
               aes(x=TrialN, y=DEC_prop_true, group = TrialN), 
               width = 0.35, outlier.shape = 8, alpha = 0.3)+
  geom_point(data=df_true_dc_props_easy_by_trial_v2, 
               aes(x=TrialN, y=DEC_prop_true, group = TrialN,
                   color = ChoiceDemand), 
               position = position_dodge2(width = 0.35), alpha = 0.3)+
  scale_color_lancet() + 
  stat_boxplot(data=df_true_dc_props_easy_by_trial_v2, 
               aes(x=TrialN, y=DEC_prop_true, group = TrialN),
               geom = "errorbar", width = 0.15) + 
  scale_x_continuous(limits = c(0, 11), n.breaks = 10, expand = c(0,0))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability in True Choice Phase", x="TrialN", y = "P(Easy Choice)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```


```{r}
df_true_dc_props_easy_by_trial_v2_mean %>% 
  ggplot(aes(x=TrialN, y=DEC_prop_true_mean))+
  geom_line(linewidth = 0.7)+
  geom_point()+
  geom_point(data=df_true_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_true, group = TrialN, color = factor(PID)),
               position = position_dodge2(width = 0.15), size = 0.5 ,alpha = 0.3)+
  geom_line(data=df_true_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_true, color = factor(PID)),
            linetype = "dotted", alpha = 0.5)+
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level") ,color = "red")+
  geom_hline(aes(yintercept = mean(DEC_prop_true_mean, na.rm = TRUE), 
                 linetype = "Mean across TrialN"),color = "blue")+
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  scale_x_continuous(limits = c(0, 11), n.breaks = 10, expand = c(0,0))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability in True Choice Phase", x="TrialN", y = "P(Easy Choice)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```


group by DemandAversive
```{r}
df_true_dc_props_easy_by_trial_v2_by_aversive <- 
  df_v2_good_TRUECHOICE %>%
  group_by(PID, DemandAversive) %>% 
  mutate(DEC_prop_true = cummean(DemandChoice_code)) %>% 
  mutate(DemandAvoidance_false = ifelse(DEC_prop_true > 0.5, "demand_avoid", "no_demand_avoid"))
```

```{r}
df_true_dc_props_easy_by_trial_v2_mean_by_aversive <- 
  df_true_dc_props_easy_by_trial_v2_by_aversive %>% 
  group_by(TrialN, DemandAversive) %>% 
  summarise(DEC_prop_true_mean = mean(DEC_prop_true, na.rm = TRUE), .groups = "drop")
```

```{r}
df_true_dc_props_easy_by_trial_v2_mean_by_aversive %>% 
  ggplot(aes(x=TrialN, y=DEC_prop_true_mean))+
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level") ,color = "red")+
  geom_hline(aes(yintercept = mean(DEC_prop_true_mean, na.rm = TRUE), 
                 linetype = "Mean across TrialN"),color = "blue")+
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  geom_boxplot(data=df_true_dc_props_easy_by_trial_v2_by_aversive, 
               aes(x=TrialN, y=DEC_prop_true, group = TrialN), 
               width = 0.35, outlier.shape = 8, alpha = 0.3)+
  geom_point(data=df_true_dc_props_easy_by_trial_v2_by_aversive, 
               aes(x=TrialN, y=DEC_prop_true, group = TrialN,
                   color = ChoiceDemand), 
               position = position_dodge2(width = 0.35), alpha = 0.3)+
  geom_line(linewidth = 0.7)+
  geom_point(shape=17)+
  # scale_color_d3()+
  scale_color_lancet() +
  stat_boxplot(data=df_true_dc_props_easy_by_trial_v2_by_aversive, 
               aes(x=TrialN, y=DEC_prop_true, group = TrialN),
               geom = "errorbar", width = 0.15) + 
  scale_x_continuous(limits = c(0, 11), n.breaks = 10, expand = c(0,0))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  facet_wrap(~DemandAversive)+
  labs(title = "Easy Choice Probability in True Choice Phase", x="TrialN", y = "P(Easy Choice)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

each participant
```{r}
df_true_dc_props_easy_by_trial_v2_mean_by_aversive %>% 
  ggplot(aes(x=TrialN, y=DEC_prop_true_mean))+
  geom_point(data=df_true_dc_props_easy_by_trial_v2_by_aversive,
               aes(x=TrialN, y=DEC_prop_true, group = TrialN, color = factor(PID)),
               position = position_dodge2(width = 0.15), size = 0.5 ,alpha = 0.3)+
  geom_line(data=df_true_dc_props_easy_by_trial_v2_by_aversive,
               aes(x=TrialN, y=DEC_prop_true, color = factor(PID)),
            linetype = "dotted", alpha = 0.5)+
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level") ,color = "red")+
  geom_hline(aes(yintercept = mean(DEC_prop_true_mean, na.rm = TRUE), 
                 linetype = "Mean across TrialN"),color = "blue")+
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  geom_line(linewidth = 0.7)+
  geom_point()+
  facet_wrap(~DemandAversive)+
  scale_x_continuous(limits = c(0, 11), n.breaks = 10, expand = c(0,0))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability in True Choice Phase", 
       x="TrialN", y = "P(Easy Choice)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```




## False Choice
### Descriptive
```{r}
df_v2_good_FALSECHOICE <- 
  df_v2_good_FALSECHOICE %>% 
  mutate(DemandChoice_code = ifelse(ChoiceDemand == "easy", 1, 0)) %>% 
  merge(df_ra_means_de_diff_v2[, c("PID", "DemandAversive_on")], by = "PID", all.x = T)
```


```{r}
df_false_dc_props_easy_v2 <- 
  df_v2_good_FALSECHOICE %>% 
  group_by(PID, DemandAversive_on) %>% 
  summarise(DEC_prop_false = mean(DemandChoice_code, na.rm=TRUE), .groups = "drop") %>% 
  mutate(DemandAvoidance_false = ifelse(DEC_prop_false > 0.5, "demand_avoid", "no_demand_avoid")) 
```


Boxplots of easy choice proportions
Overall
```{r}
df_false_dc_props_easy_by_trial_v2 %>% 
  filter(TrialN ==50) %>% 
  ggplot(aes(x="", y=DEC_prop_false))+
  geom_boxplot(position="identity", width = 0.4)+
  geom_point(position = position_dodge2(width = 0.15), 
             aes(color = DemandAvoidance_false), alpha = 0.8)+
  stat_boxplot(geom = "errorbar", width = 0.15)+
  scale_color_d3()+
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level") ,color = "red")+
  geom_hline(aes(yintercept = mean(DEC_prop_false, na.rm = TRUE), linetype = "Mean") ,color = "blue")+
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability in False Choice Phase", x="", y = "P(Easy Choice)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

```{r}
table(df_false_dc_props_easy_v2$DemandAvoidance_false)
```

```{r}
w_prop_false_v2 <- 
  df_false_dc_props_easy_v2 %>% 
  wilcox_test(DEC_prop_false ~ 1, mu=.5, alternative = "greater", detailed = TRUE) %>% 
  add_significance()
```



### False Choice Learning Curve
#### Participant Proportion
All
```{r}
df_v2_good_FALSECHOICE_mean <- 
  df_v2_good_FALSECHOICE %>% 
  group_by(TrialN) %>% 
  summarise(DEC_parti_prop_false = mean(DemandChoice_code))
```

```{r}
df_v2_good_FALSECHOICE_mean %>% 
  ggplot(aes(x = TrialN, y = DEC_parti_prop_false)) +
  geom_point()+
  geom_line()+
  scale_x_continuous(breaks = seq(1, 50)) +
  labs(title = "Easy Choice Participant Proportion (mean) ", x="TrialN", y = "Proportion") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

```{r}
df_v2_good_FALSECHOICE_count <- 
  df_v2_good_FALSECHOICE %>% 
  group_by(TrialN, ChoiceDemand) %>% 
  summarise(DEC_parti_count_false = n(), .groups = "drop")
```

```{r}
df_v2_good_FALSECHOICE_count %>% 
  ggplot(aes(x = TrialN, y = DEC_parti_count_false, color=ChoiceDemand)) +
  geom_point()+
  geom_line()+
  scale_x_continuous(breaks = seq(1, 50)) +
  scale_color_lancet()+
  labs(title = "Amount of Participants Choosing Easy/Hard", x="TrialN", y = "Count") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```


group by Demand Aversive
```{r}
df_v2_good_FALSECHOICE_mean_by_aversive <- 
  df_v2_good_FALSECHOICE %>% 
  group_by(TrialN, DemandAversive_on) %>% 
  summarise(DEC_parti_prop_false = mean(DemandChoice_code), .groups = "drop")
```

```{r}
df_v2_good_FALSECHOICE_mean_by_aversive %>% 
  ggplot(aes(x = TrialN, y = DEC_parti_prop_false, color = DemandAversive)) +
  geom_point()+
  geom_line()+
  scale_color_d3()+
  scale_x_continuous(breaks = seq(1, 50)) +
  labs(title = "Easy Choice Participant Proportion (mean) ", 
       x="TrialN", y = "Proportion") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

```{r}
df_v2_good_FALSECHOICE_count_by_aversive <- 
  df_v2_good_FALSECHOICE %>% 
  group_by(TrialN, DemandAversive, ChoiceDemand) %>% 
  summarise(DEC_parti_count_false = n(), .groups = "drop")
```

```{r}
df_v2_good_FALSECHOICE_count_by_aversive %>% 
  ggplot(aes(x = TrialN, y = DEC_parti_count_false, color=ChoiceDemand)) +
  geom_point()+
  geom_line()+
  scale_x_continuous(breaks = seq(1, 50)) +
  scale_color_lancet()+
  facet_wrap(~DemandAversive)+
  labs(title = "Amount of Participants Choosing Easy/Hard", x="TrialN", y = "Count") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```



#### Accumulated proportion by trialN
```{r}
df_false_dc_props_easy_by_trial_v2 <- 
  df_v2_good_FALSECHOICE %>%
  group_by(PID, DemandAversive) %>% 
  mutate(DEC_prop_false = cummean(DemandChoice_code)) %>% 
  mutate(DemandAvoidance_false = ifelse(DEC_prop_false > 0.5, "demand_avoid", "no_demand_avoid"))
```


All
```{r}
df_false_dc_props_easy_by_trial_v2_mean <- 
  df_false_dc_props_easy_by_trial_v2 %>% 
  group_by(TrialN) %>% 
  summarise(DEC_prop_false_mean = mean(DEC_prop_false, na.rm = TRUE), .groups = "drop")
```

```{r}
df_false_dc_props_easy_by_trial_v2_mean %>% 
  ggplot(aes(x=TrialN, y=DEC_prop_false_mean))+
  geom_line(linewidth=0.7)+
  geom_point(shape=17)+
  geom_boxplot(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, group = TrialN),
               width = 0.35, outlier.shape = 8, alpha = 0.3)+
  geom_point(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, group = TrialN),
               position = position_dodge2(width = 0.15), size = 0.5 ,alpha = 0.3)+
  geom_line(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, color = factor(PID)),
            linetype = "dotted")+
  stat_boxplot(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, group = TrialN),
               geom = "errorbar", width = 0.15) +
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level"), 
             color = "red", linewidth = 0.7)+
  geom_hline(aes(yintercept = mean(DEC_prop_false_mean, na.rm = TRUE), linetype = "Mean across TrialN"),
             color = "blue", linewidth = 0.7)+
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  
  scale_x_continuous(limits = c(0, 51), n.breaks = 50, expand = c(0,0))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability in False Choice Phase", x="TrialN", y = "P(Easy Choice)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

group by Demand Aversive
```{r}
df_false_dc_props_easy_by_trial_v2_mean_by_aversive <- 
  df_false_dc_props_easy_by_trial_v2 %>% 
  group_by(TrialN, DemandAversive) %>% 
  summarise(DEC_prop_false_mean = mean(DEC_prop_false, na.rm = TRUE), .groups = "drop")
```

```{r}
df_false_dc_props_easy_by_trial_v2_mean_by_aversive %>% 
  ggplot(aes(x=TrialN, y=DEC_prop_false_mean, group = DemandAversive))+
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level"), 
             color = "red", linewidth = 0.7)+
  geom_hline(aes(yintercept = mean(DEC_prop_false_mean[DemandAversive == "demand_aversive"], na.rm = TRUE), 
                 linetype = "Mean-Aversive"), # Mean across TrialN of Aversive group
             color = "blue", linewidth = 0.7)+
  geom_hline(aes(yintercept = mean(DEC_prop_false_mean[DemandAversive == "no_demand_aversive"], na.rm = TRUE), 
                 linetype = "Mean-No-Aversive"), #Mean across TrialN of No-Aversive group
             color = "orange", linewidth = 0.7)+
  scale_linetype_manual(name = "Lines", values = c(2, 2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue", "orange"))))+
  geom_boxplot(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, group = TrialN),
               width = 0.35, outlier.shape = 8, alpha = 0.3)+
  geom_point(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, group = TrialN, color = ChoiceDemand),
               position = position_dodge2(width = 0.15), size = 0.5 ,alpha = 0.3)+
  # geom_line(data=df_false_dc_props_easy_by_trial_v2,
  #              aes(x=TrialN, y=DEC_prop_false),
  #           linetype = "dotted")+
  stat_boxplot(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, group = TrialN),
               geom = "errorbar", width = 0.15) +
  geom_line(linewidth=0.7)+
  geom_point(shape=17)+
  scale_color_lancet()+
  facet_wrap(~DemandAversive)+
  scale_x_continuous(limits = c(0, 51), n.breaks = 50, expand = c(0,0))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability in False Choice Phase", x="TrialN", y = "P(Easy Choice)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```


each participant
```{r}
df_false_dc_props_easy_by_trial_v2_mean_by_aversive %>% 
  ggplot(aes(x=TrialN, y=DEC_prop_false_mean))+
  geom_boxplot(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, group = TrialN),
               width = 0.35, outlier.shape = 8, alpha = 0.3)+
  geom_point(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, group = TrialN),
               position = position_dodge2(width = 0.15), size = 0.5 ,alpha = 0.3)+
  geom_line(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, color = factor(PID)),
            linetype = "dotted")+
  stat_boxplot(data=df_false_dc_props_easy_by_trial_v2,
               aes(x=TrialN, y=DEC_prop_false, group = TrialN),
               geom = "errorbar", width = 0.15) +
  geom_line(linewidth=0.7)+
  geom_point(shape=17)+
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level"), 
             color = "red", linewidth = 0.7)+
  geom_hline(aes(yintercept = mean(DEC_prop_false_mean, na.rm = TRUE), linetype = "Mean across TrialN"),
             color = "blue", linewidth = 0.7)+
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  facet_wrap(~DemandAversive)+
  scale_x_continuous(limits = c(0, 51), n.breaks = 50, expand = c(0,0))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability in False Choice Phase", x="TrialN", y = "P(Easy Choice)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```




#### Separate Proportions by sections (10 trials) 
```{r}
# review the number of easy and hard trials in each trial section
# it turns out to be relatively balanced

result_table <- 
  df_v2_good_FALSECHOICE %>%
  mutate(Trial_N_Group = cut(TrialN, breaks = c(1, 10, 20, 30, 40, 50), labels = FALSE)) %>%
  # mutate(Trial_N_Group = cut(TrialN, breaks = c(1,5,10,15,20,25,30,35,40,45,50), labels = FALSE)) %>%
  group_by(PID, Trial_N_Group) %>%
  summarise(DEC_prop_false = mean(DemandChoice_code, na.ra = TRUE), .groups = "drop")

# Print the resulting table
# print(result_table)

```


#### lines plot of false choice rates in easy and hard demands
```{r}
# Create a function to calculate mean for each section of trials
calculate_section_means <- function(data, col_reps, section) {
  data %>%
    group_by(PID) %>%
    summarise(across(starts_with(col_reps),
                     ~ mean(.[TrialN %in% section], na.rm = TRUE),
                     .names = "DEC_prop_false"), .groups = "drop")
}

# Define the sections
trial_sections <- list("10" = 1:10, "20" = 11:20, "30" = 21:30, "40" = 31:40, "50" = 41:50)
# trial_sections <- list("05" = 1:5, "10" = 6:10,
#                        "15" = 11:15, "20" = 16:20,
#                        "25" = 21:25, "30" = 26:30,
#                        "35" = 31:35, "40" = 36:40,
#                        "45" = 41:45, "50" = 46:50)

# Initialize an empty list to store the results
result_list <- list()

# Use lapply to iterate over each section and calculate means
result_list <- lapply(names(trial_sections), function(section_name) {
  section <- trial_sections[[section_name]]
  
  # Calculate means for the current section
  result <- calculate_section_means(df_v2_good_FALSECHOICE, "DemandChoice_code", section)
  
  # Add a column indicating the section name
  result$Section <- section_name
  
  return(result)
})

# Combine the results into a single data frame
df_false_dc_means_demands_tsecs_v2 <- 
  bind_rows(result_list) %>% 
  mutate(DemandAvoidance_false = ifelse(DEC_prop_false > 0.5, "demand_avoid", "no_demand_avoid")) %>% 
  merge(df_ra_means_de_diff_v2[, c("PID", "DemandAversive_on")], by = "PID", all.x = T)
```

```{r}
# use for correlation analysis
df_false_dc_means_demands_tsecs_v2_wide <- 
  df_false_dc_means_demands_tsecs_v2 %>% 
  dplyr::select(-DemandAvoidance_false) %>% 
  pivot_wider(names_from = Section, values_from = DEC_prop_false, names_prefix = "DEC_prop_false_") %>% 
  mutate(DEC_prop_false_drop = DEC_prop_false_10 - DEC_prop_false_50) %>% 
  merge(df_ra_means_de_diff_v2[, c("PID", "DemandAversive_on")], by = "PID", all.x = T)
```

```{r}
w_prop_false_sec1_v2 <- 
  df_false_dc_means_demands_tsecs_v2_wide %>% 
  wilcox_test(DEC_prop_false_10 ~ 1, mu=.5, alternative = "greater", detailed = TRUE) %>% 
  add_significance()

w_prop_false_sec2_v2 <- 
  df_false_dc_means_demands_tsecs_v2_wide %>% 
  wilcox_test(DEC_prop_false_20 ~ 1, mu=.5, alternative = "greater", detailed = TRUE) %>% 
  add_significance()

w_prop_false_sec3_v2 <- 
  df_false_dc_means_demands_tsecs_v2_wide %>% 
  wilcox_test(DEC_prop_false_30 ~ 1, mu=.5, alternative = "greater", detailed = TRUE) %>% 
  add_significance()

w_prop_false_sec4_v2 <- 
  df_false_dc_means_demands_tsecs_v2_wide %>% 
  wilcox_test(DEC_prop_false_40 ~ 1, mu=.5, alternative = "greater", detailed = TRUE) %>% 
  add_significance()

# Section 5 became non-significant
w_prop_false_sec5_v2 <- 
  df_false_dc_means_demands_tsecs_v2_wide %>% 
  wilcox_test(DEC_prop_false_50 ~ 1, mu=.5, alternative = "greater", detailed = TRUE) %>% 
  add_significance()
```


```{r}
df_false_dc_grandmeans_demands_tsecs_v2 <-
  df_false_dc_means_demands_tsecs_v2 %>% 
  group_by(Section) %>% 
  summarise(DEC_prop_false_gmean = mean(DEC_prop_false, na.rm = TRUE), .groups = "drop") 
```

points in different colors of each participant
```{r}
df_false_dc_means_demands_tsecs_v2 %>% 
  ggplot(aes(x = Section, y = DEC_prop_false))+
  geom_boxplot(position = position_dodge(width = 0.5))+
  stat_boxplot(geom = "errorbar", width = 0.15) +
  geom_point(aes(color = factor(PID)), position = position_dodge(width = 0.35), alpha = 0.5)+
  stat_summary(fun = "mean", shape = 17)+
  labs(title = "Easy Choice Probability in False Choice Phase", 
       x="Trial Section", y = "P (Easy Choice)", color = "PID") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```


grand means of participants in each trial section
```{r}
df_false_dc_grandmeans_demands_tsecs_v2 %>% 
  mutate(TrialSection = as.numeric(Section)) %>%
  ggplot(aes(x = TrialSection, y = DEC_prop_false_gmean)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level"), color = "red") +
  geom_hline(aes(yintercept = mean(df_false_dc_means_demands_tsecs_v2$DEC_prop_false, na.rm = TRUE), 
             linetype = "Mean"), color = "blue") +
  # scale_x_continuous(breaks = c(5,10,15,20,25,30,35,40,45,50))+
  scale_x_continuous(breaks = c(10,20,30,40,50))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability by Trial Sections in False Choice Phase", 
       x = "Trial Section", y = "Easy Choice Probability (grandmean)") +
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```


The first 10 trials
```{r}
df_false_dc_means_demands_tsecs_v2 %>% 
  filter(Section == 10) %>% 
  ggplot(aes(x="", y=DEC_prop_false))+
  geom_boxplot(position="identity", width = 0.4)+
  geom_point(position = position_dodge2(width = 0.15), aes(color = DemandAvoidance_false))+
  stat_boxplot(geom = "errorbar", width = 0.15)+
  scale_color_d3()+
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level"),color = "red")+
  geom_hline(aes(yintercept = mean(DEC_prop_false, na.rm = TRUE), 
                 linetype = "Mean") ,color = "blue")+
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability in False Choice Phase 1-10 Trials", x="", y = "P(Easy Choice)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

Identify participants without exploration in first 10 trials
```{r}
no_explore_pid <- 
  df_false_dc_means_demands_tsecs_v2 %>% 
  filter(PID %in% PID[Section == 10 & DEC_prop_false == 1]) %>% 
  dplyr::select(PID)
no_explore_pid <- unique(as.numeric(no_explore_pid$PID))

df_false_dc_means_demands_tsecs_v2_explored <-  
  df_false_dc_means_demands_tsecs_v2 %>% 
  filter(!PID %in% no_explore_pid)
  
df_false_dc_grandmeans_demands_tsecs_v2_explored <-
  df_false_dc_means_demands_tsecs_v2_explored %>%
  group_by(Section) %>% 
  summarise(DEC_prop_false_gmean = mean(DEC_prop_false, na.rm = TRUE), .groups = "drop") 
```

```{r}
# Plot
df_false_dc_grandmeans_demands_tsecs_v2_explored %>% 
  mutate(TrialSection = as.numeric(Section)) %>%
  ggplot(aes(x = TrialSection, y = DEC_prop_false_gmean)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level"), color = "red") +
  geom_hline(aes(yintercept = mean(df_false_dc_means_demands_tsecs_v2_explored$DEC_prop_false, na.rm = TRUE),
             linetype = "Mean across TrialN"), color = "blue") +
  # scale_x_continuous(breaks = c(5,10,15,20,25,30,35,40,45,50))+
  scale_x_continuous(breaks = c(10,20,30,40,50))+
  # scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "\n Easy Choice Probability by Trial Sections in False Choice Phase \n
       (exclude participants without exploration in 1st section)", 
       x = "Trial Section", y = "Easy Choice Probability (grandmean)") +
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

Accumulated proportion by trialN after excluding non-exploration participants
```{r}
df_false_dc_props_easy_by_trial_v2_mean <- 
  df_false_dc_props_easy_by_trial_v2 %>% 
  group_by(TrialN) %>% 
  summarise(DEC_prop_false_mean = mean(DEC_prop_false, na.rm = TRUE), .groups = "drop")
```

```{r}
df_false_dc_props_easy_by_trial_v2_mean %>% 
  ggplot(aes(x=TrialN, y=DEC_prop_false_mean))+
  geom_line()+
  geom_point()+
  scale_color_d3()+
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level") ,color = "red")+
  geom_hline(aes(yintercept = mean(DEC_prop_false_mean, na.rm = TRUE), linetype = "Mean across TrialN"),
             color = "blue")+
  scale_linetype_manual(name = "Lines", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue"))))+
  scale_x_continuous(limits = c(0, 51), n.breaks = 50, expand = c(0,0))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability in False Choice Phase", x="TrialN", y = "P(Easy Choice)")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```

group by Demand Aversive
```{r}
df_false_dc_grandmeans_demands_tsecs_v2_by_aversive_on <-
  df_false_dc_means_demands_tsecs_v2 %>% 
  group_by(Section, DemandAversive_on) %>% 
  summarise(DEC_prop_false_gmean = mean(DEC_prop_false, na.rm = TRUE), .groups = "drop") 
```

```{r}
df_false_dc_grandmeans_demands_tsecs_v2_by_aversive %>% 
  mutate(TrialSection = as.numeric(Section)) %>%
  ggplot(aes(x = TrialSection, y = DEC_prop_false_gmean)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = 0.5, linetype = "Chance Level"), color = "red") +
  geom_hline(aes(yintercept = mean(DEC_prop_false_gmean[DemandAversive == "demand_aversive"], na.rm = TRUE), 
             linetype = "Mean-Aversive"), color = "blue") +
  geom_hline(aes(yintercept = mean(DEC_prop_false_gmean[DemandAversive == "no_demand_aversive"], na.rm = TRUE), 
             linetype = "Mean-No-Aversive"), color = "orange") +
  # scale_x_continuous(breaks = c(5,10,15,20,25,30,35,40,45,50))+
  scale_x_continuous(breaks = c(10,20,30,40,50))+
  scale_y_continuous(limits = c(0, 1), n.breaks = 10)+
  labs(title = "Easy Choice Probability by Trial Sections in False Choice Phase", 
       x = "Trial Section", y = "Easy Choice Probability (grandmean)") +
  scale_linetype_manual(name = "Lines", values = c(2, 2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "blue", "orange"))))+
  facet_wrap(~DemandAversive)+
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))
```





## Questionnaires
### Load participants' data
```{r combine files}
main_path <- "/Users/ottolab/Desktop/Yue/DARDC_v2_Data_Analysis/data_move_here"
participants_folders <- list.dirs(main_path, full.names = TRUE, recursive = FALSE)

all_dataframes <- list()

for (p_folder in participants_folders) {
  pid <- basename(p_folder)
  file_path <- file.path(p_folder, paste0(pid, "_survey.csv"))
  if (file.exists(file_path)){
    p_data <- read.csv(file_path, header = TRUE, nrows = 1)
    all_dataframes[[pid]] <- p_data
  }
  else{
    warning(paste("File not found. SONA ID:", pid))
  }
}

combined_dataframe <- do.call(rbind, all_dataframes) %>% 
  dplyr::select(-c(success, trial_type, trial_index, time_elapsed, internal_node_id, subject))
```


### Pre-processing
```{r}
# All items
all_items <- c("^.*[0-9]+") 

# BIS_BAS
bai_items <- c("^BAI.*")  # all BAI items
bai_items_reverse <- c("BAI2_bis", "BAI22_bis") # reverse items
bis_items <- c("^BAI.*bis$")  # BIS
bas_d_items <- c("^BAI.*bas_d$") # BAS drive
bas_fs_items <- c("^BAI.*bas_fs$") # BAS fun seeking
bas_rr_items <- c("^BAI.*bas_rr$") # BAS reward responsiveness

# NFC
nfc_items <- c("^NFC[0-9]+")
nfc_items_reverse <- c("NFC3", "NFC4", "NFC5", "NFC7", "NFC8", "NFC9", "NFC12", "NFC16", "NFC17")

# BFI
bfi_items <- c("BFI.*")  # all BFI items
bfi_items_reverse <- c("BFI2_agr", "BFI6_ext", "BFI12_agr", "BFI18_con", "BFI21_ext",
                       "BFI23_con", "BFI24_neu", "BFI27_agr", "BFI31_ext", "BFI34_neu",
                       "BFI35_open", "BFI37_agr", "BFI41_open", "BFI43_con")
bfi_ext_items <- c("^BFI.*ext$")  # BFI Extraversion
bfi_agr_items <- c("^BFI.*agr$")  # BFI Agreeableness
bfi_con_items <- c("^BFI.*con$")  # BFI Conscientiousness
bfi_neu_items <- c("^BFI.*neu$")  # BFI Neuroticism
bfi_open_items <- c("^BFI.*open$")  # BFI openness

# UPPS-P
upps_items <- c("^UPPS.*")  # all UPPS-P items
upps_items_reverse <- c("UPPS1_lps", "UPPS4_lps", "UPPS5_lpm", "UPPS7_lps", "UPPS12_lpm", "UPPS19_lpm")
upps_lps_items <- c("^UPPS.*lps$")  # UPPS-P Lack of Perseverance
upps_lpm_items <- c("^UPPS.*lpm$")  # UPPS-P Lack of Premeditation
upps_pu_items <- c("^UPPS.*pu$")  # UPPS-P Positive Urgency
upps_nu_items <- c("^UPPS.*nu$")  # UPPS-P Negative Urgency
```


```{r Rescore Items}
rescore_items <- function(data, items){
  data %>%
    mutate(across(matches(items), ~ dplyr::recode(., "0" = 1L, "1" = 2L, "2" = 3L, "3" = 4L, "4" = 5L)))
}

all_items <- c("^.*[0-9]+") 

combined_dataframe_rescored <- 
  rescore_items(combined_dataframe, all_items) %>% 
  dplyr::select(PID, SONA, matches(all_items))
```


```{r Reverse-score Items}
reverse_score_items_bai <- function(data, items){
  data %>%
    mutate(across(items, ~ dplyr::recode(., "1" = 4L, "2" = 3L, "3" = 2L, "4" = 1L)))
}

reverse_score_items_nfc <- function(data, items){
  data %>%
    mutate(across(items, ~ dplyr::recode(., "1" = 5L, "2" = 4L, "3" = 3L, "4" = 2L, "5" = 1L)))
}

reverse_score_items_bfi <- function(data, items){
  data %>%
    mutate(across(items, ~ dplyr::recode(., "1" = 5L, "2" = 4L, "3" = 3L, "4" = 2L, "5" = 1L)))
}

reverse_score_items_upps <- function(data, items){
  data %>%
    mutate(across(items, ~ dplyr::recode(., "1" = 4L, "2" = 3L, "3" = 2L, "4" = 1L)))
}


# Reverse scoring
combined_dataframe_reversed <- 
  combined_dataframe_rescored %>% 
  reverse_score_items_bai(., bai_items_reverse) %>%  ## BIS_BAS
  reverse_score_items_nfc(., nfc_items_reverse) %>%  ## NFC
  reverse_score_items_bfi(., bfi_items_reverse) %>%  ## BFI
  reverse_score_items_upps(., upps_items_reverse)  ## UPPS-P
```


Compute scores of variables
```{r}
scale_items <- function(x, na.rm = TRUE){
  (x - mean(x)) / sd(x)
  }
```


```{r}
ques_items <- c(".*_sum$")

# compute sum of each subscale
df_ques <- 
  combined_dataframe_reversed %>% 
  mutate(BIS_sum = rowSums(dplyr::select(., dplyr::matches(bis_items)), na.rm = TRUE),
         BAS_D_sum = rowSums(dplyr::select(., dplyr::matches(bas_d_items)), na.rm = TRUE),
         BAS_FS_sum = rowSums(dplyr::select(., dplyr::matches(bas_fs_items)), na.rm = TRUE),
         BAS_RR_sum = rowSums(dplyr::select(., dplyr::matches(bas_rr_items)), na.rm = TRUE),
         NFC_sum = rowSums(dplyr::select(., dplyr::matches(nfc_items)), na.rm = TRUE),
         BFI_EXT_sum = rowSums(dplyr::select(., dplyr::matches(bfi_ext_items)), na.rm = TRUE),
         BFI_AGR_sum = rowSums(dplyr::select(., dplyr::matches(bfi_agr_items)), na.rm = TRUE),
         BFI_CON_sum = rowSums(dplyr::select(., dplyr::matches(bfi_con_items)), na.rm = TRUE),
         BFI_NEU_sum = rowSums(dplyr::select(., dplyr::matches(bfi_neu_items)), na.rm = TRUE),
         BFI_OPEN_sum = rowSums(dplyr::select(., dplyr::matches(bfi_open_items)), na.rm = TRUE),
         UPPS_LPS_sum = rowSums(dplyr::select(., dplyr::matches(upps_lps_items)), na.rm = TRUE),
         UPPS_LPM_sum = rowSums(dplyr::select(., dplyr::matches(upps_lpm_items)), na.rm = TRUE),
         UPPS_PU_sum = rowSums(dplyr::select(., dplyr::matches(upps_pu_items)), na.rm = TRUE),
         UPPS_NU_sum = rowSums(dplyr::select(., dplyr::matches(upps_nu_items)), na.rm = TRUE)
         ) %>% 
  dplyr::select(PID, SONA, matches(ques_items)) %>% 
  mutate(across(all_of(matches(ques_items)), ~ scale_items(.), .names ="{.col}_Z"))
```



### Distribution of questionnaires data
```{r}
gghistogram(df_ques$NFC_sum, bins = 15, fill = "lightblue")+
  geom_vline(xintercept = mean(df_ques$NFC_sum, na.rm=TRUE), 
             linetype="dashed", linewidth=0.7, color="darkblue")+
  labs(title = "NFC Distribution", x="NFC score", y = "Count")+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(size = 5))

nfc_sum <- 
  df_ques %>% 
  summarise(nfc_mean = mean(.$NFC_sum, na.rm=TRUE),
         nfc_sd = sd(.$NFC_sum, na.rm=TRUE),
         nfc_med = median(.$NFC_sum, na.rm=TRUE),
         nfc_min = min(.$NFC_sum, na.rm=TRUE),
         nfc_max = max(.$NFC_sum, na.rm=TRUE))

gghistogram(df_ques$NFC_sum_Z, bins = 15, fill = "lightblue")
```






## Correlations
### Tasks
```{r}
df_ra_means_de_diffs_v2 <- 
  df_ra_means_de_v2_wide %>% 
  group_by(PID, DemandAversive_on) %>% 
  mutate(OnlineRatingZs_diff = OnlineRatingZs_easy - OnlineRatingZs_hard) %>% 
  ungroup()
```

```{r}
df_ra_dc_v2 <- 
  merge(df_ra_means_de_diffs_v2, df_ra_means_diffs_10ts_v2_wide, by="PID", all = TRUE) %>%
  merge(df_true_dc_props_easy_v2, by="PID", all = TRUE) %>% 
  merge(df_false_dc_props_easy_v2, by="PID", all = TRUE) %>% 
  merge(df_false_dc_means_demands_tsecs_v2_wide, by="PID", all = TRUE)
```

```{r}
df_ra_on_off_v2 <- 
  merge(df_ra_means_de_diffs_v2, df_ra_means_diffs_10ts_v2_wide, by="PID", all = TRUE) %>% 
  merge(df_off_ra_means_de_v2_by_demands_by_times, by="PID", all = TRUE)
```


```{r}
df_cors_v2 <- 
  merge(df_ra_means_de_diffs_v2, df_ra_means_diffs_10ts_v2_wide, by="PID", all=TRUE) %>% 
  merge(df_off_ra_means_de_v2_by_demands_by_times, by="PID", all=TRUE) %>% 
  merge(df_true_dc_props_easy_v2, by="PID", all=TRUE) %>% 
  merge(df_false_dc_props_easy_v2, by="PID", all = TRUE) %>% 
  merge(df_false_dc_means_demands_tsecs_v2_wide, by="PID", all = TRUE)

df_cors_pure_v2 <- 
  merge(df_ra_means_de_diffs_v2, df_ra_means_diffs_10ts_v2_wide, by="PID", all=TRUE) %>% 
  merge(df_off_ra_means_de_v2_by_demands_by_times, by="PID", all=TRUE) %>% 
  merge(df_true_dc_props_easy_v2, by="PID", all=TRUE) %>% 
  merge(df_false_dc_props_easy_v2, by="PID", all = TRUE) %>% 
  merge(df_false_dc_means_demands_tsecs_v2_wide, by="PID", all = TRUE) %>% 
  dplyr::select(-c(PID, ISI, DemandAvoidance_true, DemandAvoidance_false))
```

Combine Task + Questionnaires data
```{r}
df_cors_all_v2 <- 
  df_cors_v2 %>% 
  merge(df_ques, by="PID", all = TRUE) %>% 
  dplyr::select(-SONA)
```


correlation matrix output
```{r}
ra_dc_cors_matrix_v2 <- rcorr(as.matrix(df_cors_pure_v2))
write.csv(ra_dc_cors_matrix_v2$r, 
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/cors_matrix_r_v2.csv")
write.csv(ra_dc_cors_matrix_v2[["P"]], 
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/cors_matrix_p_v2.csv")
```

```{r}
ra_dc_cors_v2 <- cor(df_cors_pure_v2)
print(ra_dc_cors_v2)
write.csv(ra_dc_cors_v2, 
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/cors_v2.csv")


print("Online ~ True Choice")
bf <- correlationBF(df_cors_v2$OnlineRatingZs_diff, df_cors_v2$DCERs_true)
bf

print("Offline_T1 ~ True Choice")
bf <- correlationBF(df_cors_v2$OfflineRating_T1Zs_diff, df_cors_v2$DCERs_true)
bf

print("Offline_T2 ~ True Choice")
bf <- correlationBF(df_cors_v2$OfflineRating_T2Zs_diff, df_cors_v2$DCERs_true)
bf


print("Offline_T2 ~ False Choice")
bf <- correlationBF(df_cors_v2$OfflineRating_T2Zs_diff, df_cors_v2$DCERs_false)
bf

print("Offline_T3 ~ False Choice")
bf <- correlationBF(df_cors_v2$OfflineRating_T3Zs_diff, df_cors_v2$DCERs_false)
bf

# samples <- posterior(bf, iterations = 1000)
# summary(samples)
```



Scatter plots
#### Online Rating ~ Offline Rating
```{r}
# ggplot(df_cors_v2, aes(x = OnlineRatingZs_diff, y = OfflineRating_T1Zs_diff, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZs_diff, y = OfflineRating_T1Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & Offline Rating T1", 
       x = "Online Rating Diff (easy - hard)", y = "Offline Rating T1 Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_v2, aes(x = OnlineRatingZs_diff, y = OfflineRating_T2Zs_diff, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZs_diff, y = OfflineRating_T2Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & Offline Rating T2", 
       x = "Online Rating Diff (easy - hard)", y = "Offline Rating T2 Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_v2, aes(x = OnlineRatingZs_diff, y = OfflineRating_T2Zs_diff, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZs_diff, y = OfflineRating_T3Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & Offline Rating T3", 
       x = "Online Rating Diff (easy - hard)", y = "Offline Rating T3 Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_v2, aes(x = OfflineRating_T1Zs_diff, y = OfflineRating_T2Zs_diff, label = PID)) +
ggplot(df_cors_v2, aes(x = OfflineRating_T1Zs_diff, y = OfflineRating_T2Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline Rating T1 & T2", 
       x = "Offline Rating T1 Diff (easy - hard)", y = "Offline Rating T2 Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OfflineRating_T1Zs_diff, y = OfflineRating_T2Zs_diff, label = PID)) +
ggplot(df_cors_v2, aes(x = OfflineRating_T1Zs_diff, y = OfflineRating_T3Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline Rating T1 & T3", 
       x = "Offline Rating T1 Diff (easy - hard)", y = "Offline Rating T3 Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OfflineRating_T1Zs_diff, y = OfflineRating_T2Zs_diff, label = PID)) +
ggplot(df_cors_v2, aes(x = OfflineRating_T2Zs_diff, y = OfflineRating_T3Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline Rating T2 & T3", 
       x = "Offline Rating T2 Diff (easy - hard)", y = "Offline Rating T3 Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

#### Online Rating ~ Choice
```{r}
# ggplot(df_cors_v2, aes(x = OnlineRatingZs_diff, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZs_diff, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating & True Choice", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OnlineRatingZs_diff, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZs_diff, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating & False Choice", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating in 5th Trial Section & True Choice", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating in 5th Trial Section & False Choice", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DEC_prop_false_10)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating in 5th Trial Section & False Choice 1st Trial Section", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DEC_prop_false_20)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating in 5th Trial Section & False Choice 2nd Trial Section", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DEC_prop_false_30)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating in 5th Trial Section & False Choice 3rd Trial Section", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DEC_prop_false_40)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating in 5th Trial Section & False Choice 4th Trial Section", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OnlineRatingZ_diff_50, y = DEC_prop_false_50)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating in 5th Trial Section & False Choice 5th Trial Section", 
       x = "Online Rating Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### Offline Rating ~ Choice
```{r}
# ggplot(df_cors_v2, aes(x = OfflineRating_T1Zs_diff, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OfflineRating_T1Zs_diff, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors)) + 
  labs(title = "Correlation: Offline Rating T1 & True Choice", 
       x = "Offline Rating T1 Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OfflineRating_T2Zs_diff, y = DCERs_true, label = PID)) +
ggplot(df_cors_v2, aes(x = OfflineRating_T2Zs_diff, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: True Choice & Offline Rating T2", 
       x = "Offline Rating T2 Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = OfflineRating_T2Zs_diff, y = DCERs_false, label = PID)) +
ggplot(df_cors_v2, aes(x = OfflineRating_T2Zs_diff, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline Rating T2 & False Choice", 
       x = "Offline Rating T2 Diff (easy - hard)", y = "Easy Choice Proportion") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = DCERs_false, y = OfflineRating_T3Zs_diff, label = PID)) +
ggplot(df_cors_v2, aes(x = DEC_prop_false, y = OfflineRating_T3Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: False Choice & Offline Rating T3", 
       x = "Easy Choice Proportion", y = "Offline Rating T2 Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

#### True Choice ~ False Choice
```{r}
# ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false, label = PID)) +
ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation:True & False Choice", 
       x = "Easy Choice Proportion (True)", y = "Easy Choice Proportion (False)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false, label = PID)) +
ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false_10)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation:True & False Choice (1st section)", 
       x = "Easy Choice Proportion (True)", y = "Easy Choice Proportion (False 1-10)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false, label = PID)) +
ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false_20)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation:True & False Choice (2nd section)", 
       x = "Easy Choice Proportion (True)", y = "Easy Choice Proportion (False 11-20)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false, label = PID)) +
ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false_30)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation:True & False Choice (3rd section)", 
       x = "Easy Choice Proportion (True)", y = "Easy Choice Proportion (False 21-30)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false, label = PID)) +
ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false_40)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation:True & False Choice (4th section)", 
       x = "Easy Choice Proportion (True)", y = "Easy Choice Proportion (False 31-40)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false, label = PID)) +
ggplot(df_cors_v2, aes(x = DEC_prop_true, y = DEC_prop_false_50)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation:True & False Choice (5th section)", 
       x = "Easy Choice Proportion (True)", y = "Easy Choice Proportion (False 41-50)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


### Questionnaires
#### NFC
significant with False Easy Choice Probability
```{r}
# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = NFC_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating & NFC", 
       x = "NFC",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = DCERs_true, label = PID)) +
ggplot(df_cors_all_v2, aes(x = NFC_sum_Z, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: NFC & True Choice", 
       x = "NFC", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = DCERs_false, label = PID)) +
ggplot(df_cors_all_v2, aes(x = NFC_sum_Z, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: NFC & False Choice", 
       x = "NFC", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = DCERs_false, label = PID)) +
ggplot(df_cors_all_v2, aes(x = NFC_sum_Z, y = DEC_prop_false_10)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: NFC & False Choice (1-10)", 
       x = "NFC", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = DCERs_false, label = PID)) +
ggplot(df_cors_all_v2, aes(x = NFC_sum_Z, y = DEC_prop_false_20)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: NFC & False Choice (11-20)", 
       x = "NFC", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = DCERs_false, label = PID)) +
ggplot(df_cors_all_v2, aes(x = NFC_sum_Z, y = DEC_prop_false_30)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: NFC & False Choice (21-30)", 
       x = "NFC", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = DCERs_false, label = PID)) +
ggplot(df_cors_all_v2, aes(x = NFC_sum_Z, y = DEC_prop_false_40)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: NFC & False Choice (31-40)", 
       x = "NFC", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = DCERs_false, label = PID)) +
ggplot(df_cors_all_v2, aes(x = NFC_sum_Z, y = DEC_prop_false_50)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: NFC & False Choice (41-50)", 
       x = "NFC", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = DCERs_false, label = PID)) +
ggplot(df_cors_all_v2, aes(x = NFC_sum_Z, y = DEC_prop_false_drop)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: NFC & False Choice drop", 
       x = "NFC", y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

#### BFI
no significant r with Online Rating; NEU seems to be most related.
```{r}
# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_EXT_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & BFI_EXT", 
       x = "BFI_EXT",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_AGR_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & BFI_AGR", 
       x = "BFI_AGR",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_CON_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & BFI_CON", 
       x = "BFI_CON",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & BFI_NEU", 
       x = "BFI_NEU",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_OPEN_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & BFI_OPEN", 
       x = "BFI_OPEN",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


no significant r with Offline Rating T1; NEU seems most related.
```{r}
# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_EXT_sum_Z, y = OfflineRating_T1Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline T1 & BFI_EXT", 
       x = "BFI_EXT",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_AGR_sum_Z, y = OfflineRating_T1Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline T1 & BFI_AGR", 
       x = "BFI_AGR",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))


# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_CON_sum_Z, y = OfflineRating_T1Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline T1 & BFI_CON", 
       x = "BFI_CON",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))



# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OfflineRating_T1Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline T1 & BFI_NEU", 
       x = "BFI_NEU",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OfflineRating_T2Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline T2 & BFI_NEU", 
       x = "BFI_NEU",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OfflineRating_T3Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline T3 & BFI_NEU", 
       x = "BFI_NEU",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))



# ggplot(df_cors_all_v2, aes(x = BFI_OPEN_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BFI_OPEN_sum_Z, y = OfflineRating_T1Zs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Offline T1 & BFI_OPEN", 
       x = "BFI_OPEN",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


#### BAI
Online Rating
no significant
```{r}
# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BAS_D_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & BAS_D", 
       x = "BAS_D",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BAS_FS_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & BAS_FS", 
       x = "BAS_FS",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BAS_RR_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & BAS_RR", 
       x = "BAS_RR",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BIS_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online & BIS", 
       x = "BIS",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BAS_D_sum_Z, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: True Easy Choice & BAS_D", 
       x = "BAS_D",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BAS_FS_sum_Z, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: True Easy Choice & BAS_FS", 
       x = "BAS_FS",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BAS_RR_sum_Z, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: True Easy Choice & BAS_RR", 
       x = "BAS_RR",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BIS_sum_Z, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: True Easy Choice & BIS", 
       x = "BIS",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```


no correlated to True Choice; but correlated to False Choice.
```{r}
# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BAS_D_sum_Z, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: False Easy Choice & BAS_D", 
       x = "BAS_D",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BAS_FS_sum_Z, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: False Easy Choice & BAS_FS", 
       x = "BAS_FS",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BAS_RR_sum_Z, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: False Easy Choice & BAS_RR", 
       x = "BAS_RR",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = BIS_sum_Z, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: False Easy Choice & BIS", 
       x = "BIS",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



#### UPPS
Online Ratings
(Significant correlation with Lack of premeditation.)
```{r}
# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_NU_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating & UPPS_NU", 
       x = "UPPS_NU",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_PU_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating & UPPS_PU", 
       x = "UPPS_PU",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_LPS_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating & UPPS_LPS", 
       x = "UPPS_LPS",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_LPM_sum_Z, y = OnlineRatingZs_diff)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating & UPPS_LPM", 
       x = "UPPS_LPM",
       y = "Online Rating Diff (easy - hard)") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_LPM_sum_Z, y = OnlineRatingZs_easy)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating & UPPS_LPM", 
       x = "UPPS_LPM",
       y = "Online Rating Easy") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = NFC_sum, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_LPM_sum_Z, y = OnlineRatingZs_hard)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: Online Rating & UPPS_LPM", 
       x = "UPPS_LPM",
       y = "Online Rating Hard") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

True choice
(No significant correlation.)
```{r}
# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_NU_sum_Z, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: True Easy Choice & UPPS_NU", 
       x = "UPPS_NU",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_PU_sum_Z, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: True Easy Choice & UPPS_PU", 
       x = "UPPS_PU",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_LPS_sum_Z, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: True Easy Choice & UPPS_LPS", 
       x = "UPPS_LPS",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_LPM_sum_Z, y = DEC_prop_true)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: True Easy Choice & UPPS_LPM", 
       x = "UPPS_LPM",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

False choice
(No significant correlation.)
```{r}
# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_NU_sum_Z, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: False Easy Choice & UPPS_NU", 
       x = "UPPS_NU",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_PU_sum_Z, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: False Easy Choice & UPPS_PU", 
       x = "UPPS_PU",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_LPS_sum_Z, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: False Easy Choice & UPPS_LPS", 
       x = "UPPS_LPS",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

# ggplot(df_cors_all_v2, aes(x = BFI_NEU_sum_Z, y = OnlineRatingZs_diff, label = PID)) +
ggplot(df_cors_all_v2, aes(x = UPPS_LPM_sum_Z, y = DEC_prop_false)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  stat_cor(method = "spearman")+
  # geom_text_repel(max.overlaps = nrow(df_cors_v2)) + 
  labs(title = "Correlation: False Easy Choice & UPPS_LPM", 
       x = "UPPS_LPM",
       y = "Easy Choice Probability") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




## Regressions
### Online Rating ~ Task Demand
Y: All
```{r}
reg.m1.1_lmerTest_v2 <- 
  df_v2_good_LEARNING %>% 
  lmerTest::lmer(OnlineRatingZ ~ TaskDemand*TrialN + RT_preZ + ACC_preZ + 
                   (TaskDemand|TrialN) + 
                   (TaskDemand|TrialTransition_pre) + (TaskDemand|TrialDemandLearning_pre) +
                   (1|PID) + (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
# summary(reg.m1.1_lmerTest_v2)
tab_model(reg.m1.1_lmerTest_v2,
          show.re.var = FALSE, show.ngroups = FALSE
          )
```


```{r}
emtrends(reg.m1.1_lmerTest_v2, ~TaskDemand, var="TrialN")
```

```{r}
mod_levels <- list(TrialN=seq(1,50,1), TaskDemand=c("easy", "hard"))
```


```{r}
reg.m1.1_emmeans <- emmeans(reg.m1.1_lmerTest_v2, ~TaskDemand*TrialN, at=mod_levels)

reg.m1.1_contrast <- contrast(reg.m1.1_emmeans, "pairwise", by="TrialN")
# contrast between demands in each participant
```

interaction plot
```{r}
emmip(reg.m1.1_lmerTest_v2, TaskDemand ~ TrialN, at=mod_levels) +
  scale_color_lancet()+
  labs(title = "Regression: Online Rating ~ TaskDemand*TrialN", 
       x = "TrialN",
       y = "Online Rating (z-scored)",
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
plot_model(reg.m1.1_lmerTest_v2, type = "pred", terms = c("TrialN", "TaskDemand"),
           colors = c("#00468BFF", "#ED0000FF"),
           # show.data = TRUE, jitter = 0.5, 
           show.values = TRUE, show.p = TRUE)+
  labs(title = "Regression: Online Rating ~ TaskDemand*TrialN", 
       x = "TrialN",
       y = "Online Rating (z-scored)",
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

Y: online ratings in 5th section
```{r}
reg.m1.2_lmerTest_v2 <- 
  df_v2_good_LEARNING %>% 
  filter(TrialN %in% c(seq(40,50))) %>% 
  lmerTest::lmer(OnlineRatingZ ~ TaskDemand*TrialN + RT_preZ + ACC_preZ + 
                   (TaskDemand | TrialN) +
                   (TaskDemand|TrialTransition_pre) + (TaskDemand|TrialDemandLearning_pre) +
                   (1|PID) + (1|Gender) + (1|CueMapping) + (1|ISI) + (1|TrialN),
                 data = .)
tab_model(reg.m1.2_lmerTest_v2)
# not significant
```

```{r}
plot_model(reg.m1.2_lmerTest_v2, type = "pred", terms = c("TrialN", "TaskDemand"),
           colors = c("#00468BFF", "#ED0000FF"),
           # show.data = TRUE, jitter = 0.5, 
           show.values = TRUE, show.p = TRUE)+
  labs(title = "Regression: Online Rating (5th section) ~ TaskDemand*TrialN", 
       x = "TrialN",
       y = "Online Rating (z-scored)",
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```




Compare models
```{r}
tab_model(reg.m1.1_lmerTest_v2, reg.m1.2_lmerTest_v2,
          show.re.var = FALSE, show.ngroups = FALSE,
          dv.labels = c("OnlineRatingZ", "OnlineRatingZ in 40-50 Trial Section"),
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/reg_m1_v2.html")
```

(AIC counts more on model fitness; BIC counts more on model simplicity. Smaller value, better model)
```{r}
AIC(logLik(reg.m1.1_lmerTest_v2))
AIC(logLik(reg.m1.2_lmerTest_v2))
```

```{r}
# higher penalty on complex models than AIC
BIC(logLik(reg.m1.1_lmerTest_v2))
BIC(logLik(reg.m1.2_lmerTest_v2))
```


with Personalities' random effects
```{r}
df_cors_all_v2_pure <- 
  df_cors_all_v2 %>% 
  select(-c(Gender, ISI))

df_v2_good_LEARNING_ques <- 
  df_v2_good_LEARNING %>% 
  merge(df_cors_all_v2_pure, by.x = "PID", by.y = "PID", all.x = TRUE) 
```

```{r}
reg.m1.3_lmerTest_v2 <- 
  df_v2_good_LEARNING_ques %>% 
  lmerTest::lmer(OnlineRatingZ ~ TaskDemand*TrialN + RT_preZ + ACC_preZ +
                   NFC_sum_Z +
                  BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                  BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                  UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
                   (0 + TaskDemand|TrialN) + 
                   (0 + TaskDemand|TrialTransition_pre) + (0 + TaskDemand|TrialDemandLearning_pre) +
                   (1 | Gender) + (1|CueMapping.x) + (1|ISI) +
                   (1 | NFC_sum_Z) +
                   (1 | BFI_NEU_sum_Z) +
                   (1 | BFI_AGR_sum_Z) +
                   (1 | BFI_CON_sum_Z) +
                   (1 | BFI_EXT_sum_Z) +
                   (1 | BFI_OPEN_sum_Z) +
                   (1 | BAS_D_sum_Z) + (1 | BAS_FS_sum_Z) +
                   (1 | BIS_sum_Z) +
                   (1 | UPPS_NU_sum_Z) + (1 | UPPS_PU_sum_Z) +
                   (1 | UPPS_LPS_sum_Z) + (1 | UPPS_LPM_sum_Z),
                 data = .)
tab_model(reg.m1.3_lmerTest_v2, show.re.var = FALSE, show.ngroups = FALSE)
```

```{r}
plot_model(reg.m1.3_lmerTest_v2, type = "pred", terms = c("TrialN", "TaskDemand"),
           colors = c("#00468BFF", "#ED0000FF"),
           # show.data = TRUE, jitter = 0.5, 
           show.values = TRUE, show.p = TRUE)+
  labs(title = "Regression: Online Rating ~ TaskDemand*TrialN", 
       x = "TrialN",
       y = "Online Rating (z-scored)",
       color = "Task Demand") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



### Online Rating Diff ~ Personalities
Y: All
```{r}
reg.m1.4_lmerTest_v2 <- 
  df_cors_all_v2 %>% 
  lmerTest::lmer(OnlineRatingZs_diff ~ 
                  NFC_sum_Z +
                  BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                  BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                  UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
                   (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
tab_model(reg.m1.4_lmerTest_v2)
# no personality had significant effect
```

Y: ratings in 5th section
```{r}
reg.m1.5_lmerTest_v2 <- 
  df_cors_all_v2 %>% 
  lmerTest::lmer(OnlineRatingZ_diff_50 ~ 
                  NFC_sum_Z +
                  BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                  BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                  UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
                   (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
tab_model(reg.m1.5_lmerTest_v2)
# no personality had significant effect
```



### True Choice ~ Affective Ratings
#### Online Rating
Y: All trials
```{r}
reg.m2.1_lmerTest_v2 <- 
  df_cors_all_v2 %>% 
  lmerTest::lmer(DEC_prop_true ~ OnlineRatingZs_diff + 
                  # NFC_sum_Z +
                  # BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                  # BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                  # UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
                   (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
tab_model(reg.m2.1_lmerTest_v2)
# no personality had significant effect
```

Trial Section 40-50 Online Rating
```{r}
reg.m2.2_lmerTest_v2 <- 
  df_cors_all_v2 %>% 
  lmerTest::lmer(DEC_prop_true ~ OnlineRatingZ_diff_50 + 
                  NFC_sum_Z +
                  BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                  BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                  UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
                   (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
tab_model(reg.m2.2_lmerTest_v2)
```


Compare models

(AIC counts more on model fitness; BIC counts more on model simplicity. Smaller value, better model)
```{r}
AIC(logLik(reg.m2.1_lmerTest_v2))
AIC(logLik(reg.m2.2_lmerTest_v2))
```

```{r}
# higher penalty on complex models than AIC
BIC(logLik(reg.m2.1_lmerTest_v2))
BIC(logLik(reg.m2.2_lmerTest_v2))
```



#### Offline Rating
```{r}
reg.m2.3_lmerTest_v2 <- 
  df_cors_all_v2 %>% 
  lmerTest::lmer(DEC_prop_true ~ OfflineRating_T1Zs_diff + 
                  # NFC_sum_Z +
                  # BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                  # BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                  # UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
                   (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
tab_model(reg.m2.3_lmerTest_v2)
# no personality had significant effect
```

```{r}
reg.m2.4_lmerTest_v2 <- 
  df_cors_all_v2 %>% 
  lmerTest::lmer(OfflineRating_T2Zs_diff ~ DEC_prop_true + 
                  NFC_sum_Z +
                  BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                  BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                  UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
                   (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
tab_model(reg.m2.4_lmerTest_v2)
# no personality had significant effect
```

Compare models
(AIC counts more on model fitness; BIC counts more on model simplicity. Smaller value, better model)
```{r}
AIC(logLik(reg.m2.3_lmerTest_v2))
AIC(logLik(reg.m2.4_lmerTest_v2))
```

```{r}
# higher penalty on complex models than AIC
BIC(logLik(reg.m2.3_lmerTest_v2))
BIC(logLik(reg.m2.4_lmerTest_v2))
```


output
```{r}
tab_model(reg.m2.1_lmerTest_v2, reg.m2.2_lmerTest_v2, reg.m2.3_lmerTest_v2, reg.m2.4_lmerTest_v2,
          show.re.var = FALSE, show.ngroups = FALSE,
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/reg_m2_v2.html")
```



### False Choice ~ Affective Ratings
#### Online Rating
Y: All trials
```{r}
reg.m3.1_lmerTest_v2 <- 
  df_cors_all_v2 %>% 
  lmerTest::lmer(DEC_prop_false ~ OnlineRatingZs_diff + 
                   # NFC_sum_Z +
                   # BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                   # BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                   # UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
                   (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
tab_model(reg.m3.1_lmerTest_v2)
# no personality had significant effect
```

Y: in the 5th section
```{r}
reg.m3.2_lmerTest_v2 <- 
  df_cors_all_v2 %>% 
  lmerTest::lmer(DEC_prop_false ~ OnlineRatingZ_diff_50 + 
                  # NFC_sum_Z +
                  # BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                  # BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                  # UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
                   (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
tab_model(reg.m3.2_lmerTest_v2)
# no more personality had significant effect beside Online Rating
```

Compare models
(AIC counts more on model fitness; BIC counts more on model simplicity. Smaller value, better model)
```{r}
AIC(logLik(reg.m3.1_lmerTest_v2))
AIC(logLik(reg.m3.2_lmerTest_v2))
```

```{r}
# higher penalty on complex models than AIC
BIC(logLik(reg.m3.1_lmerTest_v2))
BIC(logLik(reg.m3.2_lmerTest_v2))
```




#### Offline Rating T2
```{r}
reg.m3.3_lmerTest_v2 <- 
  df_cors_all_v2 %>% 
  lmerTest::lmer(DEC_prop_false ~ OfflineRating_T2Zs_diff +
                  # NFC_sum_Z +
                  # BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                  # BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                  # UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z + 
                   (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
tab_model(reg.m3.3_lmerTest_v2)
# no more personality had significant effect beside Offline Rating
```

#### Offline Rating T3
```{r}
reg.m3.4_lmerTest_v2 <- 
  df_cors_all_v2 %>% 
  lmerTest::lmer(OfflineRating_T3Zs_diff ~ DEC_prop_false + 
                  NFC_sum_Z +
                  BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
                  BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
                  UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
                   (1|Gender) + (1|CueMapping) + (1|ISI),
                 data = .)
tab_model(reg.m3.4_lmerTest_v2)
# no more personality had significant effect beside Offline Rating
```


```{r}
tab_model(reg.m3.1_lmerTest_v2, reg.m3.2_lmerTest_v2, reg.m3.3_lmerTest_v2, reg.m3.4_lmerTest_v2,
          show.re.var = FALSE, show.ngroups = FALSE,
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/reg_m3_v2.html")
```


```{r}
tab_model(reg.m2.1_lmerTest_v2, 
          reg.m3.1_lmerTest_v2, 
          reg.m2.3_lmerTest_v2, reg.m3.3_lmerTest_v2,
          show.re.var = FALSE, show.ngroups = FALSE)
```



#### Logistic Regression of Flase Choice
```{r}
df_cors_all_v2_pure <- 
  df_cors_all_v2 %>% 
  dplyr::select(-c(Gender, ISI))

df_v2_good_FALSECHOICE_logistic <- 
  df_v2_good_FALSECHOICE %>% 
  merge(df_cors_all_v2_pure, by.x = "PID", by.y = "PID", all.x = TRUE) 
```

```{r}
reg.log.false1 <- 
  df_v2_good_FALSECHOICE_logistic %>% 
  glmer(DemandChoice_code ~ TrialN + 
          OnlineRatingZs_diff + 
          OfflineRating_T1Zs_diff + OfflineRating_T2Zs_diff +
          DEC_prop_true + 
          (0 + TrialN|OnlineRatingZs_diff) + 
          (0 + TrialN|OfflineRating_T1Zs_diff) + (0 + TrialN|OfflineRating_T2Zs_diff) + 
          (0 + TrialN|DEC_prop_true)+ 
          # NFC_sum_Z +
          # BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
          # BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
          # UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
          (1 | PID) + (1 | Gender) + (1 | ISI),
          # (1 | TrialN/Gender) +
          # (1 | TrialN/ISI)  +
          # (1 | TrialN/NFC_sum_Z) +
          # (1 | TrialN/BFI_NEU_sum_Z) +
          # (1 | TrialN/BFI_AGR_sum_Z) +
          # (1 | TrialN/BFI_CON_sum_Z) +
          # (1 | TrialN/BFI_EXT_sum_Z) +
          # (1 | TrialN/BFI_OPEN_sum_Z) +
          # (1 | TrialN/BAS_D_sum_Z) + (1 | TrialN/BAS_FS_sum_Z) +
          # (1 | TrialN/BIS_sum_Z) +
          # (1 | TrialN/UPPS_NU_sum_Z) + (1 | TrialN/UPPS_PU_sum_Z) +
          # (1 | TrialN/UPPS_LPS_sum_Z) + (1 | TrialN/UPPS_LPM_sum_Z)
       data = ., 
       family = binomial)
tab_model(reg.log.false1, dv.labels = c("ChoiceDemand"),
          show.re.var = FALSE, show.ngroups = FALSE)
```


```{r}
reg.log.false2 <- 
  df_v2_good_FALSECHOICE_logistic %>% 
  glmer(DemandChoice_code ~ TrialN*DemandAversive_on +
          TrialN*DemandAversive_off_T1 +
          TrialN*DemandAversive_off_T2 +
          TrialN*DemandAversive_off_T3 +
          TrialN*DemandAvoidance_true + 
          # OfflineRating_T1Zs_diff + OfflineRating_T2Zs_diff +
          # DEC_prop_true + 
          (1 + TrialN|DemandAversive_on) +
          (1 + TrialN|DemandAversive_off_T1) +
          (1 + TrialN|DemandAversive_off_T2) +
          (1 + TrialN|DemandAversive_off_T3) +
          # (0 + TrialN|OfflineRating_T1Zs_diff) + (0 + TrialN|OfflineRating_T2Zs_diff) + 
          (1 + TrialN|DemandAvoidance_true)+ 
          # NFC_sum_Z +
          # BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
          # BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
          # UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z +
          (1 | PID) + (1 | Gender) + (1 | ISI),
          # (1 | TrialN/Gender) +
          # (1 | TrialN/ISI)  +
          # (1 | TrialN/NFC_sum_Z) +
          # (1 | TrialN/BFI_NEU_sum_Z) +
          # (1 | TrialN/BFI_AGR_sum_Z) +
          # (1 | TrialN/BFI_CON_sum_Z) +
          # (1 | TrialN/BFI_EXT_sum_Z) +
          # (1 | TrialN/BFI_OPEN_sum_Z) +
          # (1 | TrialN/BAS_D_sum_Z) + (1 | TrialN/BAS_FS_sum_Z) +
          # (1 | TrialN/BIS_sum_Z) +
          # (1 | TrialN/UPPS_NU_sum_Z) + (1 | TrialN/UPPS_PU_sum_Z) +
          # (1 | TrialN/UPPS_LPS_sum_Z) + (1 | TrialN/UPPS_LPM_sum_Z)
       data = ., 
       family = binomial)
tab_model(reg.log.false2, dv.labels = c("ChoiceDemand"),
          show.re.var = FALSE, show.ngroups = FALSE)
```
interaction plot
```{r}
plot_model(reg.log.false2, type = "pred", 
           terms = c("TrialN[all]", "DemandAversive_off_T3"),
           colors = c("#1f77b4", "#ff7f0e"),
           # show.data = TRUE, jitter = 0.5, 
           show.values = TRUE, show.p = TRUE)+
  labs(title = "Logistic Regression: False Choice ~ DemandAversive_off_T3*TrialN", 
       x = "TrialN",
       y = "Easy Choice Probability",
       color = "DemandAversive_off_T3") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
plot_model(reg.log.false2, type = "pred", 
           terms = c("TrialN[all]", "DemandAvoidance_true"),
           colors = c("#1f77b4", "#ff7f0e"),
           # show.data = TRUE, jitter = 0.5, 
           show.values = TRUE, show.p = TRUE)+
  labs(title = "Logistic Regression: False Choice ~ DemandAvoidance_true*TrialN", 
       x = "TrialN",
       y = "Easy Choice Probability",
       color = "DemandAvoidance_true") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```



### Mediation Analysis
```{r}
med_model <- 
  df_cors_all_v2 %>% 
  lm(data=., DEC_prop_true ~ OnlineRatingZs_diff )

out_model <- 
  df_cors_all_v2 %>% 
  lm(data=.,DEC_prop_false ~ OnlineRatingZs_diff +
                   DEC_prop_true)

reg.med1_v2 <- 
  mediate(model.m = med_model,
          model.y = out_model,
          treat = "OnlineRatingZs_diff",
          mediator = "DEC_prop_true",
          control.value = 3,
          treat.value = 4,
          boot = TRUE,
          sims = 100)

summary(reg.med1_v2)
# ACME: Average Causal Mediation Effects
# ADE: Average Direct Effects
# chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cran.r-project.org/web/packages/mediation/vignettes/mediation.pdf
```

```{r}
covs <- 
  c("Gender", "CueMapping", "ISI",
    "NFC_sum_Z",
    "BFI_NEU_sum_Z","BFI_AGR_sum_Z","BFI_CON_sum_Z","BFI_EXT_sum_Z","BFI_OPEN_sum_Z",
      "BAS_D_sum_Z","BAS_FS_sum_Z"," BIS_sum_Z",
       "UPPS_NU_sum_Z","UPPS_PU_sum_Z","UPPS_LPS_sum_Z","UPPS_LPM_sum_Z")

med_model <- 
  df_cors_all_v2 %>% 
  lm(data=.,
       DEC_prop_true ~ OnlineRatingZs_diff +
         Gender + CueMapping + ISI +
       NFC_sum_Z +
       BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
       BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
       UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z
       ,)

out_model <- 
  df_cors_all_v2 %>% 
  lm(data=.,
       DEC_prop_false ~ OnlineRatingZs_diff +
         DEC_prop_true +
         Gender + CueMapping + ISI+
       NFC_sum_Z +
       BFI_NEU_sum_Z + BFI_AGR_sum_Z + BFI_CON_sum_Z + BFI_EXT_sum_Z + BFI_OPEN_sum_Z +
       BAS_D_sum_Z + BAS_FS_sum_Z + BIS_sum_Z +
       UPPS_NU_sum_Z + UPPS_PU_sum_Z + UPPS_LPS_sum_Z + UPPS_LPM_sum_Z
     ,)

reg.med2_v2 <- 
  mediate(model.m = med_model,
          model.y = out_model,
          treat = "OnlineRatingZs_diff",
          mediator = "DEC_prop_true",
          covariates = covs,
          sims = 100)

summary(reg.med2_v2)
```






### Exporatory Analysis
#### Trial Performance ~ Online Rating
```{r}
reg.m4.1_lmerTest_v2 <- 
  df_v2_good_LEARNING %>% 
  lmerTest::lmer(RTZ ~ OnlineRatingZ + TaskDemand*TrialN + 
                   RT_preZ + ACC_preZ + Trial_A_measure_pre +
                   (1 + TaskDemand|TrialN) + (1 |TrialTransition_pre) + (1 |TrialDemandLearning_pre) +
                   (1| PID) + (1 | ISI) + (1 | CueMapping) + (1 | Gender), 
                 data = .)
# summary(reg.m4.1_lmerTest_v2)
```

```{r}
reg.m4.2_lmerTest_v2 <- 
  df_v2_good_LEARNING %>% 
  lmerTest::lmer(ACCZ ~ OnlineRatingZ + TaskDemand*TrialN + 
                   RT_preZ + ACC_preZ + Trial_A_measure_pre +
                   (1 + TaskDemand|TrialN) + (1 |TrialTransition_pre) + (1 |TrialDemandLearning_pre) +
                   (1| PID) + (1 | ISI) + (1 | CueMapping) + (1 | Gender), 
                 data = .)
```

```{r}
reg.m4.3_lmerTest_v2 <- 
  df_v2_good_LEARNING %>% 
  lmerTest::lmer(Trial_A_measure ~ OnlineRatingZ + TaskDemand*TrialN + 
                   RT_preZ + ACC_preZ + Trial_A_measure_pre +
                   (1 + TaskDemand|TrialN) + (1 |TrialTransition_pre) + (1 |TrialDemandLearning_pre) +
                   (1| PID) + (1 | ISI) + (1 | CueMapping) + (1 | Gender), 
                 data = .)
```

```{r}
tab_model(reg.m4.1_lmerTest_v2, reg.m4.2_lmerTest_v2, reg.m4.3_lmerTest_v2,
          show.re.var = FALSE, show.ngroups = FALSE,
          file = "E:/Dissertation/Methods/Exp2_Demand_Choice/DARDCpy_v2/data_analysis/reg_m4_v2.html")
```










#### RL-DDM
False Choice Phase
```{r}
df_v2_good_FALSECHOICE_rlddm <- 
  df_v2_good_FALSECHOICE_remove_outliers %>% 
  merge(df_cors_all_v2_pure, by.x = "PID", by.y = "PID", all.x = TRUE) %>% 
  dplyr::select(c(PID, ChoiceRT, ChoiceDemand, TaskDemand, TrialN,
         DemandAversive_on,
         DemandAversive_off_T1, DemandAversive_off_T2, DemandAversive_off_T3,
         DemandAvoidance_true))
df_v2_good_FALSECHOICE_rlddm_format <- 
  df_v2_good_FALSECHOICE_rlddm %>% 
  mutate(response = ifelse(ChoiceDemand == "easy", 0, 1),
         rt = ChoiceRT, # in seconds
         split_by = ifelse(ChoiceDemand == "easy", 0, 1),
         feedback = ifelse(ChoiceDemand == "easy", -1, 1),
         q_init = ifelse(DemandAversive_on == "demand_aversive", 0, 1)) %>% 
  rename(subj_idx = PID,
         trial = TrialN)
```

```{r}

write.csv(df_v2_good_FALSECHOICE_rlddm_format,
          "/Users/ottolab/Desktop/Yue/DARDC_v2_Data_Analysis/rlddm_false_choice.csv",
          row.names = FALSE)
```



```{r}
histogram(df_v2_good_FALSECHOICE_rlddm_format$rt[df_v2_good_FALSECHOICE_rlddm_format$subj_idx == 49], breaks = 20)
# PID 49 once caused extreme estimated value of parameter t in RLDDM gelman rubin statistics) 
# PID 117 once caused extreme estimated value of parameter t in RLDDM gelman rubin statistics) 
```





Learning Phase
```{r}

```


# Tutorial of R Notebook

{r} or {bash} or {python} # indicate what language is used in code


This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.



